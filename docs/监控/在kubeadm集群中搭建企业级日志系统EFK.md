# åœ¨kubeadmé›†ç¾¤ä¸­æ­å»ºä¼ä¸šçº§æ—¥å¿—ç³»ç»ŸEFK


1. ç¡®è®¤é›†ç¾¤æ­£å¸¸ï¼š

```
kubectl cluster-info
kubectl get nodes -o wide
```

1. ç”±äºä½ æ˜¯ **å•èŠ‚ç‚¹é›†ç¾¤**ï¼Œé»˜è®¤ `control-plane` èŠ‚ç‚¹æœ‰æ±¡ç‚¹ï¼ŒPod ä¸ä¼šè°ƒåº¦åˆ°å®ƒã€‚å¯ä»¥é€‰æ‹©ï¼š

   - **æ–¹æ³•Aï¼ˆæ¨èï¼‰ï¼š** åœ¨åé¢ YAML é‡Œéƒ½åŠ ä¸Š tolerationsã€‚

   - **æ–¹æ³•Bï¼š** ç›´æ¥ç§»é™¤æ±¡ç‚¹ï¼š

     ```
     kubectl taint nodes --all node-role.kubernetes.io/control-plane- || true
     ```

2. æ£€æŸ¥æœ‰æ²¡æœ‰é»˜è®¤çš„ **StorageClass**ï¼ˆåé¢ Elasticsearch è¦ç”¨PVCå­˜å‚¨æ•°æ®ï¼‰ï¼š

```
kubectl get sc
```

- å¦‚æœæ²¡æœ‰ï¼Œå¯ä»¥å¿«é€Ÿè£…ä¸€ä¸ªè½»é‡çš„æœ¬åœ°å­˜å‚¨ï¼š

  ```
  kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
  kubectl annotate sc local-path storageclass.kubernetes.io/is-default-class="true"
  ```

------

è¦ä¸è¦æˆ‘å¸®ä½ å…ˆå†™ä¸€ä¸ª **åˆ›å»ºå‘½åç©ºé—´ efk** çš„ YAML å’Œå‘½ä»¤ï¼Ÿè¿™æ ·æˆ‘ä»¬ä»è¿™é‡Œå¼€å§‹ï¼Œä¸€æ­¥æ­¥å¾€ä¸‹æ­å»ºã€‚

### 1ï¸âƒ£ åˆ›å»ºå‘½åç©ºé—´ efk

ä½ å¯ä»¥ç›´æ¥æ‰§è¡Œå‘½ä»¤ï¼š

```
kubectl create namespace efk
```

### 1ï¸âƒ£ åˆ›å»ºå‘½åç©ºé—´ efk

ä½ å¯ä»¥ç›´æ¥æ‰§è¡Œå‘½ä»¤ï¼š

```
kubectl create namespace efk
```

æˆ–è€…å†™æˆä¸€ä¸ª YAML æ¸…å•ï¼ˆå»ºè®®ç”¨è¿™ç§æ–¹å¼ï¼Œæ–¹ä¾¿ç‰ˆæœ¬ç®¡ç†ï¼‰ï¼š

```
# efk-namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: efk
```

æ‰§è¡Œï¼š

```
kubectl apply -f efk-namespace.yaml
```

ç„¶åéªŒè¯ï¼š

```
kubectl get ns
```

------

ğŸ‘‰ ä¸‹ä¸€æ­¥æˆ‘ä»¬è¦å¤„ç† **Elasticsearch çš„å‰ç½®å†…æ ¸å‚æ•° vm.max_map_count**ï¼ˆå¦åˆ™å¯åŠ¨ä¼šæŠ¥é”™ï¼‰ã€‚
 è¦ä¸è¦æˆ‘å…ˆå¸®ä½ å†™ä¸€ä¸ª **DaemonSet æ¥è®¾ç½® vm.max_map_count**ï¼Ÿ

### 2ï¸âƒ£ åˆ›å»º DaemonSet è®¾ç½®å†…æ ¸å‚æ•°

æ–°å»ºæ–‡ä»¶ **`efk-sysctl.yaml`**ï¼š

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: efk-sysctl
  namespace: efk
spec:
  selector:
    matchLabels:
      app: efk-sysctl
  template:
    metadata:
      labels:
        app: efk-sysctl
    spec:
      hostPID: true
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
        - name: sysctl
          image: busybox:1.36
          securityContext:
            privileged: true
          command: ["sh", "-c"]
          args:
            - |
              sysctl -w vm.max_map_count=262144
              sysctl -w fs.file-max=1048576
              # å®¹å™¨ä¿æŒå­˜æ´»
              sleep 3600000
```

åº”ç”¨ï¼š

```
kubectl apply -f efk-sysctl.yaml
```

æ£€æŸ¥æ˜¯å¦è¿è¡ŒæˆåŠŸï¼š

```
kubectl -n efk get pods -o wide
```

ç™»å½•èŠ‚ç‚¹ç¡®è®¤å‚æ•°å·²ç”Ÿæ•ˆï¼š

```
sysctl -n vm.max_map_count
```

æ­£å¸¸çš„è¯åº”è¯¥è¾“å‡ºï¼š`262144`

### 3ï¸âƒ£ éƒ¨ç½² Elasticsearchï¼ˆå•èŠ‚ç‚¹ï¼‰

æ–°å»ºæ–‡ä»¶ **`elasticsearch-single.yaml`**ï¼š

```
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: efk
  labels:
    app: elasticsearch
spec:
  ports:
    - name: http
      port: 9200
      targetPort: 9200
  selector:
    app: elasticsearch
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: efk
spec:
  serviceName: elasticsearch
  replicas: 1
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
        - name: es
          image: docker.io/elasticsearch:8.14.1
          ports:
            - containerPort: 9200
              name: http
            - containerPort: 9300
              name: transport
          env:
            - name: discovery.type
              value: single-node
            - name: ES_JAVA_OPTS
              value: "-Xms512m -Xmx512m"
            - name: xpack.security.enabled
              value: "false"   # å•æœºæµ‹è¯•å…³é—­å®‰å…¨è®¤è¯
          volumeMounts:
            - name: es-data
              mountPath: /usr/share/elasticsearch/data
          resources:
            requests:
              cpu: "250m"
              memory: "1Gi"
            limits:
              cpu: "1"
              memory: "2Gi"
  volumeClaimTemplates:
    - metadata:
        name: es-data
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: "local-path"  # å¦‚æœä½ ç”¨äº† local-path-provisioner
        resources:
          requests:
            storage: 20Gi
```

åº”ç”¨ï¼š

```
kubectl apply -f elasticsearch-single.yaml
```

ç­‰å¾…å¯åŠ¨å®Œæˆï¼š

```
kubectl -n efk rollout status sts/elasticsearch
```

æ£€æŸ¥ Podï¼š

```
kubectl -n efk get pods -o wide
```

æµ‹è¯•è®¿é—®ï¼š

```
kubectl -n efk port-forward svc/elasticsearch 9200:9200 &
curl http://127.0.0.1:9200
```

æ­£å¸¸çš„è¯ä¼šè¿”å› Elasticsearch çš„ JSON ä¿¡æ¯ã€‚

###### ChatGPT è¯´ï¼š

å¥½çš„ ğŸ‘ æˆ‘ä»¬ç»§ç»­æ¥éƒ¨ç½² **Kibana**ï¼Œå®ƒæ˜¯ Elasticsearch çš„å¯è§†åŒ–ç•Œé¢ã€‚

### 4ï¸âƒ£ éƒ¨ç½² Kibana

æ–°å»ºæ–‡ä»¶ **`kibana.yaml`**ï¼š

```
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: efk
  labels:
    app: kibana
spec:
  type: NodePort   # å•èŠ‚ç‚¹é›†ç¾¤ï¼Œæ–¹ä¾¿å¤–éƒ¨è®¿é—®
  ports:
    - name: http
      port: 5601
      targetPort: 5601
      nodePort: 30601   # è®¿é—®ç«¯å£ï¼Œå¯è‡ªè¡Œä¿®æ”¹
  selector:
    app: kibana
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: efk
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
        - name: kibana
          image: docker.io/kibana:8.14.1
          ports:
            - containerPort: 5601
          env:
            - name: ELASTICSEARCH_HOSTS
              value: "[\"http://elasticsearch.efk.svc:9200\"]"
            - name: NODE_OPTIONS
              value: "--max-old-space-size=2048"
          resources:
            requests:
              cpu: "100m"
              memory: "256Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
```

åº”ç”¨ï¼š

```
kubectl apply -f kibana.yaml
```

ç­‰å¾…å¯åŠ¨ï¼š

```
kubectl -n efk rollout status deploy/kibana
```

æ£€æŸ¥ Podï¼š

```
kubectl -n efk get pods -o wide
```

ç„¶åè®¿é—®ï¼š

```
http://<ä½ çš„èŠ‚ç‚¹IP>:30601
```

å°±èƒ½çœ‹åˆ° Kibana ç•Œé¢å•¦ ğŸ‰

------

âœ… Kibana å·²ç»æ­å»ºå¥½äº†ã€‚

### 5ï¸âƒ£ éƒ¨ç½² Fluent Bit

#### 5.1 åˆ›å»º RBAC

æ–°å»ºæ–‡ä»¶ **`fluent-bit-rbac.yaml`**ï¼š

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluent-bit
  namespace: efk
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluent-bit-read
rules:
  - apiGroups: [""]
    resources: ["pods", "namespaces"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluent-bit-read
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: fluent-bit-read
subjects:
  - kind: ServiceAccount
    name: fluent-bit
    namespace: efk
```

åº”ç”¨ï¼š

```
kubectl apply -f fluent-bit-rbac.yaml
```

------

#### 5.2 åˆ›å»º ConfigMap

æ–°å»ºæ–‡ä»¶ **`fluent-bit-config.yaml`**ï¼š

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
  namespace: efk
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush        5
        Daemon       Off
        Log_Level    debug
        Parsers_File parsers.conf

    [INPUT]
        Name              tail
        Path              /var/log/containers/*.log
        Exclude_Path      /var/log/containers/*_fluent-bit-*
        Tag               kube.*
        Mem_Buf_Limit     50MB
        Skip_Long_Lines   On
        Refresh_Interval  5
        Parser            docker

    [FILTER]
        Name                kubernetes
        Match               kube.*
        Kube_URL            https://kubernetes.default.svc:443
        Kube_Tag_Prefix     kube.var.log.containers.
        Merge_Log           On
        Keep_Log            Off
        K8S-Logging.Parser  On
        K8S-Logging.Exclude Off

    [OUTPUT]
        Name            es
        Match           kube.*
        Host            elasticsearch.efk.svc
        Port            9200
        Logstash_Format On
        Logstash_Prefix kubernetes
        Replace_Dots    On
        Retry_Limit     False

  parsers.conf: |
    [PARSER]
        Name        docker
        Format      json
        Time_Key    time
        Time_Format %Y-%m-%dT%H:%M:%S.%L%z

```

åº”ç”¨ï¼š

```
kubectl apply -f fluent-bit-config.yaml
```

------

âœ… åˆ°è¿™é‡Œï¼ŒRBAC å’Œé…ç½®æ–‡ä»¶å°±å‡†å¤‡å¥½äº†ã€‚
 ä¸‹ä¸€æ­¥å°±æ˜¯éƒ¨ç½² **Fluent Bit DaemonSet**ï¼Œå®ƒä¼šåœ¨æ¯ä¸ªèŠ‚ç‚¹è¿è¡Œå¹¶é‡‡é›†æ—¥å¿—ã€‚



å¥½ï¼Œæˆ‘ä»¬æœ€åä¸€æ­¥æ¥å†™ **Fluent Bit DaemonSet**ï¼Œè¿™æ ·å°±èƒ½åœ¨èŠ‚ç‚¹ä¸Šé‡‡é›†å®¹å™¨æ—¥å¿—å¹¶å‘é€åˆ° Elasticsearchã€‚

------

#### 5.3 åˆ›å»º Fluent Bit DaemonSet

æ–°å»ºæ–‡ä»¶ **`fluent-bit-daemonset.yaml`**ï¼š

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: efk
  labels:
    app.kubernetes.io/name: fluent-bit
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: fluent-bit
  template:
    metadata:
      labels:
        app.kubernetes.io/name: fluent-bit
    spec:
      serviceAccountName: fluent-bit
      securityContext:
        runAsUser: 0
        runAsGroup: 0
        fsGroup: 0
      tolerations:
        - key: "node-role.kubernetes.io/control-plane"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
        - name: fluent-bit
          image: fluent/fluent-bit:2.9.9
          args: ["-c", "/fluent-bit/etc/fluent-bit.conf"]
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 500m
              memory: 256Mi
          volumeMounts:
            - name: varlog
              mountPath: /var/log
            - name: config
              mountPath: /fluent-bit/etc/
      terminationGracePeriodSeconds: 10
      volumes:
        - name: varlog
          hostPath:
            path: /var/log
            type: Directory
        - name: config
          configMap:
            name: fluent-bit-config
            defaultMode: 420

```

åº”ç”¨ï¼š

```
kubectl apply -f fluent-bit-daemonset.yaml
```

------

### ğŸ” éªŒè¯æ—¥å¿—é‡‡é›†

1. æŸ¥çœ‹ Pod æ˜¯å¦æ­£å¸¸è¿è¡Œï¼š

```
kubectl -n efk get pods -o wide
```

1. æŸ¥çœ‹ Fluent Bit æ—¥å¿—ç¡®è®¤è¾“å‡ºåˆ° ESï¼š

```
kubectl -n efk logs -l app.kubernetes.io/name=fluent-bit
```

1. æ£€æŸ¥ Elasticsearch æ˜¯å¦æœ‰æ–°ç´¢å¼•ï¼š

```
kubectl -n efk port-forward svc/elasticsearch 9200:9200 &
curl 'http://127.0.0.1:9200/_cat/indices?v'
```

åº”è¯¥èƒ½çœ‹åˆ° `kubernetes-*` å¼€å¤´çš„ç´¢å¼•ã€‚

1. æ‰“å¼€ Kibanaï¼ˆhttp://<ä½ çš„èŠ‚ç‚¹IP>:30601ï¼‰ï¼Œåœ¨ **Stack Management â†’ Index Patterns** é‡Œæ·»åŠ  `kubernetes-*`ï¼Œé€‰æ‹© `@timestamp` å­—æ®µï¼Œå°±èƒ½å¼€å§‹åœ¨ **Discover** é‡ŒæŸ¥çœ‹æ—¥å¿—äº† ğŸ‰

------

âœ… åˆ°è¿™é‡Œï¼Œä¸€æ•´å¥— **EFK æ—¥å¿—ç³»ç»Ÿ** å°±éƒ¨ç½²å®Œæˆäº†ã€‚



æ‰§è¡ŒæŠ¥`kubectl -n efk port-forward svc/elasticsearch 9200:9200 &`æŠ¥é”™

``` shell
[root@k8s-master efk]# kubectl -n efk port-forward svc/elasticsearch 9200:9200 &
[1] 67634
[root@k8s-master efk]# Forwarding from 127.0.0.1:9200 -> 9200
Handling connection for 9200
E0825 16:44:40.755824   67634 portforward.go:424] "Unhandled Error" err="an error occurred forwarding 9200 -> 9200: error forwarding port 9200 to pod 95b8653b6a8b5d4382d72af972b42cdc3a967c0cb50429cbd00ca703b424a510, uid : unable to do port forwarding: socat not found"
error: lost connection to pod

```

è§£å†³æ–¹æ¡ˆï¼š

``` shell
# è¯´æ˜ Kubernetes æœ¬åœ°çš„ port-forward åŠŸèƒ½éœ€è¦ socat æ¥è½¬å‘ç«¯å£ï¼Œä½†ä½ çš„ç³»ç»Ÿé‡Œæ²¡æœ‰å®‰è£…å®ƒã€‚
yum install -y socat
```

