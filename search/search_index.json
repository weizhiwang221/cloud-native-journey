{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Slogan \ud83d\udee1\ufe0f \u4e0d\u8981\u88ab\u654c\u4eba\u7684\u6c14\u52bf\u6c79\u6c79\u6240\u5413\u5012 \ud83d\udcaa \u4e0d\u8981\u88ab\u5c1a\u80fd\u5fcd\u8010\u7684\u56f0\u96be\u6240\u6cae\u4e27 \ud83c\udf31 \u4e0d\u8981\u88ab\u4e00\u65f6\u7684\u632b\u6298\u6240\u7070\u5fc3 \ud83e\udded \u9053\u8def\u662f\u66f2\u6298\u7684\uff0c\u524d\u9014\u662f\u5149\u660e\u7684 \ud83c\udf05 \u9ed1\u6697\u5373\u5c06\u8fc7\u53bb\uff0c\u66d9\u5149\u5c31\u5728\u773c\u524d \u2728 \u6709\u5229\u7684\u6761\u4ef6\u548c\u4e3b\u52a8\u7684\u6062\u590d\uff0c\u4ea7\u751f\u4e8e\u201c\u518d\u575a\u6301\u4e00\u4e0b\u201d\u7684\u52aa\u529b\u4e4b\u4e2d\u3002 \u5927\u5bb6\u597d\uff0c\u6211\u662f\u4e00\u540d\u4e91\u8ba1\u7b97\u8fd0\u7ef4\u5de5\u7a0b\u5e08\uff0c\u8fd9\u662f\u6211\u7684\u535a\u5ba2\uff0c\u6211\u6253\u7b97\u5728\u6b64\u66f4\u65b0\u6211\u7684\u6280\u672f\u7ecf\u9a8c\u4e0e\u5927\u5bb6\u4e00\u8d77\u5206\u4eab\u3001\u63a2\u8ba8\u5173\u4e8e\u8fd0\u7ef4\u7684\u77e5\u8bc6\uff0c\u5e0c\u671b\u80fd\u591f\u4e92\u76f8\u6210\u957f\u3002 \ud83d\udcd6 \u76ee\u5f55\u6982\u89c8 Kubernetes \u4e91\u5e73\u53f0 CI/CD \u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801 \u76d1\u63a7\u4e0e\u65e5\u5fd7 \u9879\u76ee\u5b9e\u8df5 \u6545\u969c\u6392\u67e5 \u5b66\u4e60\u8d44\u6e90 \u8054\u7cfb\u65b9\u5f0f \ud83d\ude80 \u5185\u5bb9\u5bfc\u822a \u2638\ufe0f Kubernetes \u6838\u5fc3\u6982\u5ff5 - Pods, Deployments, Services \u7b49 \u7f51\u7edc\u8be6\u89e3 - CNI, Service, Ingress, \u7f51\u7edc\u7b56\u7565 \u5b58\u50a8\u7ba1\u7406 - PV, PVC, StorageClass, CSI \u5b89\u5168\u5b9e\u8df5 - RBAC, ServiceAccounts, \u5b89\u5168\u4e0a\u4e0b\u6587 \u8fd0\u7ef4\u7ba1\u7406 - Helm, Operator, \u5347\u7ea7\u7b56\u7565 \u9ad8\u7ea7\u8c03\u5ea6 - \u4eb2\u548c\u6027/\u53cd\u4eb2\u548c\u6027, \u6c61\u70b9\u4e0e\u5bb9\u5fcd \u2601\ufe0f \u4e91\u5e73\u53f0 AWS EKS \u5b9e\u6218 IAM \u6743\u9650\u7ba1\u7406 RDS \u4e0e\u5b58\u50a8\u670d\u52a1 Azure AKS \u90e8\u7f72\u6307\u5357 GCP GKE \u5165\u95e8 \ud83d\udd01 CI/CD GitHub Actions \u5b9e\u6218 GitLab CI \u5b8c\u6574\u6d41\u7a0b ArgoCD GitOps \u5b9e\u8df5 Jenkins Pipeline \u8fdb\u9636 \ud83d\udcdc \u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801 Terraform \u6a21\u5757\u5316\u5b9e\u8df5 Ansible \u6700\u4f73\u5b9e\u8df5 \ud83d\udcca \u76d1\u63a7\u4e0e\u65e5\u5fd7 Prometheus + Grafana \u5168\u65b9\u4f4d\u76d1\u63a7 Loki \u65e5\u5fd7\u7cfb\u7edf\u5b9e\u6218 \u544a\u8b66\u914d\u7f6e\u4e0e\u7ba1\u7406 \ud83d\udee0\ufe0f \u9879\u76ee\u5b9e\u8df5 \u9879\u76ee\u4e00\uff1a\u57fa\u4e8eK8s\u7684\u5fae\u670d\u52a1\u7535\u5546\u5e73\u53f0 \u9879\u76ee\u4e8c\uff1aTerraform\u81ea\u52a8\u5316\u90e8\u7f72EKS\u96c6\u7fa4 \u9879\u76ee\u4e09\uff1aGitOps\u6d41\u6c34\u7ebf\u5b9e\u6218 \ud83d\udd27 \u6545\u969c\u6392\u67e5 Kubernetes \u5e38\u89c1\u95ee\u9898\u6392\u67e5 \u7f51\u7edc\u95ee\u9898\u6392\u67e5\u6307\u5357 \u5b58\u50a8\u95ee\u9898\u6392\u67e5 \u6027\u80fd\u8c03\u4f18\u6848\u4f8b \u63a8\u8350\u4e66\u7c4d \u300aKubernetes\u6743\u5a01\u6307\u5357\u300b \u300a\u4e91\u539f\u751f\u6a21\u5f0f\u300b \u300aSite Reliability Engineering\u300b \ud83d\udcde \u8054\u7cfb\u65b9\u5f0f GitHub\uff1a cloud-native-journey \u90ae\u7bb1\uff1a \u7f51\u6613\u90ae\u7bb1 \ud83d\udcc4 \u6211\u7684\u7b80\u5386 \u6b22\u8fce\u4e0b\u8f7d\u6211\u7684\u6700\u65b0\u7b80\u5386\uff0c\u4e86\u89e3\u6211\u7684\u4e13\u4e1a\u6280\u80fd\u548c\u5de5\u4f5c\u7ecf\u9a8c\u3002 \ud83d\udcc4 \u67e5\u770b\u7b80\u5386 (PDF) .resume-button { display: inline-block; background-color: #3b82f6; color: white; padding: 12px 24px; border-radius: 6px; text-decoration: none; font-weight: bold; margin: 10px 0; transition: background-color 0.3s; } .resume-button:hover { background-color: #2563eb; color: white; text-decoration: none; }","title":"\u9996\u9875"},{"location":"#slogan","text":"\ud83d\udee1\ufe0f \u4e0d\u8981\u88ab\u654c\u4eba\u7684\u6c14\u52bf\u6c79\u6c79\u6240\u5413\u5012 \ud83d\udcaa \u4e0d\u8981\u88ab\u5c1a\u80fd\u5fcd\u8010\u7684\u56f0\u96be\u6240\u6cae\u4e27 \ud83c\udf31 \u4e0d\u8981\u88ab\u4e00\u65f6\u7684\u632b\u6298\u6240\u7070\u5fc3 \ud83e\udded \u9053\u8def\u662f\u66f2\u6298\u7684\uff0c\u524d\u9014\u662f\u5149\u660e\u7684 \ud83c\udf05 \u9ed1\u6697\u5373\u5c06\u8fc7\u53bb\uff0c\u66d9\u5149\u5c31\u5728\u773c\u524d \u2728 \u6709\u5229\u7684\u6761\u4ef6\u548c\u4e3b\u52a8\u7684\u6062\u590d\uff0c\u4ea7\u751f\u4e8e\u201c\u518d\u575a\u6301\u4e00\u4e0b\u201d\u7684\u52aa\u529b\u4e4b\u4e2d\u3002 \u5927\u5bb6\u597d\uff0c\u6211\u662f\u4e00\u540d\u4e91\u8ba1\u7b97\u8fd0\u7ef4\u5de5\u7a0b\u5e08\uff0c\u8fd9\u662f\u6211\u7684\u535a\u5ba2\uff0c\u6211\u6253\u7b97\u5728\u6b64\u66f4\u65b0\u6211\u7684\u6280\u672f\u7ecf\u9a8c\u4e0e\u5927\u5bb6\u4e00\u8d77\u5206\u4eab\u3001\u63a2\u8ba8\u5173\u4e8e\u8fd0\u7ef4\u7684\u77e5\u8bc6\uff0c\u5e0c\u671b\u80fd\u591f\u4e92\u76f8\u6210\u957f\u3002","title":"Slogan"},{"location":"#_1","text":"Kubernetes \u4e91\u5e73\u53f0 CI/CD \u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801 \u76d1\u63a7\u4e0e\u65e5\u5fd7 \u9879\u76ee\u5b9e\u8df5 \u6545\u969c\u6392\u67e5 \u5b66\u4e60\u8d44\u6e90 \u8054\u7cfb\u65b9\u5f0f","title":"\ud83d\udcd6 \u76ee\u5f55\u6982\u89c8"},{"location":"#_2","text":"","title":"\ud83d\ude80 \u5185\u5bb9\u5bfc\u822a"},{"location":"#kubernetes","text":"\u6838\u5fc3\u6982\u5ff5 - Pods, Deployments, Services \u7b49 \u7f51\u7edc\u8be6\u89e3 - CNI, Service, Ingress, \u7f51\u7edc\u7b56\u7565 \u5b58\u50a8\u7ba1\u7406 - PV, PVC, StorageClass, CSI \u5b89\u5168\u5b9e\u8df5 - RBAC, ServiceAccounts, \u5b89\u5168\u4e0a\u4e0b\u6587 \u8fd0\u7ef4\u7ba1\u7406 - Helm, Operator, \u5347\u7ea7\u7b56\u7565 \u9ad8\u7ea7\u8c03\u5ea6 - \u4eb2\u548c\u6027/\u53cd\u4eb2\u548c\u6027, \u6c61\u70b9\u4e0e\u5bb9\u5fcd","title":"\u2638\ufe0f Kubernetes"},{"location":"#_3","text":"AWS EKS \u5b9e\u6218 IAM \u6743\u9650\u7ba1\u7406 RDS \u4e0e\u5b58\u50a8\u670d\u52a1 Azure AKS \u90e8\u7f72\u6307\u5357 GCP GKE \u5165\u95e8","title":"\u2601\ufe0f \u4e91\u5e73\u53f0"},{"location":"#cicd","text":"GitHub Actions \u5b9e\u6218 GitLab CI \u5b8c\u6574\u6d41\u7a0b ArgoCD GitOps \u5b9e\u8df5 Jenkins Pipeline \u8fdb\u9636","title":"\ud83d\udd01 CI/CD"},{"location":"#_4","text":"Terraform \u6a21\u5757\u5316\u5b9e\u8df5 Ansible \u6700\u4f73\u5b9e\u8df5","title":"\ud83d\udcdc \u57fa\u7840\u8bbe\u65bd\u5373\u4ee3\u7801"},{"location":"#_5","text":"Prometheus + Grafana \u5168\u65b9\u4f4d\u76d1\u63a7 Loki \u65e5\u5fd7\u7cfb\u7edf\u5b9e\u6218 \u544a\u8b66\u914d\u7f6e\u4e0e\u7ba1\u7406","title":"\ud83d\udcca \u76d1\u63a7\u4e0e\u65e5\u5fd7"},{"location":"#_6","text":"\u9879\u76ee\u4e00\uff1a\u57fa\u4e8eK8s\u7684\u5fae\u670d\u52a1\u7535\u5546\u5e73\u53f0 \u9879\u76ee\u4e8c\uff1aTerraform\u81ea\u52a8\u5316\u90e8\u7f72EKS\u96c6\u7fa4 \u9879\u76ee\u4e09\uff1aGitOps\u6d41\u6c34\u7ebf\u5b9e\u6218","title":"\ud83d\udee0\ufe0f \u9879\u76ee\u5b9e\u8df5"},{"location":"#_7","text":"Kubernetes \u5e38\u89c1\u95ee\u9898\u6392\u67e5 \u7f51\u7edc\u95ee\u9898\u6392\u67e5\u6307\u5357 \u5b58\u50a8\u95ee\u9898\u6392\u67e5 \u6027\u80fd\u8c03\u4f18\u6848\u4f8b","title":"\ud83d\udd27 \u6545\u969c\u6392\u67e5"},{"location":"#_8","text":"\u300aKubernetes\u6743\u5a01\u6307\u5357\u300b \u300a\u4e91\u539f\u751f\u6a21\u5f0f\u300b \u300aSite Reliability Engineering\u300b","title":"\u63a8\u8350\u4e66\u7c4d"},{"location":"#_9","text":"GitHub\uff1a cloud-native-journey \u90ae\u7bb1\uff1a \u7f51\u6613\u90ae\u7bb1","title":"\ud83d\udcde \u8054\u7cfb\u65b9\u5f0f"},{"location":"#_10","text":"\u6b22\u8fce\u4e0b\u8f7d\u6211\u7684\u6700\u65b0\u7b80\u5386\uff0c\u4e86\u89e3\u6211\u7684\u4e13\u4e1a\u6280\u80fd\u548c\u5de5\u4f5c\u7ecf\u9a8c\u3002 \ud83d\udcc4 \u67e5\u770b\u7b80\u5386 (PDF) .resume-button { display: inline-block; background-color: #3b82f6; color: white; padding: 12px 24px; border-radius: 6px; text-decoration: none; font-weight: bold; margin: 10px 0; transition: background-color 0.3s; } .resume-button:hover { background-color: #2563eb; color: white; text-decoration: none; }","title":"\ud83d\udcc4 \u6211\u7684\u7b80\u5386"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/","text":"\u642d\u5efa\u4f01\u4e1a\u7ea7 DevOps \u5e73\u53f0 1\u3001\u73af\u5883\u51c6\u5907 \u4e3b\u673a\u540d \u4e3b\u673aIP \u63cf\u8ff0 \u64cd\u4f5c\u7cfb\u7edf \u5185\u6838\u7248\u672c \u673a\u5668\u914d\u7f6e gitlab-31-24 192.168.31.24 \u4f7f\u7528docker\u90e8\u7f72gitlab Rocky Linux 9.0 (Blue Onyx) Linux 5.14.0-70.13.1.el9_0.x86_64 8C 8G 100G Jenkins-31-171 192.168.31.171 \u4f7f\u7528docker\u90e8\u7f72jenkins Rocky Linux 9.0 (Blue Onyx) Linux 5.14.0-70.13.1.el9_0.x86_64 4C 4G 100G k8s-31-54 192.168.31.54 \u4f7f\u7528kubeadm\u90e8\u7f72k8s\u5355\u8282\u70b9\u96c6\u7fa4 Rocky Linux 9.0 (Blue Onyx) Linux 5.14.0-70.13.1.el9_0.x86_64 4C 4G 100G \u524d\u7f6e\u6761\u4ef6 \u5173\u95ed\u9632\u706b\u5899\u548cselinux systemctl disable firewalld.service --now sed -i 's@SELINUX=enabled@SELINUX=disabled@g' /etc/selinux/config # \u91cd\u542f\u751f\u6548\u6240\u4ee5\u4e34\u65f6\u8bbe\u7f6e\u4e3a\u5bbd\u5bb9\u6a21\u5f0f setenforce 0 reboot \u56e0\u4e3a\u865a\u62df\u673a\u91c7\u7528\u7684\u7f51\u7edc\u6a21\u5f0f\u662f\u6865\u63a5\uff0c\u4e3a\u4e86\u9632\u6b62IP\u81ea\u52a8\u5206\u914d\uff0c\u8bf7\u624b\u52a8\u8bbe\u7f6eIP\u5730\u5740\u3002\u4e09\u53f0\u540c\u6837\u914d\u7f6e # Rocky \u64cd\u4f5c\u7cfb\u7edf\u4f7f\u7528NetworkManager\u7ba1\u7406\u7f51\u5361\uff0c\u7f16\u8f91\u7f51\u5361\u6587\u4ef6 vim /etc/NetworkManager/system-connections/ens33.nmconnection [connection] id=ens33 uuid=22b59732-7826-371f-8b8d-f20c5d47bcce type=ethernet autoconnect-priority=-999 interface-name=ens33 timestamp=1755509104 [ethernet] [ipv4] method=manual # auto\u6539\u4e3a manual\uff0c\u6ce8\u610f\u540e\u9762\u4e0d\u80fd\u6709\u7a7a\u683c address=192.168.31.54/24,192.168.31.1 # IP\u5730\u5740/\u5b50\u7f51\u63a9\u7801,\u7f51\u5173\uff0c\u6ce8\u610f\u540e\u9762\u4e0d\u80fd\u6709\u7a7a\u683c dns=8.8.8.8 # DNS\u670d\u52a1\u5668\uff0c\u6ce8\u610f\u540e\u9762\u4e0d\u80fd\u6709\u7a7a\u683c [ipv6] addr-gen-mode=eui64 method=auto [proxy] # \u4fdd\u5b58\u9000\u51fa\uff0c\u5e76\u91cd\u542f\u7f51\u5361 nmcli connection reload && nmcli connection down ens33 && nmcli connection up ens33 \u90e8\u7f72\u76ee\u5f55 /usr/local/ \u8f6f\u4ef6\u7248\u672c \u7ec4\u4ef6 \u63a8\u8350\u7248\u672c \u8bf4\u660e GitLab CE 17.5.0-ce.0 \u4f60\u5df2\u7ecf\u62c9\u53d6\u7684\u7248\u672c\uff0c\u5f53\u524d LTS\uff0c\u652f\u6301\u65b0\u529f\u80fd\u548c\u5b89\u5168\u8865\u4e01\u3002 Jenkins 2.462.3 LTS Jenkins \u5b98\u65b9 LTS \u7a33\u5b9a\u5206\u652f\uff0c\u63a8\u8350\u4f01\u4e1a\u7528 LTS\uff0c\u800c\u4e0d\u662f weekly\u3002 Harbor 2.11.0 \u6700\u65b0\u7a33\u5b9a\u7248\uff0c\u652f\u6301 OCI Artifact\u3001Trivy \u5b89\u5168\u626b\u63cf\uff0c\u517c\u5bb9 Docker 20+ \u548c K8s 1.32\u3002 SonarQube 10.6.0 LTS \u957f\u671f\u652f\u6301\u7248\u672c\uff0c\u517c\u5bb9 JDK 17+\uff0c\u9002\u5408\u548c Jenkins Pipeline \u96c6\u6210\u3002 Maven 3.8.9 \u6700\u65b0\u7a33\u5b9a\u7248\uff08\u4e0d\u662f 3.8.5\uff0c\u5b98\u65b9\u5df2\u66f4\u65b0\u5230 3.8.8\uff09\uff0c\u4fee\u590d\u4e86 3.8.5 \u7684\u90e8\u5206\u4f9d\u8d56\u89e3\u6790\u95ee\u9898\u3002 OpenJDK 17 (LTS) \u5efa\u8bae\u4f7f\u7528 JDK 17\uff08\u76ee\u524d\u7684 LTS\uff0c\u652f\u6301\u5230 2029\uff09\uff0cMaven 3.8.x\u3001Jenkins\u3001SonarQube \u90fd\u517c\u5bb9\uff1b\u4e0d\u5efa\u8bae JDK 21\uff08\u592a\u65b0\uff0c\u90e8\u5206\u63d2\u4ef6\u672a\u5b8c\u5168\u9002\u914d\uff09\u3002 Kubernetes 1.32.8 \u4f60\u5df2\u5b89\u88c5\uff0c\u5c5e\u6700\u65b0\u7a33\u5b9a\u5206\u652f\u3002 Docker CE 28.3.3 \u4f60\u5df2\u5b89\u88c5\uff0cOK\u3002 docker-compose v2.27.0 \u4f60\u5df2\u5b89\u88c5\uff0cOK\u3002 2\u3001\u5b89\u88c5Docker \u914d\u7f6eYUM\u6e90 \u914d\u7f6eyum\u6e90 \uff081\uff09\u786e\u8ba4\u6587\u4ef6\u662f\u5426\u5b58\u5728\u4e14\u53ef\u8bfb sudo cat /etc/yum.repos.d/rocky.repo \u5982\u679c\u6587\u4ef6\u4e0d\u5b58\u5728\u6216\u5185\u5bb9\u4e3a\u7a7a\uff0c\u91cd\u65b0\u521b\u5efa\u5b83\u3002 \uff082\uff09\u91cd\u65b0\u4e0b\u8f7d\u6b63\u786e\u7684\u963f\u91cc\u4e91\u6e90\u6587\u4ef6 sudo rm -f /etc/yum.repos.d/rocky.repo # \u5220\u9664\u65e7\u6587\u4ef6\uff08\u5982\u679c\u6709\uff09 sudo curl -o /etc/yum.repos.d/rocky.repo https://mirrors.aliyun.com/rockylinux/rocky.repo?repo=rocky-9 \uff083\uff09\u624b\u52a8\u7f16\u8f91\u6587\u4ef6\uff08\u5982\u679c\u4e0b\u8f7d\u5931\u8d25\uff09 sudo vi /etc/yum.repos.d/rocky.repo \u7c98\u8d34\u4ee5\u4e0b\u5185\u5bb9\uff08\u963f\u91cc\u4e91 Rocky Linux 9 \u955c\u50cf\u6e90\uff09\uff1a [baseos] name=Rocky Linux $releasever - BaseOS - Aliyun baseurl=https://mirrors.aliyun.com/rockylinux/$releasever/BaseOS/$basearch/os/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficial [appstream] name=Rocky Linux $releasever - AppStream - Aliyun baseurl=https://mirrors.aliyun.com/rockylinux/$releasever/AppStream/$basearch/os/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficial [extras] name=Rocky Linux $releasever - Extras - Aliyun baseurl=https://mirrors.aliyun.com/rockylinux/$releasever/extras/$basearch/os/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficial \uff084\uff09\u4e5f\u53ef\u4ee5\u76f4\u63a5\u66ff\u6362yum\u6e90\u91cc\u7684\u5730\u5740 sed -e 's|^mirrorlist=|#mirrorlist=|g' \\ -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\ -i.bak \\ /etc/yum.repos.d/rocky*.repo \u5f3a\u5236\u66ff\u6362\u53d8\u91cf\u4e3a Rocky Linux 9 \u786e\u4fdd $releasever \u548c $basearch \u88ab\u6b63\u786e\u89e3\u6790\uff1a sudo sed -i 's/$releasever/9/g' /etc/yum.repos.d/rocky.repo sudo sed -i 's/$basearch/x86_64/g' /etc/yum.repos.d/rocky.repo # \u5982\u679c\u662f x86_64 \u67b6\u6784 \u5bfc\u5165 GPG \u5bc6\u94a5 bash sudo rpm --import https://mirrors.aliyun.com/rockylinux/RPM-GPG-KEY-rockyofficial \u68c0\u67e5\u6587\u4ef6\u6743\u9650\u548c\u683c\u5f0f \uff081\uff09\u786e\u4fdd\u6587\u4ef6\u6743\u9650\u6b63\u786e bash sudo chmod 644 /etc/yum.repos.d/rocky.repo \uff082\uff09\u68c0\u67e5\u6587\u4ef6\u683c\u5f0f\uff08\u907f\u514d UTF-8 BOM \u6216 Windows \u6362\u884c\u7b26\uff09 ```bash sudo dos2unix /etc/yum.repos.d/rocky.repo # \u5982\u679c\u662f\u4ece Windows \u590d\u5236\u7684\u6587\u4ef6 ``` \u6e05\u9664\u7f13\u5b58\u5e76\u91cd\u65b0\u52a0\u8f7d bash sudo dnf clean all sudo dnf makecache \u9a8c\u8bc1\u4ed3\u5e93\u662f\u5426\u542f\u7528 bash sudo dnf repolist \u6b63\u5e38\u8f93\u51fa\u5e94\u7c7b\u4f3c\uff1a text repo id repo name baseos Rocky Linux 9 - BaseOS - Aliyun appstream Rocky Linux 9 - AppStream - Aliyun extras Rocky Linux 9 - Extras - Aliyun \u5728 Rocky Linux 9 \u4e2d\u542f\u7528\u5e76\u5b89\u88c5 EPEL Repo\u3002 dnf install epel-release \u5907\u4efd(\u5982\u6709\u914d\u7f6e\u5176\u4ed6epel\u6e90)\u5e76\u66ff\u6362\u4e3a\u56fd\u5185\u955c\u50cf \u6ce8\u610f\u6700\u540e\u8fd9\u4e2a\u5e93\uff0c\u963f\u91cc\u4e91\u6ca1\u6709\u5bf9\u5e94\u7684\u955c\u50cf\uff0c\u4e0d\u8981\u4fee\u6539\u5b83\uff0c\u5982\u679c\u8bef\u6539\u6062\u590d\u539f\u7248\u6e90\u5373\u53ef cp /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backup cp /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backup cp /etc/yum.repos.d/epel-cisco-openh264.repo /etc/yum.repos.d/epel-cisco-openh264.repo.backup \u5c06 repo \u914d\u7f6e\u4e2d\u7684\u5730\u5740\u66ff\u6362\u4e3a\u963f\u91cc\u4e91\u955c\u50cf\u7ad9\u5730\u5740 \u6267\u884c\u4e0b\u9762\u8bed\u53e5\uff0c\u5b83\u4f1a\u66ff\u6362epel.repo\u3001eple-testing.repo\u4e2d\u7684\u7f51\u5740\uff0c\u4e0d\u4f1a\u4fee\u6539epel-cisco-openh264.repo\uff0c\u53ef\u4ee5\u6b63\u5e38\u4f7f\u7528\u3002 sed -e 's!^metalink=!#metalink=!g' \\ -e 's!^#baseurl=!baseurl=!g' \\ -e 's!https\\?://download\\.fedoraproject\\.org/pub/epel!https://mirrors.aliyun.com/epel!g' \\ -e 's!https\\?://download\\.example/pub/epel!https://mirrors.aliyun.com/epel!g' \\ -i /etc/yum.repos.d/epel{,-testing}.repo \u66f4\u65b0\u4ed3\u5e93\u7f13\u5b58 dnf clean all dnf makecache ---\u751f\u6210\u7f13\u5b58\uff0c\u5b89\u88c5\u8f6f\u4ef6\u66f4\u5feb \u67e5\u770bdocker\u7248\u672c # Step 1: \u5b89\u88c5\u4f9d\u8d56 yum install -y yum-utils device-mapper-persistent-data lvm2 # Step 2: \u6dfb\u52a0\u8f6f\u4ef6\u6e90\u4fe1\u606f yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/rhel/docker-ce.repo # Step 3: \u5b89\u88c5Docker-CE\uff0c\u67e5\u8be2\u5b89\u88c5\u7684\u7248\u672c dnf list docker-ce --showduplicates | sort -r docker-ce.x86_64 3:28.3.3-1.el9 docker-ce-stable docker-ce.x86_64 3:28.3.2-1.el9 docker-ce-stable docker-ce.x86_64 3:28.3.1-1.el9 docker-ce-stable docker-ce.x86_64 3:28.3.0-1.el9 docker-ce-stable # dnf install docker-ce-<VERSION_STRING> docker-ce-cli-<VERSION_STRING> containerd.io docker-buildx-plugin docker-compose-plugin dnf install docker-ce-28.3.3 docker-ce-cli-28.3.3 containerd.io docker-buildx-plugin docker-compose-plugin -y # docker -v Docker version 28.3.3, build 980b856 # \u8bbe\u7f6e\u56fd\u5185\u955c\u50cf\u52a0\u901f mkdir -p /etc/docker/ cat >> /etc/docker/daemon.json << EOF { \"registry-mirrors\":[\"https://p3kgr6db.mirror.aliyuncs.com\", \"https://docker.m.daocloud.io\", \"https://your_id.mirror.aliyuncs.com\", \"https://docker.nju.edu.cn/\", \"https://docker.anyhub.us.kg\", \"https://dockerhub.jobcher.com\", \"https://dockerhub.icu\", \"https://docker.ckyl.me\", \"https://cr.console.aliyun.com\" ], \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF # \u8bbe\u7f6edocker\u5f00\u673a\u542f\u52a8\u5e76\u542f\u52a8 systemctl daemon-reload systemctl restart docker systemctl enable --now docker # \u67e5\u770bdocker\u7248\u672c [root@gitlab-31-24 ~]# docker version Client: Docker Engine - Community Version: 28.3.3 API version: 1.51 Go version: go1.24.5 Git commit: 980b856 Built: Fri Jul 25 11:36:28 2025 OS/Arch: linux/amd64 Context: default Server: Docker Engine - Community Engine: Version: 28.3.3 API version: 1.51 (minimum version 1.24) Go version: go1.24.5 Git commit: bea959c Built: Fri Jul 25 11:33:28 2025 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.7.27 GitCommit: 05044ec0a9a75232cad458027ca83437aae3f4da runc: Version: 1.2.5 GitCommit: v1.2.5-0-g59923ef docker-init: Version: 0.19.0 GitCommit: de40ad0 \u5b89\u88c5docker-compose # \u4ecegithub\u4e0a\u4e0b\u8f7d\u5305\uff0c\u4e0a\u4f20\u5230\u670d\u52a1\u5668\u7684/usr/local/bin/\u4e0b https://github.com/docker/compose/releases/download/v2.27.0/docker-compose-linux-x86_64 mv /usr/local/bin/docker-compose-linux-x86_64 /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose version Docker Compose version v2.27.0 3\u3001\u5b89\u88c5gitlab \u62c9\u53d6gitlab\u955c\u50cf docker pull gitlab/gitlab-ce:17.5.0-ce.0 \u521b\u5efa\u5b89\u88c5\u76ee\u5f55\uff0c\u5e76\u521b\u5efadocker-compose.yaml\u6587\u4ef6 mkdir -p /usr/local/docker/docker_gitlab cd /usr/local/docker/docker_gitlab vim docker-compose.yaml services: gitlab: image: 'gitlab/gitlab-ce:17.5.0-ce.0' container_name: gitlab restart: always hostname: '192.168.31.24' environment: GITLAB_OMNIBUS_CONFIG: | external_url 'http://192.168.31.24:8929' gitlab_rails['gitlab_shell_ssh_port'] = 2224 ports: - '8929:8929' - '2224:22' volumes: - './config:/etc/gitlab' - './logs:/var/log/gitlab' - './data:/var/opt/gitlab' docker-compose up -d # \u542f\u52a8\u6210\u529f\u540e\uff0c\u6d4f\u89c8\u5668\u8bbf\u95eehttp://192.168.31.24:8929 \u5982\u679c\u51fa\u73b0502\u9700\u8981\u5728\u591a\u7b49\u4e00\u4f1a\uff0cgitlab\u542f\u52a8\u6210\u529f \u67e5\u770bgitlab\u5bc6\u7801 # \u8fdb\u5165gitlab \u5bb9\u5668\u5185\u90e8\u67e5\u770b\u5bc6\u7801 docker exec -it gitlab bash root@192:/# cat /etc/gitlab/initial_root_password # WARNING: This value is valid only in the following conditions # 1. If provided manually (either via `GITLAB_ROOT_PASSWORD` environment variable or via `gitlab_rails['initial_root_password']` setting in `gitlab.rb`, it was provided before database was seeded for the first time (usually, the first reconfigure run). # 2. Password hasn't been changed manually, either via UI or via command line. # # If the password shown here doesn't work, you must reset the admin password following https://docs.gitlab.com/ee/security/reset_user_password.html#reset-your-root-password. Password: QeAKBSuBxlrGG0tbksCAIOAErunmqNb2DG99zFNGdck= # NOTE: This file will be automatically deleted in the first reconfigure run after 24 hours. root@192:/# 4\u3001\u5b89\u88c5 Maven\u548cJDK17 Maven\u4e0b\u8f7d\u5730\u5740\uff1a https://dlcdn.apache.org/maven/maven-3/3.8.9/binaries/apache-maven-3.8.9-bin.tar.gz JDK17\u4e0b\u8f7d\u5730\u5740\uff1a https://download.oracle.com/java/17/archive/jdk-17_linux-x64_bin.tar.gz \u4e0a\u4f20\u90e8\u7f72\u5305\u5230\u76ee\u6807\u670d\u52a1\u5668 # \u653e\u5230root\u76ee\u5f55\u4e0b [root@jenkins-31-171 local]# ls -ltr /root/ \u603b\u7528\u91cf 184436 -rw-r--r-- 1 root root 180555480 8\u6708 28 11:20 jdk-17_linux-x64_bin.tar.gz -rw-r--r-- 1 root root 8296518 8\u6708 28 11:23 apache-maven-3.8.9-bin.tar.gz # \u89e3\u538b\u5e76\u6539\u540d tar -zxvf apache-maven-3.8.9-bin.tar.gz -C /usr/local/ tar -zxvf jdk-17_linux-x64_bin.tar.gz -C /usr/local/ cd /usr/local/ mv jdk-17/ jdk mv apache-maven-3.8.9/ maven \u914d\u7f6emaven\u7684\u4ed3\u5e93\u5730\u5740 # \u914d\u7f6e\u6587\u4ef6 vim /usr/local/maven/conf/settings.xml <mirror> <id>aliyunmaven</id> <mirrorOf>*</mirrorOf> <name>\u963f\u91cc\u4e91\u516c\u5171\u4ed3\u5e93</name> <url>https://maven.aliyun.com/repository/public</url> </mirror> \u914d\u7f6eJava JDK # \u914d\u7f6e\u6587\u4ef6 vim /usr/local/maven/conf/settings.xml <profile> <id>jdk17</id> <activation> <activeByDefault>true</activeByDefault> <jdk>17</jdk> </activation> <properties> <maven.compiler.source>17</maven.compiler.source> <maven.compiler.target>17</maven.compiler.target> <maven.compiler.compilerVersion>17</maven.compiler.compilerVersion> </properties> </profile> </profiles> <!-- activeProfiles | List of profiles that are active for all builds. | <activeProfiles> <activeProfile>alwaysActiveProfile</activeProfile> <activeProfile>anotherAlwaysActiveProfile</activeProfile> </activeProfiles> --> <activeProfiles> <activeProfile>jdk17</activeProfile> </activeProfiles> 5\u3001\u5b89\u88c5Jenkins \u767b\u5f55\u5b98\u7f51Jenkins.io\u4e0a\u4e0b\u8f7d\u5bf9\u5e94\u7684Jenkins\u955c\u50cf docker pull jenkins/jenkins:2.462.3-lts \u4f7f\u7528docker-compose\u90e8\u7f72Jenkins mkdir -p /usr/local/docker/jenkins_docker cd /usr/local/docker/jenkins_docker vim docker-compose.yaml services: jenkins: image: jenkins/jenkins:2.462.3-lts container_name: jenkins ports: - 8080:8080 - 50000:50000 volumes: - ./data/:/var/jenkins_home/ [root@jenkins-31-171 jenkins_docker]# docker-compose up -d [+] Running 2/2 \u2714 Network jenkins_docker_default Created 0.0s \u2714 Container jenkins Started 0.3s [root@jenkins-31-171 jenkins_docker]# docker logs -f jenkins INSTALL WARNING: User: missing rw permissions on JENKINS_HOME: /var/jenkins_home touch: cannot touch '/var/jenkins_home/copy_reference_file.log': Permission denied Can not write to /var/jenkins_home/copy_reference_file.log. Wrong volume permissions? [root@jenkins-31-171 jenkins_docker]# ls -tlr \u603b\u7528\u91cf 4 -rw-r--r-- 1 root root 185 8\u6708 28 11:51 docker-compose.yaml drwxr-xr-x 2 root root 6 8\u6708 28 11:51 data [root@jenkins-31-171 jenkins_docker]# chmod -R 777 data [root@jenkins-31-171 jenkins_docker]# docker-compose up -d [root@jenkins-31-171 jenkins_docker]# docker logs -f jenkins INSTALL WARNING: User: missing rw permissions on JENKINS_HOME: /var/jenkins_home touch: cannot touch '/var/jenkins_home/copy_reference_file.log': Permission denied Can not write to /var/jenkins_home/copy_reference_file.log. Wrong volume permissions? Running from: /usr/share/jenkins/jenkins.war webroot: /var/jenkins_home/war 2025-08-28 03:52:19.721+0000 [id=1] INFO winstone.Logger#logInternal: Beginning extraction from war file 2025-08-28 03:52:20.611+0000 [id=1] WARNING o.e.j.s.handler.ContextHandler#setContextPath: Empty contextPath 2025-08-28 03:52:20.664+0000 [id=1] INFO org.eclipse.jetty.server.Server#doStart: jetty-10.0.24; built: 2024-08-26T17:58:21.070Z; git: d5384207795da96fad32db8ea8d26b69955bcc03; jvm 17.0.12+7 2025-08-28 03:52:21.016+0000 [id=1] INFO o.e.j.w.StandardDescriptorProcessor#visitServlet: NO JSP Support for /, did not find org.eclipse.jetty.jsp.JettyJspServlet 2025-08-28 03:52:21.050+0000 [id=1] INFO o.e.j.s.s.DefaultSessionIdManager#doStart: Session workerName=node0 2025-08-28 03:52:21.538+0000 [id=1] INFO hudson.WebAppMain#contextInitialized: Jenkins home directory: /var/jenkins_home found at: EnvVars.masterEnvVars.get(\"JENKINS_HOME\") 2025-08-28 03:52:21.655+0000 [id=1] INFO o.e.j.s.handler.ContextHandler#doStart: Started w.@58860997{Jenkins v2.462.3,/,file:///var/jenkins_home/war/,AVAILABLE}{/var/jenkins_home/war} 2025-08-28 03:52:21.670+0000 [id=1] INFO o.e.j.server.AbstractConnector#doStart: Started ServerConnector@5c530d1e{HTTP/1.1, (http/1.1)}{0.0.0.0:8080} 2025-08-28 03:52:21.684+0000 [id=1] INFO org.eclipse.jetty.server.Server#doStart: Started Server@1c6804cd{STARTING}[10.0.24,sto=0] @2329ms 2025-08-28 03:52:21.686+0000 [id=26] INFO winstone.Logger#logInternal: Winstone Servlet Engine running: controlPort=disabled 2025-08-28 03:52:21.870+0000 [id=34] INFO jenkins.InitReactorRunner$1#onAttained: Started initialization 2025-08-28 03:52:21.888+0000 [id=35] INFO jenkins.InitReactorRunner$1#onAttained: Listed all plugins 2025-08-28 03:52:22.496+0000 [id=34] INFO jenkins.InitReactorRunner$1#onAttained: Prepared all plugins 2025-08-28 03:52:22.499+0000 [id=36] INFO jenkins.InitReactorRunner$1#onAttained: Started all plugins 2025-08-28 03:52:22.507+0000 [id=33] INFO jenkins.InitReactorRunner$1#onAttained: Augmented all extensions 2025-08-28 03:52:22.645+0000 [id=39] INFO jenkins.InitReactorRunner$1#onAttained: System config loaded 2025-08-28 03:52:22.645+0000 [id=39] INFO jenkins.InitReactorRunner$1#onAttained: System config adapted 2025-08-28 03:52:22.646+0000 [id=39] INFO jenkins.InitReactorRunner$1#onAttained: Loaded all jobs 2025-08-28 03:52:22.647+0000 [id=39] INFO jenkins.InitReactorRunner$1#onAttained: Configuration for all jobs updated 2025-08-28 03:52:22.675+0000 [id=52] INFO hudson.util.Retrier#start: Attempt #1 to do the action check updates server 2025-08-28 03:52:22.916+0000 [id=39] INFO jenkins.install.SetupWizard#init: ************************************************************* ************************************************************* ************************************************************* Jenkins initial setup is required. An admin user has been created and a password generated. Please use the following password to proceed to installation: e0127ee5810548959b3b175c3fca7545 This may also be found at: /var/jenkins_home/secrets/initialAdminPassword ************************************************************* ************************************************************* ************************************************************* 2025-08-28 03:52:29.800+0000 [id=39] INFO jenkins.InitReactorRunner$1#onAttained: Completed initialization 2025-08-28 03:52:29.878+0000 [id=25] INFO hudson.lifecycle.Lifecycle#onReady: Jenkins is fully up and running 2025-08-28 03:52:31.681+0000 [id=52] INFO h.m.DownloadService$Downloadable#load: Obtained the updated data file for hudson.tasks.Maven.MavenInstaller 2025-08-28 03:52:31.682+0000 [id=52] INFO hudson.util.Retrier#start: Performed the action check updates server successfully at the attempt #1 Jenkins\u5bc6\u7801\u5728\u65e5\u5fd7\u4e2d e0127ee5810548959b3b175c3fca7545 \u7ee7\u7eed\u4e4b\u540e\uff0c\u9009\u62e9-\u9009\u62e9\u63d2\u4ef6\u6765\u5b89\u88c5-\u4fdd\u6301\u9ed8\u8ba4\u5b89\u88c5 http://192.168.31.171:8080/ 6\u3001\u5b89\u88c5\u63d2\u4ef6 Git Parameter Publish Over SSH 7\u3001\u5728Jenkins\u4e2d\u914d\u7f6eJDK\u548cmaven mv /usr/local/jdk /usr/local/docker/jenkins_docker/data/ mv /usr/local/maven /usr/local/docker/jenkins_docker/data/ \u5728Jenkins\u7684\u5168\u5c40\u914d\u7f6e\u4e2d\uff0c\u624b\u52a8\u6dfb\u52a0\u5b89\u88c5\u7684JDK\uff0cJAVA_HOME \u586b\u5199\u5bb9\u5668\u5185\u7684\u8def\u5f84 \u5728Jenkins\u7684\u5168\u5c40\u914d\u7f6e\u4e2d\uff0c\u624b\u52a8\u6dfb\u52a0\u5b89\u88c5\u7684maven\uff0cMAVEN_HOME \u586b\u5199\u5bb9\u5668\u5185\u7684\u8def\u5f84 \u5728\u7cfb\u7edf\u914d\u7f6e\u4e2d\uff0c\u914d\u7f6e\u8fdc\u7aef\u63a5\u6536\u9879\u76ee\u7684\u670d\u52a1\u5668\uff0c\u91c7\u7528root \u5bc6\u7801\u7684\u65b9\u5f0f\u8fde\u63a5\u3002\u586b\u5199\u76ee\u5f55\u65f6\uff0c\u5fc5\u987b\u5148\u521b\u5efa 8\u3001\u642d\u5efa\u672c\u5730\u73af\u5883 \u8f6f\u4ef6\uff1avscode + maven + git maven\uff1a https://dlcdn.apache.org/maven/maven-3/3.9.11/binaries/apache-maven-3.9.11-bin.zip git\uff1a https://github.com/git-for-windows/git/releases/download/v2.51.0.windows.1/Git-2.51.0-64-bit.exe \u50bb\u74dc\u5f0f\u5b89\u88c5 \u8be6\u60c5\u8bf7\u53c2\u8003\uff1a \u8d85\u8be6\u7ec6\u7684VsCode\u521b\u5efaSpringBoot\u9879\u76ee(\u56fe\u6587\u5e76\u8302)_vscode\u521b\u5efaspring boot\u9879\u76ee-CSDN\u535a\u5ba2 9\u3001\u4f7f\u7528Jenkins\u62c9\u53d6gitlab\u4ee3\u7801 9.1 \u521b\u5efa\u4e00\u4e2a\u81ea\u7531\u98ce\u683c\u7684\u9879\u76ee\uff0c\u5e76\u4e14\u5728\u6e90\u7801\u7ba1\u7406\u4e2d\u6dfb\u52a0git 9.2 \u70b9\u51fb\u7acb\u5373\u6784\u5efa\uff0c\u628agitlab\u4ed3\u5e93\u7684\u4ee3\u7801\u62c9\u5230\u672c\u5730 \u53ef\u4ee5\u5728Jenkins\u670d\u52a1\u5668\u7684data/workspace\u4e2d\u67e5\u770b\u5230 10\u3001\u5728\u62c9\u53d6gitlab\u4ee3\u7801\u7684\u540c\u65f6\uff0c\u63a8\u9001\u5230\u76ee\u6807\u670d\u52a1\u5668 10.1 \u6dfb\u52a0\u6784\u5efa\u64cd\u4f5c \u4f7f\u7528\u5df2\u5b89\u88c5\u7684maven 10.2 \u70b9\u51fb\u7acb\u5373\u6784\u5efa 10.3 \u6dfb\u52a0\u6784\u5efa\u540e\u64cd\u4f5c-\u628ajar\u5305\u63a8\u9001\u5230k8s\u670d\u52a1\u5668 10.4 \u767b\u5f55k8s\u670d\u52a1\u5668\u67e5\u770b\u76ee\u6807\u6587\u4ef6 10.5 \u4f7f\u7528docker-compose\u7684\u65b9\u5f0f\u53d1\u5e03\u670d\u52a1 # \u5728\u9879\u76ee\u4e2d\u521b\u5efadocker\u76ee\u5f55\uff0c\u5e76\u521b\u5efaDockerfile\u6587\u4ef6\u548cdocker-compose.yaml\u6587\u4ef6 # Dockerfile FROM eclipse-temurin:17-jdk-jammy COPY mytest.jar /usr/local/ WORKDIR /usr/local CMD java -jar mytest.jar # docker-compose.yaml services: mytest: build: context: ./ dockerfile: Dockerfile image: mytest:v1.0.0 container_name: mytest ports: - 8081:8080 Jenkins\u914d\u7f6e\u6784\u5efa\u540e\u64cd\u4f5c \u70b9\u51fb\u7acb\u5373\u6784\u5efa\u4e4b\u540e\uff0c\u6d4f\u89c8\u5668\u8bbf\u95ee\u670d\u52a1 11\u3001Jenkins\u57fa\u4e8e\u53c2\u6570\u6784\u5efa 12\u3001Jenkins\u5bb9\u5668\u5185\u90e8\u4f7f\u7528Docker # \u627e\u5230docker.sock\u6587\u4ef6\uff0c\u628a\u6240\u5c5e\u7ec4\u6539\u6210root\u3002\u7ed9\u5176\u4ed6\u4eba\u6dfb\u52a0\u8bfb\u548c\u5199\u6743\u9650 cd /run/ chown root:root docker.sock chmod o+rw docker.sock [root@jenkins-31-171 jenkins_docker]# cat docker-compose.yaml services: jenkins: image: jenkins/jenkins:2.462.3-lts container_name: jenkins ports: - 8080:8080 - 50000:50000 volumes: - ./data/:/var/jenkins_home/ - /run/docker.sock:/run/docker.sock - /usr/bin/docker:/usr/bin/docker - /etc/docker/daemon.json:/etc/docker/daemon.json","title":"\u642d\u5efa\u4f01\u4e1a\u7ea7 DevOps \u5e73\u53f0"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#devops","text":"","title":"\u642d\u5efa\u4f01\u4e1a\u7ea7 DevOps \u5e73\u53f0"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#1","text":"\u4e3b\u673a\u540d \u4e3b\u673aIP \u63cf\u8ff0 \u64cd\u4f5c\u7cfb\u7edf \u5185\u6838\u7248\u672c \u673a\u5668\u914d\u7f6e gitlab-31-24 192.168.31.24 \u4f7f\u7528docker\u90e8\u7f72gitlab Rocky Linux 9.0 (Blue Onyx) Linux 5.14.0-70.13.1.el9_0.x86_64 8C 8G 100G Jenkins-31-171 192.168.31.171 \u4f7f\u7528docker\u90e8\u7f72jenkins Rocky Linux 9.0 (Blue Onyx) Linux 5.14.0-70.13.1.el9_0.x86_64 4C 4G 100G k8s-31-54 192.168.31.54 \u4f7f\u7528kubeadm\u90e8\u7f72k8s\u5355\u8282\u70b9\u96c6\u7fa4 Rocky Linux 9.0 (Blue Onyx) Linux 5.14.0-70.13.1.el9_0.x86_64 4C 4G 100G","title":"1\u3001\u73af\u5883\u51c6\u5907"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#_1","text":"\u5173\u95ed\u9632\u706b\u5899\u548cselinux systemctl disable firewalld.service --now sed -i 's@SELINUX=enabled@SELINUX=disabled@g' /etc/selinux/config # \u91cd\u542f\u751f\u6548\u6240\u4ee5\u4e34\u65f6\u8bbe\u7f6e\u4e3a\u5bbd\u5bb9\u6a21\u5f0f setenforce 0 reboot \u56e0\u4e3a\u865a\u62df\u673a\u91c7\u7528\u7684\u7f51\u7edc\u6a21\u5f0f\u662f\u6865\u63a5\uff0c\u4e3a\u4e86\u9632\u6b62IP\u81ea\u52a8\u5206\u914d\uff0c\u8bf7\u624b\u52a8\u8bbe\u7f6eIP\u5730\u5740\u3002\u4e09\u53f0\u540c\u6837\u914d\u7f6e # Rocky \u64cd\u4f5c\u7cfb\u7edf\u4f7f\u7528NetworkManager\u7ba1\u7406\u7f51\u5361\uff0c\u7f16\u8f91\u7f51\u5361\u6587\u4ef6 vim /etc/NetworkManager/system-connections/ens33.nmconnection [connection] id=ens33 uuid=22b59732-7826-371f-8b8d-f20c5d47bcce type=ethernet autoconnect-priority=-999 interface-name=ens33 timestamp=1755509104 [ethernet] [ipv4] method=manual # auto\u6539\u4e3a manual\uff0c\u6ce8\u610f\u540e\u9762\u4e0d\u80fd\u6709\u7a7a\u683c address=192.168.31.54/24,192.168.31.1 # IP\u5730\u5740/\u5b50\u7f51\u63a9\u7801,\u7f51\u5173\uff0c\u6ce8\u610f\u540e\u9762\u4e0d\u80fd\u6709\u7a7a\u683c dns=8.8.8.8 # DNS\u670d\u52a1\u5668\uff0c\u6ce8\u610f\u540e\u9762\u4e0d\u80fd\u6709\u7a7a\u683c [ipv6] addr-gen-mode=eui64 method=auto [proxy] # \u4fdd\u5b58\u9000\u51fa\uff0c\u5e76\u91cd\u542f\u7f51\u5361 nmcli connection reload && nmcli connection down ens33 && nmcli connection up ens33","title":"\u524d\u7f6e\u6761\u4ef6"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#_2","text":"/usr/local/","title":"\u90e8\u7f72\u76ee\u5f55"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#_3","text":"\u7ec4\u4ef6 \u63a8\u8350\u7248\u672c \u8bf4\u660e GitLab CE 17.5.0-ce.0 \u4f60\u5df2\u7ecf\u62c9\u53d6\u7684\u7248\u672c\uff0c\u5f53\u524d LTS\uff0c\u652f\u6301\u65b0\u529f\u80fd\u548c\u5b89\u5168\u8865\u4e01\u3002 Jenkins 2.462.3 LTS Jenkins \u5b98\u65b9 LTS \u7a33\u5b9a\u5206\u652f\uff0c\u63a8\u8350\u4f01\u4e1a\u7528 LTS\uff0c\u800c\u4e0d\u662f weekly\u3002 Harbor 2.11.0 \u6700\u65b0\u7a33\u5b9a\u7248\uff0c\u652f\u6301 OCI Artifact\u3001Trivy \u5b89\u5168\u626b\u63cf\uff0c\u517c\u5bb9 Docker 20+ \u548c K8s 1.32\u3002 SonarQube 10.6.0 LTS \u957f\u671f\u652f\u6301\u7248\u672c\uff0c\u517c\u5bb9 JDK 17+\uff0c\u9002\u5408\u548c Jenkins Pipeline \u96c6\u6210\u3002 Maven 3.8.9 \u6700\u65b0\u7a33\u5b9a\u7248\uff08\u4e0d\u662f 3.8.5\uff0c\u5b98\u65b9\u5df2\u66f4\u65b0\u5230 3.8.8\uff09\uff0c\u4fee\u590d\u4e86 3.8.5 \u7684\u90e8\u5206\u4f9d\u8d56\u89e3\u6790\u95ee\u9898\u3002 OpenJDK 17 (LTS) \u5efa\u8bae\u4f7f\u7528 JDK 17\uff08\u76ee\u524d\u7684 LTS\uff0c\u652f\u6301\u5230 2029\uff09\uff0cMaven 3.8.x\u3001Jenkins\u3001SonarQube \u90fd\u517c\u5bb9\uff1b\u4e0d\u5efa\u8bae JDK 21\uff08\u592a\u65b0\uff0c\u90e8\u5206\u63d2\u4ef6\u672a\u5b8c\u5168\u9002\u914d\uff09\u3002 Kubernetes 1.32.8 \u4f60\u5df2\u5b89\u88c5\uff0c\u5c5e\u6700\u65b0\u7a33\u5b9a\u5206\u652f\u3002 Docker CE 28.3.3 \u4f60\u5df2\u5b89\u88c5\uff0cOK\u3002 docker-compose v2.27.0 \u4f60\u5df2\u5b89\u88c5\uff0cOK\u3002","title":"\u8f6f\u4ef6\u7248\u672c"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#2docker","text":"","title":"2\u3001\u5b89\u88c5Docker"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#yum","text":"\u914d\u7f6eyum\u6e90 \uff081\uff09\u786e\u8ba4\u6587\u4ef6\u662f\u5426\u5b58\u5728\u4e14\u53ef\u8bfb sudo cat /etc/yum.repos.d/rocky.repo \u5982\u679c\u6587\u4ef6\u4e0d\u5b58\u5728\u6216\u5185\u5bb9\u4e3a\u7a7a\uff0c\u91cd\u65b0\u521b\u5efa\u5b83\u3002 \uff082\uff09\u91cd\u65b0\u4e0b\u8f7d\u6b63\u786e\u7684\u963f\u91cc\u4e91\u6e90\u6587\u4ef6 sudo rm -f /etc/yum.repos.d/rocky.repo # \u5220\u9664\u65e7\u6587\u4ef6\uff08\u5982\u679c\u6709\uff09 sudo curl -o /etc/yum.repos.d/rocky.repo https://mirrors.aliyun.com/rockylinux/rocky.repo?repo=rocky-9 \uff083\uff09\u624b\u52a8\u7f16\u8f91\u6587\u4ef6\uff08\u5982\u679c\u4e0b\u8f7d\u5931\u8d25\uff09 sudo vi /etc/yum.repos.d/rocky.repo \u7c98\u8d34\u4ee5\u4e0b\u5185\u5bb9\uff08\u963f\u91cc\u4e91 Rocky Linux 9 \u955c\u50cf\u6e90\uff09\uff1a [baseos] name=Rocky Linux $releasever - BaseOS - Aliyun baseurl=https://mirrors.aliyun.com/rockylinux/$releasever/BaseOS/$basearch/os/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficial [appstream] name=Rocky Linux $releasever - AppStream - Aliyun baseurl=https://mirrors.aliyun.com/rockylinux/$releasever/AppStream/$basearch/os/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficial [extras] name=Rocky Linux $releasever - Extras - Aliyun baseurl=https://mirrors.aliyun.com/rockylinux/$releasever/extras/$basearch/os/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficial \uff084\uff09\u4e5f\u53ef\u4ee5\u76f4\u63a5\u66ff\u6362yum\u6e90\u91cc\u7684\u5730\u5740 sed -e 's|^mirrorlist=|#mirrorlist=|g' \\ -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\ -i.bak \\ /etc/yum.repos.d/rocky*.repo \u5f3a\u5236\u66ff\u6362\u53d8\u91cf\u4e3a Rocky Linux 9 \u786e\u4fdd $releasever \u548c $basearch \u88ab\u6b63\u786e\u89e3\u6790\uff1a sudo sed -i 's/$releasever/9/g' /etc/yum.repos.d/rocky.repo sudo sed -i 's/$basearch/x86_64/g' /etc/yum.repos.d/rocky.repo # \u5982\u679c\u662f x86_64 \u67b6\u6784 \u5bfc\u5165 GPG \u5bc6\u94a5 bash sudo rpm --import https://mirrors.aliyun.com/rockylinux/RPM-GPG-KEY-rockyofficial \u68c0\u67e5\u6587\u4ef6\u6743\u9650\u548c\u683c\u5f0f \uff081\uff09\u786e\u4fdd\u6587\u4ef6\u6743\u9650\u6b63\u786e bash sudo chmod 644 /etc/yum.repos.d/rocky.repo \uff082\uff09\u68c0\u67e5\u6587\u4ef6\u683c\u5f0f\uff08\u907f\u514d UTF-8 BOM \u6216 Windows \u6362\u884c\u7b26\uff09 ```bash sudo dos2unix /etc/yum.repos.d/rocky.repo # \u5982\u679c\u662f\u4ece Windows \u590d\u5236\u7684\u6587\u4ef6 ``` \u6e05\u9664\u7f13\u5b58\u5e76\u91cd\u65b0\u52a0\u8f7d bash sudo dnf clean all sudo dnf makecache \u9a8c\u8bc1\u4ed3\u5e93\u662f\u5426\u542f\u7528 bash sudo dnf repolist \u6b63\u5e38\u8f93\u51fa\u5e94\u7c7b\u4f3c\uff1a text repo id repo name baseos Rocky Linux 9 - BaseOS - Aliyun appstream Rocky Linux 9 - AppStream - Aliyun extras Rocky Linux 9 - Extras - Aliyun \u5728 Rocky Linux 9 \u4e2d\u542f\u7528\u5e76\u5b89\u88c5 EPEL Repo\u3002 dnf install epel-release \u5907\u4efd(\u5982\u6709\u914d\u7f6e\u5176\u4ed6epel\u6e90)\u5e76\u66ff\u6362\u4e3a\u56fd\u5185\u955c\u50cf \u6ce8\u610f\u6700\u540e\u8fd9\u4e2a\u5e93\uff0c\u963f\u91cc\u4e91\u6ca1\u6709\u5bf9\u5e94\u7684\u955c\u50cf\uff0c\u4e0d\u8981\u4fee\u6539\u5b83\uff0c\u5982\u679c\u8bef\u6539\u6062\u590d\u539f\u7248\u6e90\u5373\u53ef cp /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backup cp /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backup cp /etc/yum.repos.d/epel-cisco-openh264.repo /etc/yum.repos.d/epel-cisco-openh264.repo.backup \u5c06 repo \u914d\u7f6e\u4e2d\u7684\u5730\u5740\u66ff\u6362\u4e3a\u963f\u91cc\u4e91\u955c\u50cf\u7ad9\u5730\u5740 \u6267\u884c\u4e0b\u9762\u8bed\u53e5\uff0c\u5b83\u4f1a\u66ff\u6362epel.repo\u3001eple-testing.repo\u4e2d\u7684\u7f51\u5740\uff0c\u4e0d\u4f1a\u4fee\u6539epel-cisco-openh264.repo\uff0c\u53ef\u4ee5\u6b63\u5e38\u4f7f\u7528\u3002 sed -e 's!^metalink=!#metalink=!g' \\ -e 's!^#baseurl=!baseurl=!g' \\ -e 's!https\\?://download\\.fedoraproject\\.org/pub/epel!https://mirrors.aliyun.com/epel!g' \\ -e 's!https\\?://download\\.example/pub/epel!https://mirrors.aliyun.com/epel!g' \\ -i /etc/yum.repos.d/epel{,-testing}.repo \u66f4\u65b0\u4ed3\u5e93\u7f13\u5b58 dnf clean all dnf makecache ---\u751f\u6210\u7f13\u5b58\uff0c\u5b89\u88c5\u8f6f\u4ef6\u66f4\u5feb","title":"\u914d\u7f6eYUM\u6e90"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#docker","text":"# Step 1: \u5b89\u88c5\u4f9d\u8d56 yum install -y yum-utils device-mapper-persistent-data lvm2 # Step 2: \u6dfb\u52a0\u8f6f\u4ef6\u6e90\u4fe1\u606f yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/rhel/docker-ce.repo # Step 3: \u5b89\u88c5Docker-CE\uff0c\u67e5\u8be2\u5b89\u88c5\u7684\u7248\u672c dnf list docker-ce --showduplicates | sort -r docker-ce.x86_64 3:28.3.3-1.el9 docker-ce-stable docker-ce.x86_64 3:28.3.2-1.el9 docker-ce-stable docker-ce.x86_64 3:28.3.1-1.el9 docker-ce-stable docker-ce.x86_64 3:28.3.0-1.el9 docker-ce-stable # dnf install docker-ce-<VERSION_STRING> docker-ce-cli-<VERSION_STRING> containerd.io docker-buildx-plugin docker-compose-plugin dnf install docker-ce-28.3.3 docker-ce-cli-28.3.3 containerd.io docker-buildx-plugin docker-compose-plugin -y # docker -v Docker version 28.3.3, build 980b856 # \u8bbe\u7f6e\u56fd\u5185\u955c\u50cf\u52a0\u901f mkdir -p /etc/docker/ cat >> /etc/docker/daemon.json << EOF { \"registry-mirrors\":[\"https://p3kgr6db.mirror.aliyuncs.com\", \"https://docker.m.daocloud.io\", \"https://your_id.mirror.aliyuncs.com\", \"https://docker.nju.edu.cn/\", \"https://docker.anyhub.us.kg\", \"https://dockerhub.jobcher.com\", \"https://dockerhub.icu\", \"https://docker.ckyl.me\", \"https://cr.console.aliyun.com\" ], \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF # \u8bbe\u7f6edocker\u5f00\u673a\u542f\u52a8\u5e76\u542f\u52a8 systemctl daemon-reload systemctl restart docker systemctl enable --now docker # \u67e5\u770bdocker\u7248\u672c [root@gitlab-31-24 ~]# docker version Client: Docker Engine - Community Version: 28.3.3 API version: 1.51 Go version: go1.24.5 Git commit: 980b856 Built: Fri Jul 25 11:36:28 2025 OS/Arch: linux/amd64 Context: default Server: Docker Engine - Community Engine: Version: 28.3.3 API version: 1.51 (minimum version 1.24) Go version: go1.24.5 Git commit: bea959c Built: Fri Jul 25 11:33:28 2025 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.7.27 GitCommit: 05044ec0a9a75232cad458027ca83437aae3f4da runc: Version: 1.2.5 GitCommit: v1.2.5-0-g59923ef docker-init: Version: 0.19.0 GitCommit: de40ad0","title":"\u67e5\u770bdocker\u7248\u672c"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#docker-compose","text":"# \u4ecegithub\u4e0a\u4e0b\u8f7d\u5305\uff0c\u4e0a\u4f20\u5230\u670d\u52a1\u5668\u7684/usr/local/bin/\u4e0b https://github.com/docker/compose/releases/download/v2.27.0/docker-compose-linux-x86_64 mv /usr/local/bin/docker-compose-linux-x86_64 /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose version Docker Compose version v2.27.0","title":"\u5b89\u88c5docker-compose"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#3gitlab","text":"\u62c9\u53d6gitlab\u955c\u50cf docker pull gitlab/gitlab-ce:17.5.0-ce.0 \u521b\u5efa\u5b89\u88c5\u76ee\u5f55\uff0c\u5e76\u521b\u5efadocker-compose.yaml\u6587\u4ef6 mkdir -p /usr/local/docker/docker_gitlab cd /usr/local/docker/docker_gitlab vim docker-compose.yaml services: gitlab: image: 'gitlab/gitlab-ce:17.5.0-ce.0' container_name: gitlab restart: always hostname: '192.168.31.24' environment: GITLAB_OMNIBUS_CONFIG: | external_url 'http://192.168.31.24:8929' gitlab_rails['gitlab_shell_ssh_port'] = 2224 ports: - '8929:8929' - '2224:22' volumes: - './config:/etc/gitlab' - './logs:/var/log/gitlab' - './data:/var/opt/gitlab' docker-compose up -d # \u542f\u52a8\u6210\u529f\u540e\uff0c\u6d4f\u89c8\u5668\u8bbf\u95eehttp://192.168.31.24:8929 \u5982\u679c\u51fa\u73b0502\u9700\u8981\u5728\u591a\u7b49\u4e00\u4f1a\uff0cgitlab\u542f\u52a8\u6210\u529f \u67e5\u770bgitlab\u5bc6\u7801 # \u8fdb\u5165gitlab \u5bb9\u5668\u5185\u90e8\u67e5\u770b\u5bc6\u7801 docker exec -it gitlab bash root@192:/# cat /etc/gitlab/initial_root_password # WARNING: This value is valid only in the following conditions # 1. If provided manually (either via `GITLAB_ROOT_PASSWORD` environment variable or via `gitlab_rails['initial_root_password']` setting in `gitlab.rb`, it was provided before database was seeded for the first time (usually, the first reconfigure run). # 2. Password hasn't been changed manually, either via UI or via command line. # # If the password shown here doesn't work, you must reset the admin password following https://docs.gitlab.com/ee/security/reset_user_password.html#reset-your-root-password. Password: QeAKBSuBxlrGG0tbksCAIOAErunmqNb2DG99zFNGdck= # NOTE: This file will be automatically deleted in the first reconfigure run after 24 hours. root@192:/#","title":"3\u3001\u5b89\u88c5gitlab"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#4-mavenjdk17","text":"Maven\u4e0b\u8f7d\u5730\u5740\uff1a https://dlcdn.apache.org/maven/maven-3/3.8.9/binaries/apache-maven-3.8.9-bin.tar.gz JDK17\u4e0b\u8f7d\u5730\u5740\uff1a https://download.oracle.com/java/17/archive/jdk-17_linux-x64_bin.tar.gz","title":"4\u3001\u5b89\u88c5 Maven\u548cJDK17"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#_4","text":"# \u653e\u5230root\u76ee\u5f55\u4e0b [root@jenkins-31-171 local]# ls -ltr /root/ \u603b\u7528\u91cf 184436 -rw-r--r-- 1 root root 180555480 8\u6708 28 11:20 jdk-17_linux-x64_bin.tar.gz -rw-r--r-- 1 root root 8296518 8\u6708 28 11:23 apache-maven-3.8.9-bin.tar.gz # \u89e3\u538b\u5e76\u6539\u540d tar -zxvf apache-maven-3.8.9-bin.tar.gz -C /usr/local/ tar -zxvf jdk-17_linux-x64_bin.tar.gz -C /usr/local/ cd /usr/local/ mv jdk-17/ jdk mv apache-maven-3.8.9/ maven","title":"\u4e0a\u4f20\u90e8\u7f72\u5305\u5230\u76ee\u6807\u670d\u52a1\u5668"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#maven","text":"# \u914d\u7f6e\u6587\u4ef6 vim /usr/local/maven/conf/settings.xml <mirror> <id>aliyunmaven</id> <mirrorOf>*</mirrorOf> <name>\u963f\u91cc\u4e91\u516c\u5171\u4ed3\u5e93</name> <url>https://maven.aliyun.com/repository/public</url> </mirror>","title":"\u914d\u7f6emaven\u7684\u4ed3\u5e93\u5730\u5740"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#java-jdk","text":"# \u914d\u7f6e\u6587\u4ef6 vim /usr/local/maven/conf/settings.xml <profile> <id>jdk17</id> <activation> <activeByDefault>true</activeByDefault> <jdk>17</jdk> </activation> <properties> <maven.compiler.source>17</maven.compiler.source> <maven.compiler.target>17</maven.compiler.target> <maven.compiler.compilerVersion>17</maven.compiler.compilerVersion> </properties> </profile> </profiles> <!-- activeProfiles | List of profiles that are active for all builds. | <activeProfiles> <activeProfile>alwaysActiveProfile</activeProfile> <activeProfile>anotherAlwaysActiveProfile</activeProfile> </activeProfiles> --> <activeProfiles> <activeProfile>jdk17</activeProfile> </activeProfiles>","title":"\u914d\u7f6eJava JDK"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#5jenkins","text":"\u767b\u5f55\u5b98\u7f51Jenkins.io\u4e0a\u4e0b\u8f7d\u5bf9\u5e94\u7684Jenkins\u955c\u50cf docker pull jenkins/jenkins:2.462.3-lts \u4f7f\u7528docker-compose\u90e8\u7f72Jenkins mkdir -p /usr/local/docker/jenkins_docker cd /usr/local/docker/jenkins_docker vim docker-compose.yaml services: jenkins: image: jenkins/jenkins:2.462.3-lts container_name: jenkins ports: - 8080:8080 - 50000:50000 volumes: - ./data/:/var/jenkins_home/ [root@jenkins-31-171 jenkins_docker]# docker-compose up -d [+] Running 2/2 \u2714 Network jenkins_docker_default Created 0.0s \u2714 Container jenkins Started 0.3s [root@jenkins-31-171 jenkins_docker]# docker logs -f jenkins INSTALL WARNING: User: missing rw permissions on JENKINS_HOME: /var/jenkins_home touch: cannot touch '/var/jenkins_home/copy_reference_file.log': Permission denied Can not write to /var/jenkins_home/copy_reference_file.log. Wrong volume permissions? [root@jenkins-31-171 jenkins_docker]# ls -tlr \u603b\u7528\u91cf 4 -rw-r--r-- 1 root root 185 8\u6708 28 11:51 docker-compose.yaml drwxr-xr-x 2 root root 6 8\u6708 28 11:51 data [root@jenkins-31-171 jenkins_docker]# chmod -R 777 data [root@jenkins-31-171 jenkins_docker]# docker-compose up -d [root@jenkins-31-171 jenkins_docker]# docker logs -f jenkins INSTALL WARNING: User: missing rw permissions on JENKINS_HOME: /var/jenkins_home touch: cannot touch '/var/jenkins_home/copy_reference_file.log': Permission denied Can not write to /var/jenkins_home/copy_reference_file.log. Wrong volume permissions? Running from: /usr/share/jenkins/jenkins.war webroot: /var/jenkins_home/war 2025-08-28 03:52:19.721+0000 [id=1] INFO winstone.Logger#logInternal: Beginning extraction from war file 2025-08-28 03:52:20.611+0000 [id=1] WARNING o.e.j.s.handler.ContextHandler#setContextPath: Empty contextPath 2025-08-28 03:52:20.664+0000 [id=1] INFO org.eclipse.jetty.server.Server#doStart: jetty-10.0.24; built: 2024-08-26T17:58:21.070Z; git: d5384207795da96fad32db8ea8d26b69955bcc03; jvm 17.0.12+7 2025-08-28 03:52:21.016+0000 [id=1] INFO o.e.j.w.StandardDescriptorProcessor#visitServlet: NO JSP Support for /, did not find org.eclipse.jetty.jsp.JettyJspServlet 2025-08-28 03:52:21.050+0000 [id=1] INFO o.e.j.s.s.DefaultSessionIdManager#doStart: Session workerName=node0 2025-08-28 03:52:21.538+0000 [id=1] INFO hudson.WebAppMain#contextInitialized: Jenkins home directory: /var/jenkins_home found at: EnvVars.masterEnvVars.get(\"JENKINS_HOME\") 2025-08-28 03:52:21.655+0000 [id=1] INFO o.e.j.s.handler.ContextHandler#doStart: Started w.@58860997{Jenkins v2.462.3,/,file:///var/jenkins_home/war/,AVAILABLE}{/var/jenkins_home/war} 2025-08-28 03:52:21.670+0000 [id=1] INFO o.e.j.server.AbstractConnector#doStart: Started ServerConnector@5c530d1e{HTTP/1.1, (http/1.1)}{0.0.0.0:8080} 2025-08-28 03:52:21.684+0000 [id=1] INFO org.eclipse.jetty.server.Server#doStart: Started Server@1c6804cd{STARTING}[10.0.24,sto=0] @2329ms 2025-08-28 03:52:21.686+0000 [id=26] INFO winstone.Logger#logInternal: Winstone Servlet Engine running: controlPort=disabled 2025-08-28 03:52:21.870+0000 [id=34] INFO jenkins.InitReactorRunner$1#onAttained: Started initialization 2025-08-28 03:52:21.888+0000 [id=35] INFO jenkins.InitReactorRunner$1#onAttained: Listed all plugins 2025-08-28 03:52:22.496+0000 [id=34] INFO jenkins.InitReactorRunner$1#onAttained: Prepared all plugins 2025-08-28 03:52:22.499+0000 [id=36] INFO jenkins.InitReactorRunner$1#onAttained: Started all plugins 2025-08-28 03:52:22.507+0000 [id=33] INFO jenkins.InitReactorRunner$1#onAttained: Augmented all extensions 2025-08-28 03:52:22.645+0000 [id=39] INFO jenkins.InitReactorRunner$1#onAttained: System config loaded 2025-08-28 03:52:22.645+0000 [id=39] INFO jenkins.InitReactorRunner$1#onAttained: System config adapted 2025-08-28 03:52:22.646+0000 [id=39] INFO jenkins.InitReactorRunner$1#onAttained: Loaded all jobs 2025-08-28 03:52:22.647+0000 [id=39] INFO jenkins.InitReactorRunner$1#onAttained: Configuration for all jobs updated 2025-08-28 03:52:22.675+0000 [id=52] INFO hudson.util.Retrier#start: Attempt #1 to do the action check updates server 2025-08-28 03:52:22.916+0000 [id=39] INFO jenkins.install.SetupWizard#init: ************************************************************* ************************************************************* ************************************************************* Jenkins initial setup is required. An admin user has been created and a password generated. Please use the following password to proceed to installation: e0127ee5810548959b3b175c3fca7545 This may also be found at: /var/jenkins_home/secrets/initialAdminPassword ************************************************************* ************************************************************* ************************************************************* 2025-08-28 03:52:29.800+0000 [id=39] INFO jenkins.InitReactorRunner$1#onAttained: Completed initialization 2025-08-28 03:52:29.878+0000 [id=25] INFO hudson.lifecycle.Lifecycle#onReady: Jenkins is fully up and running 2025-08-28 03:52:31.681+0000 [id=52] INFO h.m.DownloadService$Downloadable#load: Obtained the updated data file for hudson.tasks.Maven.MavenInstaller 2025-08-28 03:52:31.682+0000 [id=52] INFO hudson.util.Retrier#start: Performed the action check updates server successfully at the attempt #1 Jenkins\u5bc6\u7801\u5728\u65e5\u5fd7\u4e2d e0127ee5810548959b3b175c3fca7545 \u7ee7\u7eed\u4e4b\u540e\uff0c\u9009\u62e9-\u9009\u62e9\u63d2\u4ef6\u6765\u5b89\u88c5-\u4fdd\u6301\u9ed8\u8ba4\u5b89\u88c5 http://192.168.31.171:8080/","title":"5\u3001\u5b89\u88c5Jenkins"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#6","text":"Git Parameter Publish Over SSH","title":"6\u3001\u5b89\u88c5\u63d2\u4ef6"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#7jenkinsjdkmaven","text":"mv /usr/local/jdk /usr/local/docker/jenkins_docker/data/ mv /usr/local/maven /usr/local/docker/jenkins_docker/data/ \u5728Jenkins\u7684\u5168\u5c40\u914d\u7f6e\u4e2d\uff0c\u624b\u52a8\u6dfb\u52a0\u5b89\u88c5\u7684JDK\uff0cJAVA_HOME \u586b\u5199\u5bb9\u5668\u5185\u7684\u8def\u5f84 \u5728Jenkins\u7684\u5168\u5c40\u914d\u7f6e\u4e2d\uff0c\u624b\u52a8\u6dfb\u52a0\u5b89\u88c5\u7684maven\uff0cMAVEN_HOME \u586b\u5199\u5bb9\u5668\u5185\u7684\u8def\u5f84 \u5728\u7cfb\u7edf\u914d\u7f6e\u4e2d\uff0c\u914d\u7f6e\u8fdc\u7aef\u63a5\u6536\u9879\u76ee\u7684\u670d\u52a1\u5668\uff0c\u91c7\u7528root \u5bc6\u7801\u7684\u65b9\u5f0f\u8fde\u63a5\u3002\u586b\u5199\u76ee\u5f55\u65f6\uff0c\u5fc5\u987b\u5148\u521b\u5efa","title":"7\u3001\u5728Jenkins\u4e2d\u914d\u7f6eJDK\u548cmaven"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#8","text":"\u8f6f\u4ef6\uff1avscode + maven + git maven\uff1a https://dlcdn.apache.org/maven/maven-3/3.9.11/binaries/apache-maven-3.9.11-bin.zip git\uff1a https://github.com/git-for-windows/git/releases/download/v2.51.0.windows.1/Git-2.51.0-64-bit.exe \u50bb\u74dc\u5f0f\u5b89\u88c5 \u8be6\u60c5\u8bf7\u53c2\u8003\uff1a \u8d85\u8be6\u7ec6\u7684VsCode\u521b\u5efaSpringBoot\u9879\u76ee(\u56fe\u6587\u5e76\u8302)_vscode\u521b\u5efaspring boot\u9879\u76ee-CSDN\u535a\u5ba2","title":"8\u3001\u642d\u5efa\u672c\u5730\u73af\u5883"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#9jenkinsgitlab","text":"","title":"9\u3001\u4f7f\u7528Jenkins\u62c9\u53d6gitlab\u4ee3\u7801"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#91-git","text":"","title":"9.1 \u521b\u5efa\u4e00\u4e2a\u81ea\u7531\u98ce\u683c\u7684\u9879\u76ee\uff0c\u5e76\u4e14\u5728\u6e90\u7801\u7ba1\u7406\u4e2d\u6dfb\u52a0git"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#92-gitlab","text":"\u53ef\u4ee5\u5728Jenkins\u670d\u52a1\u5668\u7684data/workspace\u4e2d\u67e5\u770b\u5230","title":"9.2 \u70b9\u51fb\u7acb\u5373\u6784\u5efa\uff0c\u628agitlab\u4ed3\u5e93\u7684\u4ee3\u7801\u62c9\u5230\u672c\u5730"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#10gitlab","text":"","title":"10\u3001\u5728\u62c9\u53d6gitlab\u4ee3\u7801\u7684\u540c\u65f6\uff0c\u63a8\u9001\u5230\u76ee\u6807\u670d\u52a1\u5668"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#101","text":"\u4f7f\u7528\u5df2\u5b89\u88c5\u7684maven","title":"10.1 \u6dfb\u52a0\u6784\u5efa\u64cd\u4f5c"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#102","text":"","title":"10.2 \u70b9\u51fb\u7acb\u5373\u6784\u5efa"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#103-jark8s","text":"","title":"10.3 \u6dfb\u52a0\u6784\u5efa\u540e\u64cd\u4f5c-\u628ajar\u5305\u63a8\u9001\u5230k8s\u670d\u52a1\u5668"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#104-k8s","text":"","title":"10.4 \u767b\u5f55k8s\u670d\u52a1\u5668\u67e5\u770b\u76ee\u6807\u6587\u4ef6"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#105-docker-compose","text":"# \u5728\u9879\u76ee\u4e2d\u521b\u5efadocker\u76ee\u5f55\uff0c\u5e76\u521b\u5efaDockerfile\u6587\u4ef6\u548cdocker-compose.yaml\u6587\u4ef6 # Dockerfile FROM eclipse-temurin:17-jdk-jammy COPY mytest.jar /usr/local/ WORKDIR /usr/local CMD java -jar mytest.jar # docker-compose.yaml services: mytest: build: context: ./ dockerfile: Dockerfile image: mytest:v1.0.0 container_name: mytest ports: - 8081:8080 Jenkins\u914d\u7f6e\u6784\u5efa\u540e\u64cd\u4f5c \u70b9\u51fb\u7acb\u5373\u6784\u5efa\u4e4b\u540e\uff0c\u6d4f\u89c8\u5668\u8bbf\u95ee\u670d\u52a1","title":"10.5 \u4f7f\u7528docker-compose\u7684\u65b9\u5f0f\u53d1\u5e03\u670d\u52a1"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#11jenkins","text":"","title":"11\u3001Jenkins\u57fa\u4e8e\u53c2\u6570\u6784\u5efa"},{"location":"%E6%8A%80%E6%9C%AF/DevOps/#12jenkinsdocker","text":"# \u627e\u5230docker.sock\u6587\u4ef6\uff0c\u628a\u6240\u5c5e\u7ec4\u6539\u6210root\u3002\u7ed9\u5176\u4ed6\u4eba\u6dfb\u52a0\u8bfb\u548c\u5199\u6743\u9650 cd /run/ chown root:root docker.sock chmod o+rw docker.sock [root@jenkins-31-171 jenkins_docker]# cat docker-compose.yaml services: jenkins: image: jenkins/jenkins:2.462.3-lts container_name: jenkins ports: - 8080:8080 - 50000:50000 volumes: - ./data/:/var/jenkins_home/ - /run/docker.sock:/run/docker.sock - /usr/bin/docker:/usr/bin/docker - /etc/docker/daemon.json:/etc/docker/daemon.json","title":"12\u3001Jenkins\u5bb9\u5668\u5185\u90e8\u4f7f\u7528Docker"},{"location":"%E6%8A%80%E6%9C%AF/Docker%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","text":"Docker \u5e38\u7528\u547d\u4ee4 1\u3001\u4ed3\u5e93\u76f8\u5173 \u547d\u4ee4 \u63cf\u8ff0 docker search Docker\u4ed3\u5e93\u67e5\u8be2\u955c\u50cf docker login \u767b\u5f55\u5230docker\u955c\u50cf\u4ed3\u5e93 docker logout \u767b\u51fadocker\u955c\u50cf\u4ed3\u5e93 docker pull Docker\u4ed3\u5e93\u4e0b\u8f7d\u955c\u50cf docker push \u4ece\u672c\u5730\u4e0a\u4f20\u955c\u50cf\u5230Docker\u4ed3\u5e93 docker info \u663e\u793a Docker \u7cfb\u7edf\u4fe1\u606f\uff0c\u5305\u62ec\u955c\u50cf\u548c\u5bb9\u5668\u6570 docker version \u663e\u793a Docker \u7248\u672c\u4fe1\u606f 2\u3001\u955c\u50cf\u76f8\u5173 \u672c\u5730\u955c\u50cf\u7ba1\u7406 \u547d\u4ee4 \u63cf\u8ff0 docker images \u5217\u51fa\u672c\u5730\u955c\u50cf docker rmi \u5220\u9664\u672c\u5730\u4e00\u4e2a\u6216\u591a\u4e2a\u955c\u50cf docker tag \u6807\u8bb0\u672c\u5730\u955c\u50cf\uff0c\u5c06\u5176\u5f52\u5165\u67d0\u4e00\u4ed3\u5e93 docker build \u7528\u4e8e\u4f7f\u7528 Dockerfile \u521b\u5efa\u955c\u50cf docker history \u67e5\u770b\u6307\u5b9a\u955c\u50cf\u7684\u521b\u5efa\u5386\u53f2 docker save \u5c06\u6307\u5b9a\u955c\u50cf\u4fdd\u5b58\u6210 tar \u5f52\u6863\u6587\u4ef6 docker load \u5bfc\u5165\u4f7f\u7528 docker save \u547d\u4ee4\u5bfc\u51fa\u7684\u955c\u50cf docker import \u4ece\u5f52\u6863\u6587\u4ef6\u4e2d\u521b\u5efa\u955c\u50cf 3\u3001\u5bb9\u5668\u76f8\u5173 \u5bb9\u5668\u751f\u547d\u5468\u671f\u7ba1\u7406 \u547d\u4ee4 \u63cf\u8ff0 docker run \u521b\u5efa\u5e76\u542f\u52a8\u5bb9\u5668 docker create \u521b\u5efa\u4e00\u4e2a\u5bb9\u5668\uff0c\u4f46\u4e0d\u542f\u7528 docker start/stop/restart \u542f\u52a8/\u505c\u6b62/\u91cd\u542f\u5bb9\u5668 docker kill \u6740\u6b7b\u5bb9\u5668 docker rm \u79fb\u9664\u5bb9\u5668 docker pause \u6682\u505c\u5bb9\u5668\u4e2d\u6240\u6709\u8fdb\u7a0b docker unpause \u6062\u590d\u5bb9\u5668\u4e2d\u6240\u6709\u8fdb\u7a0b docker exec \u5728\u8fd0\u884c\u7684\u5bb9\u5668\u4e2d\u6267\u884c\u547d\u4ee4 4\u3001\u5bb9\u5668\u64cd\u4f5c \u547d\u4ee4 \u63cf\u8ff0 docker ps \u5217\u51fa\u5bb9\u5668 docker inspect \u83b7\u53d6\u5bb9\u5668/\u955c\u50cf\u7684\u5143\u6570\u636e docker top \u67e5\u770b\u5bb9\u5668\u4e2d\u8fd0\u884c\u7684\u8fdb\u7a0b\u4fe1\u606f\uff0c\u652f\u6301 ps \u547d\u4ee4\u53c2\u6570 docker attach \u8fde\u63a5\u5230\u6b63\u5728\u8fd0\u884c\u4e2d\u7684\u5bb9\u5668 docker events \u4ece\u670d\u52a1\u5668\u83b7\u53d6\u5b9e\u65f6\u4e8b\u4ef6 docker logs \u83b7\u53d6\u5bb9\u5668\u7684\u65e5\u5fd7 docker wait \u963b\u585e\u8fd0\u884c\u76f4\u5230\u5bb9\u5668\u505c\u6b62\uff0c\u7136\u540e\u6253\u5370\u51fa\u9000\u51fa\u4ee3\u7801 docker export \u5c06\u6587\u4ef6\u7cfb\u7edf\u4f5c\u4e3a\u4e00\u4e2atar\u5f52\u6863\u6587\u4ef6\u5bfc\u51fa\u5230STDOUT docker port \u5217\u51fa\u6307\u5b9a\u7684\u5bb9\u5668\u7684\u7aef\u53e3\u6620\u5c04\uff0c\u6216\u8005\u67e5\u627e\u5c06PRIVATE_PORT NAT\u5230\u9762\u5411\u516c\u4f17\u7684\u7aef\u53e3 5\u3001\u5bb9\u5668rootfs\u547d\u4ee4 \u547d\u4ee4 \u63cf\u8ff0 docker commit \u4ece\u5bb9\u5668\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u955c\u50cf docker cp \u7528\u4e8e\u5bb9\u5668\u4e0e\u4e3b\u673a\u4e4b\u95f4\u7684\u6570\u636e\u62f7\u8d1d docker diff \u68c0\u67e5\u5bb9\u5668\u91cc\u6587\u4ef6\u7ed3\u6784\u7684\u66f4\u6539","title":"Docker \u5e38\u7528\u547d\u4ee4"},{"location":"%E6%8A%80%E6%9C%AF/Docker%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#docker","text":"","title":"Docker \u5e38\u7528\u547d\u4ee4"},{"location":"%E6%8A%80%E6%9C%AF/Docker%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#1","text":"\u547d\u4ee4 \u63cf\u8ff0 docker search Docker\u4ed3\u5e93\u67e5\u8be2\u955c\u50cf docker login \u767b\u5f55\u5230docker\u955c\u50cf\u4ed3\u5e93 docker logout \u767b\u51fadocker\u955c\u50cf\u4ed3\u5e93 docker pull Docker\u4ed3\u5e93\u4e0b\u8f7d\u955c\u50cf docker push \u4ece\u672c\u5730\u4e0a\u4f20\u955c\u50cf\u5230Docker\u4ed3\u5e93 docker info \u663e\u793a Docker \u7cfb\u7edf\u4fe1\u606f\uff0c\u5305\u62ec\u955c\u50cf\u548c\u5bb9\u5668\u6570 docker version \u663e\u793a Docker \u7248\u672c\u4fe1\u606f","title":"1\u3001\u4ed3\u5e93\u76f8\u5173"},{"location":"%E6%8A%80%E6%9C%AF/Docker%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#2","text":"\u672c\u5730\u955c\u50cf\u7ba1\u7406 \u547d\u4ee4 \u63cf\u8ff0 docker images \u5217\u51fa\u672c\u5730\u955c\u50cf docker rmi \u5220\u9664\u672c\u5730\u4e00\u4e2a\u6216\u591a\u4e2a\u955c\u50cf docker tag \u6807\u8bb0\u672c\u5730\u955c\u50cf\uff0c\u5c06\u5176\u5f52\u5165\u67d0\u4e00\u4ed3\u5e93 docker build \u7528\u4e8e\u4f7f\u7528 Dockerfile \u521b\u5efa\u955c\u50cf docker history \u67e5\u770b\u6307\u5b9a\u955c\u50cf\u7684\u521b\u5efa\u5386\u53f2 docker save \u5c06\u6307\u5b9a\u955c\u50cf\u4fdd\u5b58\u6210 tar \u5f52\u6863\u6587\u4ef6 docker load \u5bfc\u5165\u4f7f\u7528 docker save \u547d\u4ee4\u5bfc\u51fa\u7684\u955c\u50cf docker import \u4ece\u5f52\u6863\u6587\u4ef6\u4e2d\u521b\u5efa\u955c\u50cf","title":"2\u3001\u955c\u50cf\u76f8\u5173"},{"location":"%E6%8A%80%E6%9C%AF/Docker%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#3","text":"\u5bb9\u5668\u751f\u547d\u5468\u671f\u7ba1\u7406 \u547d\u4ee4 \u63cf\u8ff0 docker run \u521b\u5efa\u5e76\u542f\u52a8\u5bb9\u5668 docker create \u521b\u5efa\u4e00\u4e2a\u5bb9\u5668\uff0c\u4f46\u4e0d\u542f\u7528 docker start/stop/restart \u542f\u52a8/\u505c\u6b62/\u91cd\u542f\u5bb9\u5668 docker kill \u6740\u6b7b\u5bb9\u5668 docker rm \u79fb\u9664\u5bb9\u5668 docker pause \u6682\u505c\u5bb9\u5668\u4e2d\u6240\u6709\u8fdb\u7a0b docker unpause \u6062\u590d\u5bb9\u5668\u4e2d\u6240\u6709\u8fdb\u7a0b docker exec \u5728\u8fd0\u884c\u7684\u5bb9\u5668\u4e2d\u6267\u884c\u547d\u4ee4","title":"3\u3001\u5bb9\u5668\u76f8\u5173"},{"location":"%E6%8A%80%E6%9C%AF/Docker%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#4","text":"\u547d\u4ee4 \u63cf\u8ff0 docker ps \u5217\u51fa\u5bb9\u5668 docker inspect \u83b7\u53d6\u5bb9\u5668/\u955c\u50cf\u7684\u5143\u6570\u636e docker top \u67e5\u770b\u5bb9\u5668\u4e2d\u8fd0\u884c\u7684\u8fdb\u7a0b\u4fe1\u606f\uff0c\u652f\u6301 ps \u547d\u4ee4\u53c2\u6570 docker attach \u8fde\u63a5\u5230\u6b63\u5728\u8fd0\u884c\u4e2d\u7684\u5bb9\u5668 docker events \u4ece\u670d\u52a1\u5668\u83b7\u53d6\u5b9e\u65f6\u4e8b\u4ef6 docker logs \u83b7\u53d6\u5bb9\u5668\u7684\u65e5\u5fd7 docker wait \u963b\u585e\u8fd0\u884c\u76f4\u5230\u5bb9\u5668\u505c\u6b62\uff0c\u7136\u540e\u6253\u5370\u51fa\u9000\u51fa\u4ee3\u7801 docker export \u5c06\u6587\u4ef6\u7cfb\u7edf\u4f5c\u4e3a\u4e00\u4e2atar\u5f52\u6863\u6587\u4ef6\u5bfc\u51fa\u5230STDOUT docker port \u5217\u51fa\u6307\u5b9a\u7684\u5bb9\u5668\u7684\u7aef\u53e3\u6620\u5c04\uff0c\u6216\u8005\u67e5\u627e\u5c06PRIVATE_PORT NAT\u5230\u9762\u5411\u516c\u4f17\u7684\u7aef\u53e3","title":"4\u3001\u5bb9\u5668\u64cd\u4f5c"},{"location":"%E6%8A%80%E6%9C%AF/Docker%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#5rootfs","text":"\u547d\u4ee4 \u63cf\u8ff0 docker commit \u4ece\u5bb9\u5668\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u955c\u50cf docker cp \u7528\u4e8e\u5bb9\u5668\u4e0e\u4e3b\u673a\u4e4b\u95f4\u7684\u6570\u636e\u62f7\u8d1d docker diff \u68c0\u67e5\u5bb9\u5668\u91cc\u6587\u4ef6\u7ed3\u6784\u7684\u66f4\u6539","title":"5\u3001\u5bb9\u5668rootfs\u547d\u4ee4"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/","text":"\ud83e\udd16\u4e3b\u673a\u914d\u7f6e \u4f5c\u7528 IP\u5730\u5740 \u64cd\u4f5c\u7cfb\u7edf \u914d\u7f6e \u5173\u952e\u7ec4\u4ef6 k8s-master 192.168.1.30 Rocky Linux release 9 4C/4G/50GB kube-apiserver, etcd,,docker k8s-node1 192.168.1.31 Rocky Linux release9 4C/4G/50GB kubelet, kube-proxy,docker k8s-node2 192.168.1.32 Rocky Linux release 9 4C/4G/50GB kubelet, kube-proxy,docker k8s-node3 192.168.1.33 Rocky Linux release 9 4C/4G/50GB kubelet, kube-proxy,docker \ud83e\udeaa\u8bbe\u7f6eIP \u65b9\u5f0f\u4e00\uff1a nmcli connection modify ens160 ipv4.addresses 192.168.1.30/24 ipv4.gateway 192.168.1.1 ipv4.method manual nmcli connection modify ens160 ipv4.addresses 192.168.1.31/24 ipv4.gateway 192.168.1.1 ipv4.method manual nmcli connection modify ens160 ipv4.addresses 192.168.1.32/24 ipv4.gateway 192.168.1.1 ipv4.method manual nmcli connection modify ens160 ipv4.addresses 192.168.1.33/24 ipv4.gateway 192.168.1.1 ipv4.method manual nmcli connection up ens160 \u65b9\u5f0f\u4e8c\uff1a vi /etc/NetworkManager/system-connections/ens160.nmconnection method=manual## \u5728IPV4\u4e0b\u9762\u4fee\u6539\u5982\u4e0b\u5185\u5bb9 address1=192.168.0.5/24,192.168.0.1## \u4fee\u6539IP\uff0c\u5b50\u7f51\u63a9\u7801\uff0824\u662f\u5b50\u7f51\u63a9\u7801\u768424\u4f4d\uff0c\u5bf9\u5e94255.255.255.0\uff09\uff0c \u7f51\u5173 dns=119.29.29.29;114.114.114.114## \u8bbe\u7f6eDNS\u670d\u52a1 may-fail=false \u91cd\u65b0\u52a0\u8f7d\u914d\u7f6e\u6587\u4ef6 nmcli connection reload ens160.nmconnection \u6fc0\u6d3b\u914d\u7f6e\u6587\u4ef6 nmcli connection up ens160 \ud83d\ude80\u914d\u7f6eYUM\u6e90 \u914d\u7f6eyum\u6e90 \uff081\uff09\u786e\u8ba4\u6587\u4ef6\u662f\u5426\u5b58\u5728\u4e14\u53ef\u8bfb sudo cat /etc/yum.repos.d/rocky.repo \u5982\u679c\u6587\u4ef6\u4e0d\u5b58\u5728\u6216\u5185\u5bb9\u4e3a\u7a7a\uff0c\u91cd\u65b0\u521b\u5efa\u5b83\u3002 \uff082\uff09\u91cd\u65b0\u4e0b\u8f7d\u6b63\u786e\u7684\u963f\u91cc\u4e91\u6e90\u6587\u4ef6 sudo rm -f /etc/yum.repos.d/rocky.repo # \u5220\u9664\u65e7\u6587\u4ef6\uff08\u5982\u679c\u6709\uff09 sudo curl -o /etc/yum.repos.d/rocky.repo https://mirrors.aliyun.com/rockylinux/rocky.repo?repo=rocky-9 \uff083\uff09\u624b\u52a8\u7f16\u8f91\u6587\u4ef6\uff08\u5982\u679c\u4e0b\u8f7d\u5931\u8d25\uff09 sudo vi /etc/yum.repos.d/rocky.repo \u7c98\u8d34\u4ee5\u4e0b\u5185\u5bb9\uff08\u963f\u91cc\u4e91 Rocky Linux 9 \u955c\u50cf\u6e90\uff09\uff1a [baseos] name=Rocky Linux $releasever - BaseOS - Aliyun baseurl=https://mirrors.aliyun.com/rockylinux/$releasever/BaseOS/$basearch/os/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficial [appstream] name=Rocky Linux $releasever - AppStream - Aliyun baseurl=https://mirrors.aliyun.com/rockylinux/$releasever/AppStream/$basearch/os/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficial [extras] name=Rocky Linux $releasever - Extras - Aliyun baseurl=https://mirrors.aliyun.com/rockylinux/$releasever/extras/$basearch/os/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficial \uff084\uff09\u4e5f\u53ef\u4ee5\u76f4\u63a5\u66ff\u6362yum\u6e90\u91cc\u7684\u5730\u5740 sed -e 's|^mirrorlist=|#mirrorlist=|g' \\ -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\ -i.bak \\ /etc/yum.repos.d/rocky*.repo \u5f3a\u5236\u66ff\u6362\u53d8\u91cf\u4e3a Rocky Linux 9 \u786e\u4fdd $releasever \u548c $basearch \u88ab\u6b63\u786e\u89e3\u6790\uff1a sudo sed -i 's/$releasever/9/g' /etc/yum.repos.d/rocky.repo sudo sed -i 's/$basearch/x86_64/g' /etc/yum.repos.d/rocky.repo # \u5982\u679c\u662f x86_64 \u67b6\u6784 \u5bfc\u5165 GPG \u5bc6\u94a5 bash sudo rpm --import https://mirrors.aliyun.com/rockylinux/RPM-GPG-KEY-rockyofficial \u68c0\u67e5\u6587\u4ef6\u6743\u9650\u548c\u683c\u5f0f \uff081\uff09\u786e\u4fdd\u6587\u4ef6\u6743\u9650\u6b63\u786e bash sudo chmod 644 /etc/yum.repos.d/rocky.repo \uff082\uff09\u68c0\u67e5\u6587\u4ef6\u683c\u5f0f\uff08\u907f\u514d UTF-8 BOM \u6216 Windows \u6362\u884c\u7b26\uff09 ```bash sudo dos2unix /etc/yum.repos.d/rocky.repo # \u5982\u679c\u662f\u4ece Windows \u590d\u5236\u7684\u6587\u4ef6 ``` \u6e05\u9664\u7f13\u5b58\u5e76\u91cd\u65b0\u52a0\u8f7d bash sudo dnf clean all sudo dnf makecache \u9a8c\u8bc1\u4ed3\u5e93\u662f\u5426\u542f\u7528 bash sudo dnf repolist \u6b63\u5e38\u8f93\u51fa\u5e94\u7c7b\u4f3c\uff1a text repo id repo name baseos Rocky Linux 9 - BaseOS - Aliyun appstream Rocky Linux 9 - AppStream - Aliyun extras Rocky Linux 9 - Extras - Aliyun \u5728 Rocky Linux 9 \u4e2d\u542f\u7528\u5e76\u5b89\u88c5 EPEL Repo\u3002 dnf install epel-release \u5907\u4efd(\u5982\u6709\u914d\u7f6e\u5176\u4ed6epel\u6e90)\u5e76\u66ff\u6362\u4e3a\u56fd\u5185\u955c\u50cf \u6ce8\u610f\u6700\u540e\u8fd9\u4e2a\u5e93\uff0c\u963f\u91cc\u4e91\u6ca1\u6709\u5bf9\u5e94\u7684\u955c\u50cf\uff0c\u4e0d\u8981\u4fee\u6539\u5b83\uff0c\u5982\u679c\u8bef\u6539\u6062\u590d\u539f\u7248\u6e90\u5373\u53ef cp /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backup cp /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backup cp /etc/yum.repos.d/epel-cisco-openh264.repo /etc/yum.repos.d/epel-cisco-openh264.repo.backup \u5c06 repo \u914d\u7f6e\u4e2d\u7684\u5730\u5740\u66ff\u6362\u4e3a\u963f\u91cc\u4e91\u955c\u50cf\u7ad9\u5730\u5740 \u6267\u884c\u4e0b\u9762\u8bed\u53e5\uff0c\u5b83\u4f1a\u66ff\u6362epel.repo\u3001eple-testing.repo\u4e2d\u7684\u7f51\u5740\uff0c\u4e0d\u4f1a\u4fee\u6539epel-cisco-openh264.repo\uff0c\u53ef\u4ee5\u6b63\u5e38\u4f7f\u7528\u3002 sed -e 's!^metalink=!#metalink=!g' \\ -e 's!^#baseurl=!baseurl=!g' \\ -e 's!https\\?://download\\.fedoraproject\\.org/pub/epel!https://mirrors.aliyun.com/epel!g' \\ -e 's!https\\?://download\\.example/pub/epel!https://mirrors.aliyun.com/epel!g' \\ -i /etc/yum.repos.d/epel{,-testing}.repo \u66f4\u65b0\u4ed3\u5e93\u7f13\u5b58 dnf clean all dnf makecache ---\u751f\u6210\u7f13\u5b58\uff0c\u5b89\u88c5\u8f6f\u4ef6\u66f4\u5feb \u6bcf\u53f0\u673a\u5668\u5355\u72ec\u505a hostnamectl set-hostname k8s-master hostnamectl set-hostname k8s-node1 hostnamectl set-hostname k8s-node2 hostnamectl set-hostname k8s-node3 \u8bbe\u7f6ehosts cat >> /etc/hosts << EOF 192.168.1.30 k8s-master 192.168.1.31 k8s-node1 192.168.1.32 k8s-node2 192.168.1.33 k8s-node3 EOF \u914d\u7f6e\u514d\u5bc6\u767b\u5f55\uff0c \u53ea\u5728k8s-master\u4e0a\u64cd\u4f5c [root@k8s-master ~]# ssh-keygen -f ~/.ssh/id_rsa -N '' -q \u62f7\u8d1d\u5bc6\u94a5\u5230\u5176\u4ed63 \u53f0\u8282\u70b9 [root@k8s-master ~]# ssh-copy-id k8s-node1 [root@k8s-master ~]# ssh-copy-id k8s-node2 [root@k8s-master ~]# ssh-copy-id k8s-node3 \ud83e\uddf1\u9632\u706b\u5899\u548cSELinux # \u5173\u95ed\u9632\u706b\u5899 systemctl disable --now firewalld # \u7981\u7528SELinux sed -i '/^SELINUX=/ c SELINUX=disabled' /etc/selinux/config # \u91cd\u542f\u751f\u6548\u6240\u4ee5\u4e34\u65f6\u8bbe\u7f6e\u4e3a\u5bbd\u5bb9\u6a21\u5f0f setenforce 0 \u23f1\ufe0f\u65f6\u95f4\u540c\u6b65\u914d\u7f6e # \u5b89\u88c5\u65f6\u95f4\u670d\u52a1\u5668\u8f6f\u4ef6\u5305 dnf install -y chrony # \u4fee\u6539\u540c\u6b65\u670d\u52a1\u5668 sed -i '/^pool/ c pool ntp1.aliyun.com iburst' /etc/chrony.conf systemctl restart chronyd systemctl enable chronyd chronyc sources \ud83d\udede\u914d\u7f6e\u5185\u6838\u8f6c\u53d1\u53ca\u7f51\u6865\u8fc7\u6ee4 # \u6dfb\u52a0\u7f51\u6865\u8fc7\u6ee4\u53ca\u5185\u6838\u8f6c\u53d1\u914d\u7f6e\u6587\u4ef6 cat > /etc/sysctl.d/k8s.conf << EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 vm.swappiness = 0 EOF # \u52a0\u8f7dbr_netfilter\u6a21\u5757 modprobe br_netfilter \u4f7f\u7528\u65b0\u6dfb\u52a0\u914d\u7f6e\u6587\u4ef6\u751f\u6548 sysctl -p /etc/sysctl.d/k8s.conf \ud83d\udcf4\u5173\u95edswap \u67e5\u770b\u4ea4\u6362\u5206\u533a\u60c5\u51b5 # \u4e34\u65f6\u5173\u95ed swapoff -a # \u6c38\u8fdc\u5173\u95edswap\u5206\u533a sed -i 's/.*swap.*/#&/' /etc/fstab \ud83d\udcc4\u542f\u7528ipvs cat >> /etc/modules-load.d/ipvs.conf << EOF br_netfilter ip_conntrack ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp ip_vs_sh nf_conntrack ip_tables ip_set xt_set ipt_set ipt_rpfilter ipt_REJECT ipip EOF # \u5b89\u88c5\u4f9d\u8d56 dnf install ipvsadm ipset sysstat conntrack libseccomp -y \u91cd\u542f\u670d\u52a1 systemctl restart systemd-modules-load.service \u67e5\u770b\u6a21\u5757\u5185\u5bb9 lsmod | grep -e ip_vs -e nf_conntrack \ud83d\udcc3\u53e5\u67c4\u6570\u6700\u5927 # \u8bbe\u7f6e\u4e3a\u6700\u5927 ulimit -SHn 65535 cat >> /etc/security/limits.conf <<EOF * soft nofile 655360 * hard nofile 131072 * soft nproc 655350 * hard nproc 655350 * seft memlock unlimited * hard memlock unlimitedd EOF # \u67e5\u770b\u4fee\u6539\u7ed3\u679c ulimit -a \ud83d\uddc3\ufe0f\u7cfb\u7edf\u4f18\u5316 cat > /etc/sysctl.d/k8s_better.conf << EOF net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 vm.swappiness=0 vm.overcommit_memory=1 vm.panic_on_oom=0 fs.inotify.max_user_instances=8192 fs.inotify.max_user_watches=1048576 fs.file-max=52706963 fs.nr_open=52706963 net.ipv6.conf.all.disable_ipv6=1 net.netfilter.nf_conntrack_max=2310720 EOF modprobe br_netfilter lsmod |grep conntrack modprobe ip_conntrack sysctl -p /etc/sysctl.d/k8s_better.conf \ud83d\udc0b\u5b89\u88c5docker # Step 1: \u5b89\u88c5\u4f9d\u8d56 yum install -y yum-utils device-mapper-persistent-data lvm2 # Step 2: \u6dfb\u52a0\u8f6f\u4ef6\u6e90\u4fe1\u606f yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/rhel/docker-ce.repo # Step 3: \u5b89\u88c5Docker-CE yum -y install docker-ce # docker -v Docker version 27.5.1, build 9f9e405 # \u8bbe\u7f6e\u56fd\u5185\u955c\u50cf\u52a0\u901f mkdir -p /etc/docker/ cat >> /etc/docker/daemon.json << EOF { \"registry-mirrors\":[\"https://p3kgr6db.mirror.aliyuncs.com\", \"https://docker.m.daocloud.io\", \"https://your_id.mirror.aliyuncs.com\", \"https://docker.nju.edu.cn/\", \"https://docker.anyhub.us.kg\", \"https://dockerhub.jobcher.com\", \"https://dockerhub.icu\", \"https://docker.ckyl.me\", \"https://cr.console.aliyun.com\" ], \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF # \u8bbe\u7f6edocker\u5f00\u673a\u542f\u52a8\u5e76\u542f\u52a8 systemctl enable --now docker # \u67e5\u770bdocker\u7248\u672c docker version \u5b89\u88c5cri-dockerd \u4e0b\u8f7d\u5730\u5740\uff1a Releases \u00b7 Mirantis/cri-dockerd (github.com) \u3002 https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.16/cri-dockerd-0.3.16-3.fc35.x86_64.rpm \u5b89\u88c5cri-docker # \u4e0b\u8f7drpm\u5305 wget -c https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.16/cri-dockerd-0.3.16-3.fc35.x86_64.rpm wget -c https://rpmfind.net/linux/almalinux/8.10/BaseOS/x86_64/os/Packages/libcgroup-0.41-19.el8.x86_64.rpm # \u5b89\u88c5rpm\u5305 yum install libcgroup-0.41-19.el8.x86_64.rpm yum install cri-dockerd-0.3.16-3.fc35.x86_64.rpm \u8bbe\u7f6ecri-docker\u670d\u52a1\u5f00\u673a\u81ea\u542f systemctl enable cri-docker cri-docke\u8bbe\u7f6e\u56fd\u5185\u955c\u50cf\u52a0\u901f # \u7f16\u8f91service\u6587\u4ef6 vim /usr/lib/systemd/system/cri-docker.service\u6587\u4ef6 \u4fee\u6539\u7b2c10\u884c\u5185\u5bb9 ------------------ ExecStart=/usr/bin/cri-dockerd --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9 --container-runtime-endpoint fd:// ----------------------------------- # \u91cd\u542fDocker\u7ec4\u4ef6 systemctl daemon-reload && systemctl restart docker cri-docker.socket cri-docker # \u68c0\u67e5Docker\u7ec4\u4ef6\u72b6\u6001 systemctl status docker cir-docker.socket cri-docker \u2638\ufe0fK8S\u8f6f\u4ef6\u5b89\u88c5 # 1\u3001\u914d\u7f6ekubernetes\u6e90 cat <<EOF | tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/ enabled=1 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/repodata/repomd.xml.key EOF # 2\u3001\u67e5\u770b\u6240\u6709\u53ef\u7528\u7684\u7248\u672c yum list kubelet --showduplicates | sort -r |grep 1.32 # 3\u3001\u5b89\u88c5kubelet\u3001kubeadm\u3001kubectl\u3001kubernetes-cni yum install -y kubelet kubeadm kubectl kubernetes-cni # 4\u3001\u914d\u7f6ecgroup \u4e3a\u4e86\u5b9e\u73b0docker\u4f7f\u7528\u7684cgroupdriver\u4e0ekubelet\u4f7f\u7528\u7684cgroup\u7684\u4e00\u81f4\u6027\uff0c\u5efa\u8bae\u4fee\u6539\u5982\u4e0b\u6587\u4ef6\u5185\u5bb9\u3002 vim /etc/sysconfig/kubelet [3\u53f0\u5168\u90e8\u8bbe\u7f6e\u4e0b] --------------------- KUBELET_EXTRA_ARGS=\"--cgroup-driver=systemd\" --------------------- # 5\u3001\u8bbe\u7f6ekubelet\u4e3a\u5f00\u673a\u81ea\u542f\u52a8\u5373\u53ef\uff0c\u7531\u4e8e\u6ca1\u6709\u751f\u6210\u914d\u7f6e\u6587\u4ef6\uff0c\u96c6\u7fa4\u521d\u59cb\u5316\u540e\u81ea\u52a8\u542f\u52a8 systemctl enable kubelet K8S\u96c6\u7fa4\u521d\u59cb\u5316 # \u53ea\u5728k8s-master\u8282\u70b9\u4e0a\u64cd\u4f5c [root@localhost ~]# kubeadm config print init-defaults > kubeadm-init.yaml # \u7f16\u8f91kubeadm-init.yaml\u4fee\u6539\u5982\u4e0b\u914d\u7f6e\uff1a - advertiseAddress\uff1a\u4e3a\u63a7\u5236\u5e73\u9762\u5730\u5740\uff0c\uff08Master\u4e3b\u673aIP\uff09 advertiseAddress: 1.2.3.4 \u4fee\u6539\u4e3a advertiseAddress: 192.168.1.30 - criSocket\uff1a\u4e3a containerd \u7684socket \u6587\u4ef6\u5730\u5740 criSocket: unix:///var/run/containerd/containerd.sock \u4fee\u6539\u4e3a criSocket: unix:///var/run/cri-dockerd.sock - name: node \u4fee\u6539node\u4e3ak8s-master name: node \u4fee\u6539\u4e3a name: k8s-master - imageRepository\uff1a\u963f\u91cc\u4e91\u955c\u50cf\u4ee3\u7406\u5730\u5740\uff0c\u5426\u5219\u62c9\u53d6\u955c\u50cf\u4f1a\u5931\u8d25 imageRepository: registry.k8s.io \u4fee\u6539\u4e3a\uff1aimageRepository: registry.aliyuncs.com/google_containers - kubernetesVersion\uff1a\u4e3ak8s\u7248\u672c kubernetesVersion: 1.32.0 \u4fee\u6539\u4e3a\uff1akubernetesVersion: 1.32.6 # \u6587\u4ef6\u672b\u5c3e\u589e\u52a0\u542f\u7528ipvs\u529f\u80fd --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: ipvs # \u6839\u636e\u914d\u7f6e\u6587\u4ef6\u542f\u52a8kubeadm\u521d\u59cb\u5316k8s $ kubeadm init --config=kubeadm-init.yaml --upload-certs --v=6 \u8f93\u51fa\u7ed3\u679c\uff1a Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.1.30:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:9d25c16abfec6ff6832ed2260c6c998d3fa6fedef61529d88520d3038bdbdde5 K8S\u96c6\u7fa4\u5de5\u4f5c\u8282\u70b9\u52a0\u5165 # \u6ce8\u610f\uff1a\u52a0\u5165\u96c6\u7fa4\u65f6\u9700\u8981\u6dfb\u52a0 --cri-socket unix:///var/run/cri-dockerd.sock kubeadm join 192.168.1.30:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:9d25c16abfec6ff6832ed2260c6c998d3fa6fedef61529d88520d3038bdbdde5 \\ --cri-socket unix:///var/run/cri-dockerd.sock K8S\u96c6\u7fa4\u7f51\u7edc\u63d2\u4ef6\u4f7f\u7528 # \u4e0b\u8f7dcalico\u8d44\u6e90\u6e05\u5355 wget --no-check-certificate https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml # \u4fee\u6539calico\u6587\u4ef6 vim calico.yaml - name: CALICO_IPV4POOL_CIDR value: \"10.244.0.0/16\" # \u53ef\u4ee5\u5c06\u955c\u50cf\u63d0\u524d\u62c9\u53d6\u4e0b\u6765,\u5982\u679c\u5b98\u7f51\u4ed3\u5e93\u4e0d\u53ef\u8fbe\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u624b\u52a8\u4ecequay.io\u4e0b\u8f7d\u955c\u50cf\uff0cquay.io\u662f\u4e00\u4e2a\u516c\u5171\u955c\u50cf\u4ed3\u5e93\u3002 docker pull calico/cni:v3.28.0 docker pull calico/node:v3.28.0 docker pull calico/kube-controllers:v3.28.0 # \u5e94\u7528calico\u8d44\u6e90\u6e05\u5355 kubectl apply -f calico.yaml Kubectl\u547d\u4ee4\u81ea\u52a8\u8865\u5168 yum -y install bash-completion source /usr/share/bash-completion/bash_completion source <(kubectl completion bash) echo \"source <(kubectl completion bash)\" >> ~/.bashrc \u5b89\u88c5helm v3.16.3 wget https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz tar xf helm-v3.16.3-linux-amd64.tar.gz cd linux-amd64/ mv helm /usr/local/bin helm version \u90e8\u7f72\u52a8\u6001sc\u5b58\u50a8 # k8s-master\u8282\u70b9\u4e0a\u6267\u884c yum -y install nfs-utils echo \"/nfs/data/ *(insecure,rw,sync,no_root_squash)\" > /etc/exports mkdir -p /nfs/data/ chmod 777 -R /nfs/data/ systemctl enable rpcbind systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server exportfs -v \u521b\u5efanfs-provisioner apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner # sa\u540d\u5b57\uff0cnfs-provisioner-deploy\u91cc\u7684\u8981\u5bf9\u5e94 namespace: kube-system # \u547d\u540d\u7a7a\u95f4 --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 # \u521b\u5efa\u96c6\u7fa4\u89c4\u5219 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding # \u5c06\u670d\u52a1\u8ba4\u8bc1\u7528\u6237\u4e0e\u96c6\u7fa4\u89c4\u5219\u8fdb\u884c\u7ed1\u5b9a apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount # \u7c7b\u578b\u4e3asa name: nfs-client-provisioner # sa\u7684\u540d\u5b57\u4e00\u81f4 namespace: kube-system # \u548cnfs provisioner\u5b89\u88c5\u7684namespace\u4e00\u81f4 roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner namespace: kube-system # \u548cnfs provisioner\u5b89\u88c5\u7684namespace\u4e00\u81f4 rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner namespace: kube-system # \u548cnfs provisioner\u5b89\u88c5\u7684namespace\u4e00\u81f4 subjects: - kind: ServiceAccount # \u7c7b\u578b\u4e3asa name: nfs-client-provisioner # sa\u7684\u540d\u5b57\u4e00\u81f4 namespace: kube-system # \u548cnfs provisioner\u5b89\u88c5\u7684namespace\u4e00\u81f4 roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner namespace: kube-system # \u90e8\u7f72\u5728\u6307\u5b9ans\u4e0b spec: replicas: 1 # \u526f\u672c\u6570\uff0c\u5efa\u8bae\u4e3a\u5947\u6570[1\uff0c3\uff0c5\uff0c7\uff0c9] strategy: type: Recreate # \u4f7f\u7528\u91cd\u5efa\u7684\u5347\u7ea7\u7b56\u7565 selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner # sa\u540d\u5b57\uff0c\u8fd9\u4e2a\u662f\u5728nfs-rbac.yaml\u91cc\u5b9a\u4e49 containers: - name: nfs-client-provisioner # \u5bb9\u5668\u540d\u5b57 image: k8s.m.daocloud.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 # \u955c\u50cf\u5730\u5740\uff0c\u8fd9\u91cc\u91c7\u7528\u79c1\u6709\u4ed3\u5e93\u3002 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes # \u6307\u5b9a\u5bb9\u5668\u5185\u6302\u8f7d\u7684\u76ee\u5f55 env: - name: PROVISIONER_NAME # \u5bb9\u5668\u5185\u7684\u53d8\u91cf\u7528\u4e8e\u6307\u5b9a\u63d0\u4f9b\u5b58\u50a8\u7684\u540d\u79f0 value: nfsnas # nfs-provisioner\u7684\u540d\u79f0\uff0c\u4ee5\u540e\u8bbe\u7f6e\u7684storage class\u8981\u548c\u8fd9\u4e2a\u4fdd\u6301\u4e00\u81f4 - name: NFS_SERVER # \u5bb9\u5668\u5185\u7684\u53d8\u91cf\u6307\u5b9anfs\u670d\u52a1\u5668\u5bf9\u5e94\u7684\u76ee\u5f55 value: 192.168.1.254 # NFS\u670d\u52a1\u5668\u7684\u5730\u5740 - name: NFS_PATH # \u5bb9\u5668\u5185\u7684\u53d8\u91cf\u6307\u5b9anfs\u670d\u52a1\u5668\u5bf9\u5e94\u7684\u76ee\u5f55 value: /volume1/\u670d\u52a1/K8s-NFS # NFS\u670d\u52a1\u7684\u6302\u8f7d\u76ee\u5f55\uff0c\u5982\u679c\u91c7\u7528\u8fd9\u4e2anfs\u52a8\u6001\u7533\u8bf7PV,\u6240\u521b\u5efa\u7684\u6587\u4ef6\u5728\u8fd9\u4e2a\u76ee\u5f55\u91cc\uff0c\u4e00\u5b9a\u8981\u7ed9\u6743\u9650\uff0c\u76f4\u63a5777\u3002 volumes: - name: nfs-client-root # \u8d4b\u503c\u5377\u540d\u5b57 nfs: server: 192.168.1.254 # NFS\u670d\u52a1\u5668\u7684\u5730\u5740 path: /volume1/\u670d\u52a1/K8s-NFS # NFS\u670d\u52a1\u7684\u6302\u8f7d\u76ee\u5f55\uff0c\u4e00\u5b9a\u8981\u7ed9\u6743\u9650\uff0c\u76f4\u63a5777\uff0c\u4e0d\u670d\u5c31\u662f\u5e72 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfsnas annotations: storageclass.kubernetes.io/is-default-class: \"true\" # \u8bbe\u4e3a\u9ed8\u8ba4\u5b58\u50a8\u7c7b provisioner: nfsnas # \u5fc5\u987b\u4e0e Deployment \u4e2d PROVISIONER_NAME \u4e00\u81f4 parameters: archiveOnDelete: \"false\" # \"true\" \u8868\u793a\u5220\u9664 PVC \u65f6\u5f52\u6863\u6570\u636e\uff08\u91cd\u547d\u540d\u76ee\u5f55\uff09","title":"K8s 1.32.6\u7248\u672c\u90e8\u7f72\u6587\u6863"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#_1","text":"\u4f5c\u7528 IP\u5730\u5740 \u64cd\u4f5c\u7cfb\u7edf \u914d\u7f6e \u5173\u952e\u7ec4\u4ef6 k8s-master 192.168.1.30 Rocky Linux release 9 4C/4G/50GB kube-apiserver, etcd,,docker k8s-node1 192.168.1.31 Rocky Linux release9 4C/4G/50GB kubelet, kube-proxy,docker k8s-node2 192.168.1.32 Rocky Linux release 9 4C/4G/50GB kubelet, kube-proxy,docker k8s-node3 192.168.1.33 Rocky Linux release 9 4C/4G/50GB kubelet, kube-proxy,docker","title":"\ud83e\udd16\u4e3b\u673a\u914d\u7f6e"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#ip","text":"\u65b9\u5f0f\u4e00\uff1a nmcli connection modify ens160 ipv4.addresses 192.168.1.30/24 ipv4.gateway 192.168.1.1 ipv4.method manual nmcli connection modify ens160 ipv4.addresses 192.168.1.31/24 ipv4.gateway 192.168.1.1 ipv4.method manual nmcli connection modify ens160 ipv4.addresses 192.168.1.32/24 ipv4.gateway 192.168.1.1 ipv4.method manual nmcli connection modify ens160 ipv4.addresses 192.168.1.33/24 ipv4.gateway 192.168.1.1 ipv4.method manual nmcli connection up ens160 \u65b9\u5f0f\u4e8c\uff1a vi /etc/NetworkManager/system-connections/ens160.nmconnection method=manual## \u5728IPV4\u4e0b\u9762\u4fee\u6539\u5982\u4e0b\u5185\u5bb9 address1=192.168.0.5/24,192.168.0.1## \u4fee\u6539IP\uff0c\u5b50\u7f51\u63a9\u7801\uff0824\u662f\u5b50\u7f51\u63a9\u7801\u768424\u4f4d\uff0c\u5bf9\u5e94255.255.255.0\uff09\uff0c \u7f51\u5173 dns=119.29.29.29;114.114.114.114## \u8bbe\u7f6eDNS\u670d\u52a1 may-fail=false \u91cd\u65b0\u52a0\u8f7d\u914d\u7f6e\u6587\u4ef6 nmcli connection reload ens160.nmconnection \u6fc0\u6d3b\u914d\u7f6e\u6587\u4ef6 nmcli connection up ens160","title":"\ud83e\udeaa\u8bbe\u7f6eIP"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#yum","text":"\u914d\u7f6eyum\u6e90 \uff081\uff09\u786e\u8ba4\u6587\u4ef6\u662f\u5426\u5b58\u5728\u4e14\u53ef\u8bfb sudo cat /etc/yum.repos.d/rocky.repo \u5982\u679c\u6587\u4ef6\u4e0d\u5b58\u5728\u6216\u5185\u5bb9\u4e3a\u7a7a\uff0c\u91cd\u65b0\u521b\u5efa\u5b83\u3002 \uff082\uff09\u91cd\u65b0\u4e0b\u8f7d\u6b63\u786e\u7684\u963f\u91cc\u4e91\u6e90\u6587\u4ef6 sudo rm -f /etc/yum.repos.d/rocky.repo # \u5220\u9664\u65e7\u6587\u4ef6\uff08\u5982\u679c\u6709\uff09 sudo curl -o /etc/yum.repos.d/rocky.repo https://mirrors.aliyun.com/rockylinux/rocky.repo?repo=rocky-9 \uff083\uff09\u624b\u52a8\u7f16\u8f91\u6587\u4ef6\uff08\u5982\u679c\u4e0b\u8f7d\u5931\u8d25\uff09 sudo vi /etc/yum.repos.d/rocky.repo \u7c98\u8d34\u4ee5\u4e0b\u5185\u5bb9\uff08\u963f\u91cc\u4e91 Rocky Linux 9 \u955c\u50cf\u6e90\uff09\uff1a [baseos] name=Rocky Linux $releasever - BaseOS - Aliyun baseurl=https://mirrors.aliyun.com/rockylinux/$releasever/BaseOS/$basearch/os/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficial [appstream] name=Rocky Linux $releasever - AppStream - Aliyun baseurl=https://mirrors.aliyun.com/rockylinux/$releasever/AppStream/$basearch/os/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficial [extras] name=Rocky Linux $releasever - Extras - Aliyun baseurl=https://mirrors.aliyun.com/rockylinux/$releasever/extras/$basearch/os/ gpgcheck=1 enabled=1 gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-rockyofficial \uff084\uff09\u4e5f\u53ef\u4ee5\u76f4\u63a5\u66ff\u6362yum\u6e90\u91cc\u7684\u5730\u5740 sed -e 's|^mirrorlist=|#mirrorlist=|g' \\ -e 's|^#baseurl=http://dl.rockylinux.org/$contentdir|baseurl=https://mirrors.aliyun.com/rockylinux|g' \\ -i.bak \\ /etc/yum.repos.d/rocky*.repo \u5f3a\u5236\u66ff\u6362\u53d8\u91cf\u4e3a Rocky Linux 9 \u786e\u4fdd $releasever \u548c $basearch \u88ab\u6b63\u786e\u89e3\u6790\uff1a sudo sed -i 's/$releasever/9/g' /etc/yum.repos.d/rocky.repo sudo sed -i 's/$basearch/x86_64/g' /etc/yum.repos.d/rocky.repo # \u5982\u679c\u662f x86_64 \u67b6\u6784 \u5bfc\u5165 GPG \u5bc6\u94a5 bash sudo rpm --import https://mirrors.aliyun.com/rockylinux/RPM-GPG-KEY-rockyofficial \u68c0\u67e5\u6587\u4ef6\u6743\u9650\u548c\u683c\u5f0f \uff081\uff09\u786e\u4fdd\u6587\u4ef6\u6743\u9650\u6b63\u786e bash sudo chmod 644 /etc/yum.repos.d/rocky.repo \uff082\uff09\u68c0\u67e5\u6587\u4ef6\u683c\u5f0f\uff08\u907f\u514d UTF-8 BOM \u6216 Windows \u6362\u884c\u7b26\uff09 ```bash sudo dos2unix /etc/yum.repos.d/rocky.repo # \u5982\u679c\u662f\u4ece Windows \u590d\u5236\u7684\u6587\u4ef6 ``` \u6e05\u9664\u7f13\u5b58\u5e76\u91cd\u65b0\u52a0\u8f7d bash sudo dnf clean all sudo dnf makecache \u9a8c\u8bc1\u4ed3\u5e93\u662f\u5426\u542f\u7528 bash sudo dnf repolist \u6b63\u5e38\u8f93\u51fa\u5e94\u7c7b\u4f3c\uff1a text repo id repo name baseos Rocky Linux 9 - BaseOS - Aliyun appstream Rocky Linux 9 - AppStream - Aliyun extras Rocky Linux 9 - Extras - Aliyun \u5728 Rocky Linux 9 \u4e2d\u542f\u7528\u5e76\u5b89\u88c5 EPEL Repo\u3002 dnf install epel-release \u5907\u4efd(\u5982\u6709\u914d\u7f6e\u5176\u4ed6epel\u6e90)\u5e76\u66ff\u6362\u4e3a\u56fd\u5185\u955c\u50cf \u6ce8\u610f\u6700\u540e\u8fd9\u4e2a\u5e93\uff0c\u963f\u91cc\u4e91\u6ca1\u6709\u5bf9\u5e94\u7684\u955c\u50cf\uff0c\u4e0d\u8981\u4fee\u6539\u5b83\uff0c\u5982\u679c\u8bef\u6539\u6062\u590d\u539f\u7248\u6e90\u5373\u53ef cp /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backup cp /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backup cp /etc/yum.repos.d/epel-cisco-openh264.repo /etc/yum.repos.d/epel-cisco-openh264.repo.backup \u5c06 repo \u914d\u7f6e\u4e2d\u7684\u5730\u5740\u66ff\u6362\u4e3a\u963f\u91cc\u4e91\u955c\u50cf\u7ad9\u5730\u5740 \u6267\u884c\u4e0b\u9762\u8bed\u53e5\uff0c\u5b83\u4f1a\u66ff\u6362epel.repo\u3001eple-testing.repo\u4e2d\u7684\u7f51\u5740\uff0c\u4e0d\u4f1a\u4fee\u6539epel-cisco-openh264.repo\uff0c\u53ef\u4ee5\u6b63\u5e38\u4f7f\u7528\u3002 sed -e 's!^metalink=!#metalink=!g' \\ -e 's!^#baseurl=!baseurl=!g' \\ -e 's!https\\?://download\\.fedoraproject\\.org/pub/epel!https://mirrors.aliyun.com/epel!g' \\ -e 's!https\\?://download\\.example/pub/epel!https://mirrors.aliyun.com/epel!g' \\ -i /etc/yum.repos.d/epel{,-testing}.repo \u66f4\u65b0\u4ed3\u5e93\u7f13\u5b58 dnf clean all dnf makecache ---\u751f\u6210\u7f13\u5b58\uff0c\u5b89\u88c5\u8f6f\u4ef6\u66f4\u5feb \u6bcf\u53f0\u673a\u5668\u5355\u72ec\u505a hostnamectl set-hostname k8s-master hostnamectl set-hostname k8s-node1 hostnamectl set-hostname k8s-node2 hostnamectl set-hostname k8s-node3 \u8bbe\u7f6ehosts cat >> /etc/hosts << EOF 192.168.1.30 k8s-master 192.168.1.31 k8s-node1 192.168.1.32 k8s-node2 192.168.1.33 k8s-node3 EOF \u914d\u7f6e\u514d\u5bc6\u767b\u5f55\uff0c \u53ea\u5728k8s-master\u4e0a\u64cd\u4f5c [root@k8s-master ~]# ssh-keygen -f ~/.ssh/id_rsa -N '' -q \u62f7\u8d1d\u5bc6\u94a5\u5230\u5176\u4ed63 \u53f0\u8282\u70b9 [root@k8s-master ~]# ssh-copy-id k8s-node1 [root@k8s-master ~]# ssh-copy-id k8s-node2 [root@k8s-master ~]# ssh-copy-id k8s-node3","title":"\ud83d\ude80\u914d\u7f6eYUM\u6e90"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#selinux","text":"# \u5173\u95ed\u9632\u706b\u5899 systemctl disable --now firewalld # \u7981\u7528SELinux sed -i '/^SELINUX=/ c SELINUX=disabled' /etc/selinux/config # \u91cd\u542f\u751f\u6548\u6240\u4ee5\u4e34\u65f6\u8bbe\u7f6e\u4e3a\u5bbd\u5bb9\u6a21\u5f0f setenforce 0","title":"\ud83e\uddf1\u9632\u706b\u5899\u548cSELinux"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#_2","text":"# \u5b89\u88c5\u65f6\u95f4\u670d\u52a1\u5668\u8f6f\u4ef6\u5305 dnf install -y chrony # \u4fee\u6539\u540c\u6b65\u670d\u52a1\u5668 sed -i '/^pool/ c pool ntp1.aliyun.com iburst' /etc/chrony.conf systemctl restart chronyd systemctl enable chronyd chronyc sources","title":"\u23f1\ufe0f\u65f6\u95f4\u540c\u6b65\u914d\u7f6e"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#_3","text":"# \u6dfb\u52a0\u7f51\u6865\u8fc7\u6ee4\u53ca\u5185\u6838\u8f6c\u53d1\u914d\u7f6e\u6587\u4ef6 cat > /etc/sysctl.d/k8s.conf << EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 vm.swappiness = 0 EOF # \u52a0\u8f7dbr_netfilter\u6a21\u5757 modprobe br_netfilter \u4f7f\u7528\u65b0\u6dfb\u52a0\u914d\u7f6e\u6587\u4ef6\u751f\u6548 sysctl -p /etc/sysctl.d/k8s.conf","title":"\ud83d\udede\u914d\u7f6e\u5185\u6838\u8f6c\u53d1\u53ca\u7f51\u6865\u8fc7\u6ee4"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#swap","text":"\u67e5\u770b\u4ea4\u6362\u5206\u533a\u60c5\u51b5 # \u4e34\u65f6\u5173\u95ed swapoff -a # \u6c38\u8fdc\u5173\u95edswap\u5206\u533a sed -i 's/.*swap.*/#&/' /etc/fstab","title":"\ud83d\udcf4\u5173\u95edswap"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#ipvs","text":"cat >> /etc/modules-load.d/ipvs.conf << EOF br_netfilter ip_conntrack ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp ip_vs_sh nf_conntrack ip_tables ip_set xt_set ipt_set ipt_rpfilter ipt_REJECT ipip EOF # \u5b89\u88c5\u4f9d\u8d56 dnf install ipvsadm ipset sysstat conntrack libseccomp -y \u91cd\u542f\u670d\u52a1 systemctl restart systemd-modules-load.service \u67e5\u770b\u6a21\u5757\u5185\u5bb9 lsmod | grep -e ip_vs -e nf_conntrack","title":"\ud83d\udcc4\u542f\u7528ipvs"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#_4","text":"# \u8bbe\u7f6e\u4e3a\u6700\u5927 ulimit -SHn 65535 cat >> /etc/security/limits.conf <<EOF * soft nofile 655360 * hard nofile 131072 * soft nproc 655350 * hard nproc 655350 * seft memlock unlimited * hard memlock unlimitedd EOF # \u67e5\u770b\u4fee\u6539\u7ed3\u679c ulimit -a","title":"\ud83d\udcc3\u53e5\u67c4\u6570\u6700\u5927"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#_5","text":"cat > /etc/sysctl.d/k8s_better.conf << EOF net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 vm.swappiness=0 vm.overcommit_memory=1 vm.panic_on_oom=0 fs.inotify.max_user_instances=8192 fs.inotify.max_user_watches=1048576 fs.file-max=52706963 fs.nr_open=52706963 net.ipv6.conf.all.disable_ipv6=1 net.netfilter.nf_conntrack_max=2310720 EOF modprobe br_netfilter lsmod |grep conntrack modprobe ip_conntrack sysctl -p /etc/sysctl.d/k8s_better.conf","title":"\ud83d\uddc3\ufe0f\u7cfb\u7edf\u4f18\u5316"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#docker","text":"# Step 1: \u5b89\u88c5\u4f9d\u8d56 yum install -y yum-utils device-mapper-persistent-data lvm2 # Step 2: \u6dfb\u52a0\u8f6f\u4ef6\u6e90\u4fe1\u606f yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/rhel/docker-ce.repo # Step 3: \u5b89\u88c5Docker-CE yum -y install docker-ce # docker -v Docker version 27.5.1, build 9f9e405 # \u8bbe\u7f6e\u56fd\u5185\u955c\u50cf\u52a0\u901f mkdir -p /etc/docker/ cat >> /etc/docker/daemon.json << EOF { \"registry-mirrors\":[\"https://p3kgr6db.mirror.aliyuncs.com\", \"https://docker.m.daocloud.io\", \"https://your_id.mirror.aliyuncs.com\", \"https://docker.nju.edu.cn/\", \"https://docker.anyhub.us.kg\", \"https://dockerhub.jobcher.com\", \"https://dockerhub.icu\", \"https://docker.ckyl.me\", \"https://cr.console.aliyun.com\" ], \"exec-opts\": [\"native.cgroupdriver=systemd\"] } EOF # \u8bbe\u7f6edocker\u5f00\u673a\u542f\u52a8\u5e76\u542f\u52a8 systemctl enable --now docker # \u67e5\u770bdocker\u7248\u672c docker version","title":"\ud83d\udc0b\u5b89\u88c5docker"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#cri-dockerd","text":"\u4e0b\u8f7d\u5730\u5740\uff1a Releases \u00b7 Mirantis/cri-dockerd (github.com) \u3002 https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.16/cri-dockerd-0.3.16-3.fc35.x86_64.rpm","title":"\u5b89\u88c5cri-dockerd"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#cri-docker","text":"# \u4e0b\u8f7drpm\u5305 wget -c https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.16/cri-dockerd-0.3.16-3.fc35.x86_64.rpm wget -c https://rpmfind.net/linux/almalinux/8.10/BaseOS/x86_64/os/Packages/libcgroup-0.41-19.el8.x86_64.rpm # \u5b89\u88c5rpm\u5305 yum install libcgroup-0.41-19.el8.x86_64.rpm yum install cri-dockerd-0.3.16-3.fc35.x86_64.rpm","title":"\u5b89\u88c5cri-docker"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#cri-docker_1","text":"systemctl enable cri-docker","title":"\u8bbe\u7f6ecri-docker\u670d\u52a1\u5f00\u673a\u81ea\u542f"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#cri-docke","text":"# \u7f16\u8f91service\u6587\u4ef6 vim /usr/lib/systemd/system/cri-docker.service\u6587\u4ef6 \u4fee\u6539\u7b2c10\u884c\u5185\u5bb9 ------------------ ExecStart=/usr/bin/cri-dockerd --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9 --container-runtime-endpoint fd:// ----------------------------------- # \u91cd\u542fDocker\u7ec4\u4ef6 systemctl daemon-reload && systemctl restart docker cri-docker.socket cri-docker # \u68c0\u67e5Docker\u7ec4\u4ef6\u72b6\u6001 systemctl status docker cir-docker.socket cri-docker","title":"cri-docke\u8bbe\u7f6e\u56fd\u5185\u955c\u50cf\u52a0\u901f"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#k8s","text":"# 1\u3001\u914d\u7f6ekubernetes\u6e90 cat <<EOF | tee /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/ enabled=1 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.32/rpm/repodata/repomd.xml.key EOF # 2\u3001\u67e5\u770b\u6240\u6709\u53ef\u7528\u7684\u7248\u672c yum list kubelet --showduplicates | sort -r |grep 1.32 # 3\u3001\u5b89\u88c5kubelet\u3001kubeadm\u3001kubectl\u3001kubernetes-cni yum install -y kubelet kubeadm kubectl kubernetes-cni # 4\u3001\u914d\u7f6ecgroup \u4e3a\u4e86\u5b9e\u73b0docker\u4f7f\u7528\u7684cgroupdriver\u4e0ekubelet\u4f7f\u7528\u7684cgroup\u7684\u4e00\u81f4\u6027\uff0c\u5efa\u8bae\u4fee\u6539\u5982\u4e0b\u6587\u4ef6\u5185\u5bb9\u3002 vim /etc/sysconfig/kubelet [3\u53f0\u5168\u90e8\u8bbe\u7f6e\u4e0b] --------------------- KUBELET_EXTRA_ARGS=\"--cgroup-driver=systemd\" --------------------- # 5\u3001\u8bbe\u7f6ekubelet\u4e3a\u5f00\u673a\u81ea\u542f\u52a8\u5373\u53ef\uff0c\u7531\u4e8e\u6ca1\u6709\u751f\u6210\u914d\u7f6e\u6587\u4ef6\uff0c\u96c6\u7fa4\u521d\u59cb\u5316\u540e\u81ea\u52a8\u542f\u52a8 systemctl enable kubelet","title":"\u2638\ufe0fK8S\u8f6f\u4ef6\u5b89\u88c5"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#k8s_1","text":"# \u53ea\u5728k8s-master\u8282\u70b9\u4e0a\u64cd\u4f5c [root@localhost ~]# kubeadm config print init-defaults > kubeadm-init.yaml # \u7f16\u8f91kubeadm-init.yaml\u4fee\u6539\u5982\u4e0b\u914d\u7f6e\uff1a - advertiseAddress\uff1a\u4e3a\u63a7\u5236\u5e73\u9762\u5730\u5740\uff0c\uff08Master\u4e3b\u673aIP\uff09 advertiseAddress: 1.2.3.4 \u4fee\u6539\u4e3a advertiseAddress: 192.168.1.30 - criSocket\uff1a\u4e3a containerd \u7684socket \u6587\u4ef6\u5730\u5740 criSocket: unix:///var/run/containerd/containerd.sock \u4fee\u6539\u4e3a criSocket: unix:///var/run/cri-dockerd.sock - name: node \u4fee\u6539node\u4e3ak8s-master name: node \u4fee\u6539\u4e3a name: k8s-master - imageRepository\uff1a\u963f\u91cc\u4e91\u955c\u50cf\u4ee3\u7406\u5730\u5740\uff0c\u5426\u5219\u62c9\u53d6\u955c\u50cf\u4f1a\u5931\u8d25 imageRepository: registry.k8s.io \u4fee\u6539\u4e3a\uff1aimageRepository: registry.aliyuncs.com/google_containers - kubernetesVersion\uff1a\u4e3ak8s\u7248\u672c kubernetesVersion: 1.32.0 \u4fee\u6539\u4e3a\uff1akubernetesVersion: 1.32.6 # \u6587\u4ef6\u672b\u5c3e\u589e\u52a0\u542f\u7528ipvs\u529f\u80fd --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: ipvs # \u6839\u636e\u914d\u7f6e\u6587\u4ef6\u542f\u52a8kubeadm\u521d\u59cb\u5316k8s $ kubeadm init --config=kubeadm-init.yaml --upload-certs --v=6 \u8f93\u51fa\u7ed3\u679c\uff1a Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.1.30:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:9d25c16abfec6ff6832ed2260c6c998d3fa6fedef61529d88520d3038bdbdde5","title":"K8S\u96c6\u7fa4\u521d\u59cb\u5316"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#k8s_2","text":"# \u6ce8\u610f\uff1a\u52a0\u5165\u96c6\u7fa4\u65f6\u9700\u8981\u6dfb\u52a0 --cri-socket unix:///var/run/cri-dockerd.sock kubeadm join 192.168.1.30:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:9d25c16abfec6ff6832ed2260c6c998d3fa6fedef61529d88520d3038bdbdde5 \\ --cri-socket unix:///var/run/cri-dockerd.sock","title":"K8S\u96c6\u7fa4\u5de5\u4f5c\u8282\u70b9\u52a0\u5165"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#k8s_3","text":"# \u4e0b\u8f7dcalico\u8d44\u6e90\u6e05\u5355 wget --no-check-certificate https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml # \u4fee\u6539calico\u6587\u4ef6 vim calico.yaml - name: CALICO_IPV4POOL_CIDR value: \"10.244.0.0/16\" # \u53ef\u4ee5\u5c06\u955c\u50cf\u63d0\u524d\u62c9\u53d6\u4e0b\u6765,\u5982\u679c\u5b98\u7f51\u4ed3\u5e93\u4e0d\u53ef\u8fbe\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u624b\u52a8\u4ecequay.io\u4e0b\u8f7d\u955c\u50cf\uff0cquay.io\u662f\u4e00\u4e2a\u516c\u5171\u955c\u50cf\u4ed3\u5e93\u3002 docker pull calico/cni:v3.28.0 docker pull calico/node:v3.28.0 docker pull calico/kube-controllers:v3.28.0 # \u5e94\u7528calico\u8d44\u6e90\u6e05\u5355 kubectl apply -f calico.yaml Kubectl\u547d\u4ee4\u81ea\u52a8\u8865\u5168 yum -y install bash-completion source /usr/share/bash-completion/bash_completion source <(kubectl completion bash) echo \"source <(kubectl completion bash)\" >> ~/.bashrc","title":"K8S\u96c6\u7fa4\u7f51\u7edc\u63d2\u4ef6\u4f7f\u7528"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#helm-v3163","text":"wget https://get.helm.sh/helm-v3.16.3-linux-amd64.tar.gz tar xf helm-v3.16.3-linux-amd64.tar.gz cd linux-amd64/ mv helm /usr/local/bin helm version","title":"\u5b89\u88c5helm v3.16.3"},{"location":"%E6%8A%80%E6%9C%AF/K8s%201.32.6%E7%89%88%E6%9C%AC%E9%83%A8%E7%BD%B2%E6%96%87%E6%A1%A3/#sc","text":"# k8s-master\u8282\u70b9\u4e0a\u6267\u884c yum -y install nfs-utils echo \"/nfs/data/ *(insecure,rw,sync,no_root_squash)\" > /etc/exports mkdir -p /nfs/data/ chmod 777 -R /nfs/data/ systemctl enable rpcbind systemctl enable nfs-server systemctl start rpcbind systemctl start nfs-server exportfs -v \u521b\u5efanfs-provisioner apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner # sa\u540d\u5b57\uff0cnfs-provisioner-deploy\u91cc\u7684\u8981\u5bf9\u5e94 namespace: kube-system # \u547d\u540d\u7a7a\u95f4 --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 # \u521b\u5efa\u96c6\u7fa4\u89c4\u5219 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding # \u5c06\u670d\u52a1\u8ba4\u8bc1\u7528\u6237\u4e0e\u96c6\u7fa4\u89c4\u5219\u8fdb\u884c\u7ed1\u5b9a apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount # \u7c7b\u578b\u4e3asa name: nfs-client-provisioner # sa\u7684\u540d\u5b57\u4e00\u81f4 namespace: kube-system # \u548cnfs provisioner\u5b89\u88c5\u7684namespace\u4e00\u81f4 roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner namespace: kube-system # \u548cnfs provisioner\u5b89\u88c5\u7684namespace\u4e00\u81f4 rules: - apiGroups: [\"\"] resources: [\"endpoints\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: leader-locking-nfs-client-provisioner namespace: kube-system # \u548cnfs provisioner\u5b89\u88c5\u7684namespace\u4e00\u81f4 subjects: - kind: ServiceAccount # \u7c7b\u578b\u4e3asa name: nfs-client-provisioner # sa\u7684\u540d\u5b57\u4e00\u81f4 namespace: kube-system # \u548cnfs provisioner\u5b89\u88c5\u7684namespace\u4e00\u81f4 roleRef: kind: Role name: leader-locking-nfs-client-provisioner apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: name: nfs-client-provisioner labels: app: nfs-client-provisioner namespace: kube-system # \u90e8\u7f72\u5728\u6307\u5b9ans\u4e0b spec: replicas: 1 # \u526f\u672c\u6570\uff0c\u5efa\u8bae\u4e3a\u5947\u6570[1\uff0c3\uff0c5\uff0c7\uff0c9] strategy: type: Recreate # \u4f7f\u7528\u91cd\u5efa\u7684\u5347\u7ea7\u7b56\u7565 selector: matchLabels: app: nfs-client-provisioner template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner # sa\u540d\u5b57\uff0c\u8fd9\u4e2a\u662f\u5728nfs-rbac.yaml\u91cc\u5b9a\u4e49 containers: - name: nfs-client-provisioner # \u5bb9\u5668\u540d\u5b57 image: k8s.m.daocloud.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2 # \u955c\u50cf\u5730\u5740\uff0c\u8fd9\u91cc\u91c7\u7528\u79c1\u6709\u4ed3\u5e93\u3002 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes # \u6307\u5b9a\u5bb9\u5668\u5185\u6302\u8f7d\u7684\u76ee\u5f55 env: - name: PROVISIONER_NAME # \u5bb9\u5668\u5185\u7684\u53d8\u91cf\u7528\u4e8e\u6307\u5b9a\u63d0\u4f9b\u5b58\u50a8\u7684\u540d\u79f0 value: nfsnas # nfs-provisioner\u7684\u540d\u79f0\uff0c\u4ee5\u540e\u8bbe\u7f6e\u7684storage class\u8981\u548c\u8fd9\u4e2a\u4fdd\u6301\u4e00\u81f4 - name: NFS_SERVER # \u5bb9\u5668\u5185\u7684\u53d8\u91cf\u6307\u5b9anfs\u670d\u52a1\u5668\u5bf9\u5e94\u7684\u76ee\u5f55 value: 192.168.1.254 # NFS\u670d\u52a1\u5668\u7684\u5730\u5740 - name: NFS_PATH # \u5bb9\u5668\u5185\u7684\u53d8\u91cf\u6307\u5b9anfs\u670d\u52a1\u5668\u5bf9\u5e94\u7684\u76ee\u5f55 value: /volume1/\u670d\u52a1/K8s-NFS # NFS\u670d\u52a1\u7684\u6302\u8f7d\u76ee\u5f55\uff0c\u5982\u679c\u91c7\u7528\u8fd9\u4e2anfs\u52a8\u6001\u7533\u8bf7PV,\u6240\u521b\u5efa\u7684\u6587\u4ef6\u5728\u8fd9\u4e2a\u76ee\u5f55\u91cc\uff0c\u4e00\u5b9a\u8981\u7ed9\u6743\u9650\uff0c\u76f4\u63a5777\u3002 volumes: - name: nfs-client-root # \u8d4b\u503c\u5377\u540d\u5b57 nfs: server: 192.168.1.254 # NFS\u670d\u52a1\u5668\u7684\u5730\u5740 path: /volume1/\u670d\u52a1/K8s-NFS # NFS\u670d\u52a1\u7684\u6302\u8f7d\u76ee\u5f55\uff0c\u4e00\u5b9a\u8981\u7ed9\u6743\u9650\uff0c\u76f4\u63a5777\uff0c\u4e0d\u670d\u5c31\u662f\u5e72 --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfsnas annotations: storageclass.kubernetes.io/is-default-class: \"true\" # \u8bbe\u4e3a\u9ed8\u8ba4\u5b58\u50a8\u7c7b provisioner: nfsnas # \u5fc5\u987b\u4e0e Deployment \u4e2d PROVISIONER_NAME \u4e00\u81f4 parameters: archiveOnDelete: \"false\" # \"true\" \u8868\u793a\u5220\u9664 PVC \u65f6\u5f52\u6863\u6570\u636e\uff08\u91cd\u547d\u540d\u76ee\u5f55\uff09","title":"\u90e8\u7f72\u52a8\u6001sc\u5b58\u50a8"},{"location":"%E6%8A%80%E6%9C%AF/Linux%E7%B3%BB%E7%BB%9F%E4%BC%98%E5%8C%96%E8%84%9A%E6%9C%AC/","text":"#!/bin/bash export LANG=en_US.UTF-8 #\u8fd4\u56de\u811a\u672c\u6587\u4ef6\u5b58\u653e\u8def\u5f84 script_path=$(cd `dirname $0`; pwd) sys_date=`date +\"%Y%m%d%H%M\"` monitor=/etc/profile.d/monitor.sh monitorlog_path=/var/log/.hist user_name=admin user_passwd='zxcvbnm!@#123' #\u5224\u65ad\u7cfb\u7edf\u53d1\u884c\u7248\u672c\u5927\u7248\u672c\u53f7\uff0ccentos 6\u3001centos 7 release_el6=`uname -a|awk '{print $3}'|grep -v grep|grep el6|wc -l` release_el7=`uname -a|awk '{print $3}'|grep -v grep|grep el7|wc -l` #\u5224\u65ad\u7cfb\u7edf\u53d1\u884c\u7248\u672c\u5927\u7248\u672c\u53f7\uff0ccentos 6\u3001centos 7 #\u7cfb\u7edf\u6700\u5927\u6587\u4ef6\u6253\u5f00\u6570 function sys_limits() { #\u4fee\u6539\u7cfb\u7edf\u6700\u5927\u6587\u4ef6\u6253\u5f00\u6570 limits=`cat /etc/security/limits.conf|grep -v grep|grep -v \"^#\"|grep -E \"65536|65536\"|wc -l` if [ $limits -le 1 ] ; then echo -e \"\\n\" >> /etc/security/limits.conf echo -e \"* soft nofile 65536\" >> /etc/security/limits.conf echo -e \"* soft nproc 65536\\n\" >> /etc/security/limits.conf echo -e \"* hard nofile 65536\\n\" >> /etc/security/limits.conf echo -e \"* hard nproc 65536\\n\" >> /etc/security/limits.conf wait sed -i \"/ulimit/d\" /etc/profile echo -e \"ulimit -n 65536\" >> /etc/profile wait #Java\u7a0b\u5e8f\u4f18\u5316---20190904\u65b0\u589e #\u89e3\u51b3Java\u7a0b\u5e8f\u5728Linux\u4e0a\u8fd0\u884c\u865a\u62df\u5185\u5b58\u8017\u7528\u5927\u7684\u95ee\u9898 sed -i '/'MALLOC_ARENA_MAX'/d' /etc/profile echo -e \"export MALLOC_ARENA_MAX=1\" >> /etc/profile wait source /etc/profile echo \"\u7cfb\u7edf\u6700\u5927\u6587\u4ef6\u6253\u5f00\u6570\u4fee\u6539\u5b8c\u6210\uff01\" fi } function sys_timezone() { ###\u68c0\u67e5\u65f6\u533a\u914d\u7f6e local time_zone=`date -R | awk '{print $6}'` if [ \"$time_zone\" = \"+0800\" ] ; then echo \"\" echo -e \"\u7cfb\u7edf\u65f6\u533a\u8bbe\u7f6e\u6b63\u786e\uff01\\n\" echo \"\" else echo \"\" echo -e \"\\n\u7cfb\u7edf\u65f6\u533a\u8bbe\u7f6e\u9519\u8bef\uff01\\n\" echo -e \"\u5f00\u59cb\u66f4\u6539\u65f6\u533a\u8bbe\u7f6e.........\\n\" sleep 2 yes|cp -f /usr/share/zoneinfo/Asia/Shanghai /etc/localtime sleep 2 ###\u5d4c\u5957if\u51fd\u6570 1*** unset time_zone local time_zone=`date -R | awk '{print $6}'` if [ \"$time_zone\" = \"+0800\" ] ; then echo -e \"\\033[32m\u7cfb\u7edf\u65f6\u533a\u66f4\u6539\u6210\u529f\uff01\\n\\033[0m\" #\u7eff\u8272\u5b57 else echo -e \"\\033[31m\u7cfb\u7edf\u65f6\u533a\u66f4\u6539\u5931\u8d25\uff01\\n\\033[0m\" #\u7ea2\u8272\u5b57 exit fi ###\u5d4c\u5957if\u51fd\u6570 1*** fi } #\u5185\u6838\u4f18\u5316 function kernel_optimize() { echo \"\u6267\u884c\u5185\u6838\u4f18\u5316..........\" #\u4e0d\u7ba1\u53c2\u6570\u5728\u4e0d\u5728\uff0c\u5148\u5220\u9664\uff0c\u907f\u514d\u91cd\u590d\u6dfb\u52a0 sed -i '/'fs.file-max'/d' /etc/sysctl.conf sed -i '/'net.ipv4.tcp_max_tw_buckets'/d' /etc/sysctl.conf sed -i '/'net.ipv4.tcp_max_syn_backlog'/d' /etc/sysctl.conf sed -i '/'net.ipv4.tcp_timestamps'/d' /etc/sysctl.conf sed -i '/'net.ipv4.ip_local_port_range'/d' /etc/sysctl.conf sed -i '/'net.ipv4.tcp_tw_recycle'/d' /etc/sysctl.conf sed -i '/'net.ipv4.tcp_tw_reuse'/d' /etc/sysctl.conf sed -i '/'net.ipv4.tcp_fin_timeout'/d' /etc/sysctl.conf sed -i '/'vm.swappiness'/d' /etc/sysctl.conf sed -i '/'net.core.netdev_max_backlog'/d' /etc/sysctl.conf sed -i '/'net.ipv4.tcp_syn_retries'/d' /etc/sysctl.conf wait #\u6dfb\u52a0\u5185\u6838\u4f18\u5316\u53c2\u6570 echo -e ' fs.file-max = 999999 net.core.netdev_max_backlog = 8096 net.ipv4.tcp_syn_retries = 1 net.ipv4.tcp_max_tw_buckets = 6000 net.ipv4.tcp_max_syn_backlog = 10000 net.ipv4.tcp_timestamps = 0 net.ipv4.ip_local_port_range = 1024 65500 net.ipv4.tcp_tw_recycle = 1 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_fin_timeout = 30 vm.swappiness = 0 ' >> /etc/sysctl.conf echo \"------------------------\" sed -i '/'bridge'/d' /etc/rc.local echo \"modprobe bridge\" >> /etc/rc.local modprobe bridge sysctl -p echo \"------------------------\" } ###\u8c03\u7528\u51fd\u6570 sys_limits sys_timezone kernel_optimize","title":"Linux\u7cfb\u7edf\u4f18\u5316\u811a\u672c"},{"location":"%E6%8A%80%E6%9C%AF/kubectl%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","text":"kubectl \u5e38\u7528\u547d\u4ee4 1\u3001\u7ba1\u7406\u8d44\u6e90 \u547d\u4ee4 \u63cf\u8ff0 kubectl create \u521b\u5efa\u8d44\u6e90\u5bf9\u8c61\uff0c\u5982Pod\u3001Deployment\u7b49 kubectl apply \u5e94\u7528\u914d\u7f6e\u6587\u4ef6\u6216\u8d44\u6e90\u5bf9\u8c61\u7684\u66f4\u6539 kubectl get \u83b7\u53d6\u8d44\u6e90\u5bf9\u8c61\u7684\u4fe1\u606f kubectl describe \u663e\u793a\u8d44\u6e90\u5bf9\u8c61\u7684\u8be6\u7ec6\u4fe1\u606f kubectl edit \u7f16\u8f91\u8d44\u6e90\u5bf9\u8c61\u7684\u914d\u7f6e kubectl delete \u5220\u9664\u8d44\u6e90\u5bf9\u8c61 kubectl replace \u7528\u65b0\u7684\u8d44\u6e90\u5bf9\u8c61\u66ff\u6362\u73b0\u6709\u7684\u8d44\u6e90\u5bf9\u8c61 kubectl scale \u8c03\u6574Deployment\u6216ReplicaSet\u7684\u526f\u672c\u6570 kubectl rollout \u7ba1\u7406\u6eda\u52a8\u5347\u7ea7Deployment\u7684\u8fc7\u7a0b 2\u3001\u67e5\u770b\u8d44\u6e90 \u547d\u4ee4 \u63cf\u8ff0 kubectl get \u83b7\u53d6\u8d44\u6e90\u5bf9\u8c61\u7684\u4fe1\u606f kubectl describe \u663e\u793a\u8d44\u6e90\u5bf9\u8c61\u7684\u8be6\u7ec6\u4fe1\u606f kubectl logs \u67e5\u770bPod\u7684\u65e5\u5fd7 kubectl port-forward \u5c06\u672c\u5730\u7aef\u53e3\u6620\u5c04\u5230Pod\u4e0a\u7684\u7aef\u53e3 kubectl top \u67e5\u770b\u8d44\u6e90\u4f7f\u7528\u60c5\u51b5\uff08CPU\u3001\u5185\u5b58\u7b49\uff09 3\u3001\u6545\u969c\u6392\u9664\u548c\u8bca\u65ad \u547d\u4ee4 \u63cf\u8ff0 kubectl exec \u5728Pod\u5185\u90e8\u6267\u884c\u547d\u4ee4 kubectl attach \u8fde\u63a5\u5230Pod\u5e76\u67e5\u770b\u5176\u8f93\u51fa kubectl cp \u4ecePod\u590d\u5236\u6587\u4ef6\u5230\u672c\u5730\uff0c\u6216\u4ece\u672c\u5730\u590d\u5236\u5230Pod kubectl describe \u663e\u793a\u8d44\u6e90\u5bf9\u8c61\u7684\u8be6\u7ec6\u4fe1\u606f\uff08\u7528\u4e8e\u6392\u67e5\u95ee\u9898\uff09 kubectl logs \u67e5\u770bPod\u7684\u65e5\u5fd7\uff08\u7528\u4e8e\u6392\u67e5\u95ee\u9898\uff09 4\u3001\u96c6\u7fa4\u7ba1\u7406 \u547d\u4ee4 \u63cf\u8ff0 kubectl cluster-info \u67e5\u770b\u96c6\u7fa4\u4fe1\u606f kubectl config \u7ba1\u7406Kubernetes\u914d\u7f6e\u6587\u4ef6 kubectl version \u67e5\u770bKubernetes\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u5668\u7248\u672c\u4fe1\u606f kubectl proxy \u542f\u52a8\u672c\u5730\u4ee3\u7406\u4ee5\u8bbf\u95eeKubernetes API kubectl taint \u6dfb\u52a0\u6216\u79fb\u9664\u8282\u70b9\u7684\u6c61\u70b9 kubectl uncordon \u4f7f\u8282\u70b9\u91cd\u65b0\u53ef\u8c03\u5ea6 kubectl drain \u9a71\u9010\u8282\u70b9\u4e0a\u7684Pod\u4ee5\u7ef4\u62a4\u8282\u70b9 kubectl cordon \u963b\u6b62\u8282\u70b9\u8c03\u5ea6\u65b0\u7684Pod 5\u3001\u5b89\u5168\u6027\u548c\u8eab\u4efd\u9a8c\u8bc1 \u547d\u4ee4 \u63cf\u8ff0 kubectl auth \u7ba1\u7406\u8eab\u4efd\u9a8c\u8bc1\u548c\u6388\u6743 kubectl create serviceaccount \u521b\u5efa\u670d\u52a1\u8d26\u53f7 kubectl create role \u521b\u5efa\u89d2\u8272 kubectl create rolebinding \u521b\u5efa\u89d2\u8272\u7ed1\u5b9a 6\u3001\u5176\u4ed6 \u547d\u4ee4 \u63cf\u8ff0 kubectl label \u4e3a\u8d44\u6e90\u5bf9\u8c61\u6dfb\u52a0\u6807\u7b7e\u6216\u4fee\u6539\u6807\u7b7e kubectl annotate \u4e3a\u8d44\u6e90\u5bf9\u8c61\u6dfb\u52a0\u6ce8\u91ca\u6216\u4fee\u6539\u6ce8\u91ca kubectl rollout \u7ba1\u7406\u6eda\u52a8\u5347\u7ea7Deployment\u7684\u8fc7\u7a0b kubectl explain \u67e5\u770bKubernetes\u8d44\u6e90\u5bf9\u8c61\u7684\u5b9a\u4e49\u548c\u5b57\u6bb5\u542b\u4e49 \u8bf7\u6ce8\u610f\uff0c\u8fd9\u53ea\u662f\u4e00\u4e9b\u5e38\u89c1\u7684kubectl\u547d\u4ee4\uff0cKubernetes\u6709\u5f88\u591a\u529f\u80fd\u548c\u8d44\u6e90\u7c7b\u578b\uff0c\u56e0\u6b64\u8fd8\u6709\u5176\u4ed6\u547d\u4ee4\u548c\u9009\u9879\u53ef\u4f9b\u4f7f\u7528\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528 kubectl --help \u67e5\u770bkubectl\u7684\u5168\u5c40\u9009\u9879\u4ee5\u53ca\u5404\u4e2a\u5b50\u547d\u4ee4\u7684\u9009\u9879\u548c\u7528\u6cd5\u3002","title":"kubectl \u5e38\u7528\u547d\u4ee4"},{"location":"%E6%8A%80%E6%9C%AF/kubectl%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#kubectl","text":"","title":"kubectl \u5e38\u7528\u547d\u4ee4"},{"location":"%E6%8A%80%E6%9C%AF/kubectl%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#1","text":"\u547d\u4ee4 \u63cf\u8ff0 kubectl create \u521b\u5efa\u8d44\u6e90\u5bf9\u8c61\uff0c\u5982Pod\u3001Deployment\u7b49 kubectl apply \u5e94\u7528\u914d\u7f6e\u6587\u4ef6\u6216\u8d44\u6e90\u5bf9\u8c61\u7684\u66f4\u6539 kubectl get \u83b7\u53d6\u8d44\u6e90\u5bf9\u8c61\u7684\u4fe1\u606f kubectl describe \u663e\u793a\u8d44\u6e90\u5bf9\u8c61\u7684\u8be6\u7ec6\u4fe1\u606f kubectl edit \u7f16\u8f91\u8d44\u6e90\u5bf9\u8c61\u7684\u914d\u7f6e kubectl delete \u5220\u9664\u8d44\u6e90\u5bf9\u8c61 kubectl replace \u7528\u65b0\u7684\u8d44\u6e90\u5bf9\u8c61\u66ff\u6362\u73b0\u6709\u7684\u8d44\u6e90\u5bf9\u8c61 kubectl scale \u8c03\u6574Deployment\u6216ReplicaSet\u7684\u526f\u672c\u6570 kubectl rollout \u7ba1\u7406\u6eda\u52a8\u5347\u7ea7Deployment\u7684\u8fc7\u7a0b","title":"1\u3001\u7ba1\u7406\u8d44\u6e90"},{"location":"%E6%8A%80%E6%9C%AF/kubectl%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#2","text":"\u547d\u4ee4 \u63cf\u8ff0 kubectl get \u83b7\u53d6\u8d44\u6e90\u5bf9\u8c61\u7684\u4fe1\u606f kubectl describe \u663e\u793a\u8d44\u6e90\u5bf9\u8c61\u7684\u8be6\u7ec6\u4fe1\u606f kubectl logs \u67e5\u770bPod\u7684\u65e5\u5fd7 kubectl port-forward \u5c06\u672c\u5730\u7aef\u53e3\u6620\u5c04\u5230Pod\u4e0a\u7684\u7aef\u53e3 kubectl top \u67e5\u770b\u8d44\u6e90\u4f7f\u7528\u60c5\u51b5\uff08CPU\u3001\u5185\u5b58\u7b49\uff09","title":"2\u3001\u67e5\u770b\u8d44\u6e90"},{"location":"%E6%8A%80%E6%9C%AF/kubectl%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#3","text":"\u547d\u4ee4 \u63cf\u8ff0 kubectl exec \u5728Pod\u5185\u90e8\u6267\u884c\u547d\u4ee4 kubectl attach \u8fde\u63a5\u5230Pod\u5e76\u67e5\u770b\u5176\u8f93\u51fa kubectl cp \u4ecePod\u590d\u5236\u6587\u4ef6\u5230\u672c\u5730\uff0c\u6216\u4ece\u672c\u5730\u590d\u5236\u5230Pod kubectl describe \u663e\u793a\u8d44\u6e90\u5bf9\u8c61\u7684\u8be6\u7ec6\u4fe1\u606f\uff08\u7528\u4e8e\u6392\u67e5\u95ee\u9898\uff09 kubectl logs \u67e5\u770bPod\u7684\u65e5\u5fd7\uff08\u7528\u4e8e\u6392\u67e5\u95ee\u9898\uff09","title":"3\u3001\u6545\u969c\u6392\u9664\u548c\u8bca\u65ad"},{"location":"%E6%8A%80%E6%9C%AF/kubectl%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#4","text":"\u547d\u4ee4 \u63cf\u8ff0 kubectl cluster-info \u67e5\u770b\u96c6\u7fa4\u4fe1\u606f kubectl config \u7ba1\u7406Kubernetes\u914d\u7f6e\u6587\u4ef6 kubectl version \u67e5\u770bKubernetes\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u5668\u7248\u672c\u4fe1\u606f kubectl proxy \u542f\u52a8\u672c\u5730\u4ee3\u7406\u4ee5\u8bbf\u95eeKubernetes API kubectl taint \u6dfb\u52a0\u6216\u79fb\u9664\u8282\u70b9\u7684\u6c61\u70b9 kubectl uncordon \u4f7f\u8282\u70b9\u91cd\u65b0\u53ef\u8c03\u5ea6 kubectl drain \u9a71\u9010\u8282\u70b9\u4e0a\u7684Pod\u4ee5\u7ef4\u62a4\u8282\u70b9 kubectl cordon \u963b\u6b62\u8282\u70b9\u8c03\u5ea6\u65b0\u7684Pod","title":"4\u3001\u96c6\u7fa4\u7ba1\u7406"},{"location":"%E6%8A%80%E6%9C%AF/kubectl%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#5","text":"\u547d\u4ee4 \u63cf\u8ff0 kubectl auth \u7ba1\u7406\u8eab\u4efd\u9a8c\u8bc1\u548c\u6388\u6743 kubectl create serviceaccount \u521b\u5efa\u670d\u52a1\u8d26\u53f7 kubectl create role \u521b\u5efa\u89d2\u8272 kubectl create rolebinding \u521b\u5efa\u89d2\u8272\u7ed1\u5b9a","title":"5\u3001\u5b89\u5168\u6027\u548c\u8eab\u4efd\u9a8c\u8bc1"},{"location":"%E6%8A%80%E6%9C%AF/kubectl%20%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/#6","text":"\u547d\u4ee4 \u63cf\u8ff0 kubectl label \u4e3a\u8d44\u6e90\u5bf9\u8c61\u6dfb\u52a0\u6807\u7b7e\u6216\u4fee\u6539\u6807\u7b7e kubectl annotate \u4e3a\u8d44\u6e90\u5bf9\u8c61\u6dfb\u52a0\u6ce8\u91ca\u6216\u4fee\u6539\u6ce8\u91ca kubectl rollout \u7ba1\u7406\u6eda\u52a8\u5347\u7ea7Deployment\u7684\u8fc7\u7a0b kubectl explain \u67e5\u770bKubernetes\u8d44\u6e90\u5bf9\u8c61\u7684\u5b9a\u4e49\u548c\u5b57\u6bb5\u542b\u4e49 \u8bf7\u6ce8\u610f\uff0c\u8fd9\u53ea\u662f\u4e00\u4e9b\u5e38\u89c1\u7684kubectl\u547d\u4ee4\uff0cKubernetes\u6709\u5f88\u591a\u529f\u80fd\u548c\u8d44\u6e90\u7c7b\u578b\uff0c\u56e0\u6b64\u8fd8\u6709\u5176\u4ed6\u547d\u4ee4\u548c\u9009\u9879\u53ef\u4f9b\u4f7f\u7528\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528 kubectl --help \u67e5\u770bkubectl\u7684\u5168\u5c40\u9009\u9879\u4ee5\u53ca\u5404\u4e2a\u5b50\u547d\u4ee4\u7684\u9009\u9879\u548c\u7528\u6cd5\u3002","title":"6\u3001\u5176\u4ed6"},{"location":"%E6%8A%80%E6%9C%AF/kubernetes%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E7%9A%84%E7%89%88%E6%9C%AC%E5%85%BC%E5%AE%B9%E6%80%A7%E5%88%86%E6%9E%90/","text":"\u4e00\u3001calico\u4e0ekubernetes\u517c\u5bb9\u6027\u5bf9\u7167\u8868 calico kubernetes\u7248\u672c \u5b98\u7f51\u5730\u5740 v3.12 v1.14 v1.15 v1.16 v1.17 https://docs.tigera.io/archive/v3.12/getting-started/kubernetes/requirements v.3.13 v1.15 v1.16 v1.17 https://docs.tigera.io/archive/v3.13/getting-started/kubernetes/requirements v3.14 v1.16 v1.17 v1.18 https://docs.tigera.io/archive/v3.14/getting-started/kubernetes/requirements v3.15 v1.16 v1.17 v1.18 https://docs.tigera.io/archive/v3.15/getting-started/kubernetes/requirements v3.16 v1.16 v1.17 v1.18 v1.19 https://docs.tigera.io/archive/v3.16/getting-started/kubernetes/requirements v3.17 v1.17 v1.18 v1.19 https://docs.tigera.io/archive/v3.17/getting-started/kubernetes/requirements v3.18 v1.18 v1.19 v1.20 https://docs.tigera.io/archive/v3.18/getting-started/kubernetes/requirements v3.19 v1.19 v1.20 v1.21 https://docs.tigera.io/archive/v3.19/getting-started/kubernetes/requirements v3.20 v1.19 v1.20 v1.21 https://docs.tigera.io/archive/v3.20/getting-started/kubernetes/requirements v3.21 v1.20 v1.21 v1.22 https://docs.tigera.io/archive/v3.21/getting-started/kubernetes/requirements v3.22 v1.21 v1.22 v1.23 https://docs.tigera.io/archive/v3.22/getting-started/kubernetes/requirements v3.23 v1.21 v1.22 v1.23 https://docs.tigera.io/archive/v3.23/getting-started/kubernetes/requirements v3.24 v1.22 v1.23 v1.24 v1.25 https://docs.tigera.io/calico/3.24/getting-started/kubernetes/requirements v3.25 v1.23 v1.24 v1.25 v1.26 https://docs.tigera.io/calico/3.25/getting-started/kubernetes/requirements v3.26 v1.24 v1.25 v1.26 v1.27 https://docs.tigera.io/calico/latest/getting-started/kubernetes/requirements \u4e8c\u3001docker\u4e0ekubernetes\u517c\u5bb9\u6027\u5bf9\u7167\u8868 k8s\u7248\u672c docker\u7248\u672c \u67e5\u770b\u5730\u5740 v1.15 1.13.1 17.03 17.06 17.09 18.06 18.09 https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.15.md#server-binaries v1.16 1.13.1 17.03 17.06 17.09 18.06 18.09 https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.16.md v1.17 19.03 https://www.cnblogs.com/jinzhenshui/p/16724379.html v1.18 19.03\u6216\u66f4\u9ad8 https://zhuanlan.zhihu.com/p/121100475 v1.19 19.03\u6216\u66f4\u9ad8 v1.20 19.03\u6216\u66f4\u9ad8 https://blog.csdn.net/alwaysbefine/article/details/119658708 v1.21 20.10\u6216\u66f4\u9ad8 https://www.cnblogs.com/superlinux/p/14676959.html v1.22 20.10\u6216\u66f4\u9ad8 https://blog.csdn.net/qq_30614345/article/details/131334199?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EYuanLiJiHua%7EPosition-2-131334199-blog-119430082.235%5Ev38%5Epc_relevant_default_base&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EYuanLiJiHua%7EPosition-2-131334199-blog-119430082.235%5Ev38%5Epc_relevant_default_base&utm_relevant_index=3 v1.23 20.10\u6216\u66f4\u9ad8 https://blog.csdn.net/yy8623977/article/details/124685772 v1.24 20.10\u6216\u66f4\u9ad8 *k8s v1.24\u4ee5\u540e\u9700\u8981\u989d\u5916\u5b89\u88c5cri-dockerd, k8s\u624d\u80fd\u591f\u6b63\u5e38\u8bc6\u522b\u5230Docker\u3002* v1.25 20.10\u6216\u66f4\u9ad8 https://blog.51cto.com/dayu/5825840 v1.26 20.10\u6216\u66f4\u9ad8 v1.27 20.10\u6216\u66f4\u9ad8 https://blog.csdn.net/zo2k123/article/details/130328617 \u4e09\u3001etcd\u4e0ekubernetes\u517c\u5bb9\u6027\u5bf9\u7167\u8868 etcd\u7248\u672c kubernetes\u7248\u672c \u67e5\u770b\u5730\u5740 3.2.x v1.11 v1.12 https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md 3.3.x v1.13 v1.14 v1.15 v1.16 3.4.x v1.17 v1.18 v1.19 v1.20 v1.21 3.5.x v1.22 v1.23 v1.24 v1.25 v1.26 v1.27 \u56db\u3001coredns\u4e0ekubernetes\u517c\u5bb9\u6027\u5bf9\u7167\u8868 coredns\u7248\u672c kubernetes\u7248\u672c \u67e5\u770b\u5730\u5740 1.0.1 v1.09 https://github.com/coredns/deployment/blob/master/kubernetes/CoreDNS-k8s_version.md 1.0.6 v1.10 1.1.3 v1.11 1.2.2 v1.12 1.2.6 v1.13 1.3.1 v1.14 v1.15 1.6.2 v1.16 1.6.5 v1.17 1.6.7 v1.18 1.7.0 v1.19 v1.20 1.8.0 v1.21 1.8.4 v1.22 1.8.6 v1.23 1.9.3 v1.25 v1.26 1.10.1 v1.27","title":"Kubernetes\u5404\u4e2a\u7ec4\u4ef6\u7684\u7248\u672c\u517c\u5bb9\u6027\u5206\u6790"},{"location":"%E6%8A%80%E6%9C%AF/kubernetes%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E7%9A%84%E7%89%88%E6%9C%AC%E5%85%BC%E5%AE%B9%E6%80%A7%E5%88%86%E6%9E%90/#calicokubernetes","text":"calico kubernetes\u7248\u672c \u5b98\u7f51\u5730\u5740 v3.12 v1.14 v1.15 v1.16 v1.17 https://docs.tigera.io/archive/v3.12/getting-started/kubernetes/requirements v.3.13 v1.15 v1.16 v1.17 https://docs.tigera.io/archive/v3.13/getting-started/kubernetes/requirements v3.14 v1.16 v1.17 v1.18 https://docs.tigera.io/archive/v3.14/getting-started/kubernetes/requirements v3.15 v1.16 v1.17 v1.18 https://docs.tigera.io/archive/v3.15/getting-started/kubernetes/requirements v3.16 v1.16 v1.17 v1.18 v1.19 https://docs.tigera.io/archive/v3.16/getting-started/kubernetes/requirements v3.17 v1.17 v1.18 v1.19 https://docs.tigera.io/archive/v3.17/getting-started/kubernetes/requirements v3.18 v1.18 v1.19 v1.20 https://docs.tigera.io/archive/v3.18/getting-started/kubernetes/requirements v3.19 v1.19 v1.20 v1.21 https://docs.tigera.io/archive/v3.19/getting-started/kubernetes/requirements v3.20 v1.19 v1.20 v1.21 https://docs.tigera.io/archive/v3.20/getting-started/kubernetes/requirements v3.21 v1.20 v1.21 v1.22 https://docs.tigera.io/archive/v3.21/getting-started/kubernetes/requirements v3.22 v1.21 v1.22 v1.23 https://docs.tigera.io/archive/v3.22/getting-started/kubernetes/requirements v3.23 v1.21 v1.22 v1.23 https://docs.tigera.io/archive/v3.23/getting-started/kubernetes/requirements v3.24 v1.22 v1.23 v1.24 v1.25 https://docs.tigera.io/calico/3.24/getting-started/kubernetes/requirements v3.25 v1.23 v1.24 v1.25 v1.26 https://docs.tigera.io/calico/3.25/getting-started/kubernetes/requirements v3.26 v1.24 v1.25 v1.26 v1.27 https://docs.tigera.io/calico/latest/getting-started/kubernetes/requirements","title":"\u4e00\u3001calico\u4e0ekubernetes\u517c\u5bb9\u6027\u5bf9\u7167\u8868"},{"location":"%E6%8A%80%E6%9C%AF/kubernetes%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E7%9A%84%E7%89%88%E6%9C%AC%E5%85%BC%E5%AE%B9%E6%80%A7%E5%88%86%E6%9E%90/#dockerkubernetes","text":"k8s\u7248\u672c docker\u7248\u672c \u67e5\u770b\u5730\u5740 v1.15 1.13.1 17.03 17.06 17.09 18.06 18.09 https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.15.md#server-binaries v1.16 1.13.1 17.03 17.06 17.09 18.06 18.09 https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.16.md v1.17 19.03 https://www.cnblogs.com/jinzhenshui/p/16724379.html v1.18 19.03\u6216\u66f4\u9ad8 https://zhuanlan.zhihu.com/p/121100475 v1.19 19.03\u6216\u66f4\u9ad8 v1.20 19.03\u6216\u66f4\u9ad8 https://blog.csdn.net/alwaysbefine/article/details/119658708 v1.21 20.10\u6216\u66f4\u9ad8 https://www.cnblogs.com/superlinux/p/14676959.html v1.22 20.10\u6216\u66f4\u9ad8 https://blog.csdn.net/qq_30614345/article/details/131334199?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EYuanLiJiHua%7EPosition-2-131334199-blog-119430082.235%5Ev38%5Epc_relevant_default_base&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EYuanLiJiHua%7EPosition-2-131334199-blog-119430082.235%5Ev38%5Epc_relevant_default_base&utm_relevant_index=3 v1.23 20.10\u6216\u66f4\u9ad8 https://blog.csdn.net/yy8623977/article/details/124685772 v1.24 20.10\u6216\u66f4\u9ad8 *k8s v1.24\u4ee5\u540e\u9700\u8981\u989d\u5916\u5b89\u88c5cri-dockerd, k8s\u624d\u80fd\u591f\u6b63\u5e38\u8bc6\u522b\u5230Docker\u3002* v1.25 20.10\u6216\u66f4\u9ad8 https://blog.51cto.com/dayu/5825840 v1.26 20.10\u6216\u66f4\u9ad8 v1.27 20.10\u6216\u66f4\u9ad8 https://blog.csdn.net/zo2k123/article/details/130328617","title":"\u4e8c\u3001docker\u4e0ekubernetes\u517c\u5bb9\u6027\u5bf9\u7167\u8868"},{"location":"%E6%8A%80%E6%9C%AF/kubernetes%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E7%9A%84%E7%89%88%E6%9C%AC%E5%85%BC%E5%AE%B9%E6%80%A7%E5%88%86%E6%9E%90/#etcdkubernetes","text":"etcd\u7248\u672c kubernetes\u7248\u672c \u67e5\u770b\u5730\u5740 3.2.x v1.11 v1.12 https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.22.md 3.3.x v1.13 v1.14 v1.15 v1.16 3.4.x v1.17 v1.18 v1.19 v1.20 v1.21 3.5.x v1.22 v1.23 v1.24 v1.25 v1.26 v1.27","title":"\u4e09\u3001etcd\u4e0ekubernetes\u517c\u5bb9\u6027\u5bf9\u7167\u8868"},{"location":"%E6%8A%80%E6%9C%AF/kubernetes%E5%90%84%E4%B8%AA%E7%BB%84%E4%BB%B6%E7%9A%84%E7%89%88%E6%9C%AC%E5%85%BC%E5%AE%B9%E6%80%A7%E5%88%86%E6%9E%90/#corednskubernetes","text":"coredns\u7248\u672c kubernetes\u7248\u672c \u67e5\u770b\u5730\u5740 1.0.1 v1.09 https://github.com/coredns/deployment/blob/master/kubernetes/CoreDNS-k8s_version.md 1.0.6 v1.10 1.1.3 v1.11 1.2.2 v1.12 1.2.6 v1.13 1.3.1 v1.14 v1.15 1.6.2 v1.16 1.6.5 v1.17 1.6.7 v1.18 1.7.0 v1.19 v1.20 1.8.0 v1.21 1.8.4 v1.22 1.8.6 v1.23 1.9.3 v1.25 v1.26 1.10.1 v1.27","title":"\u56db\u3001coredns\u4e0ekubernetes\u517c\u5bb9\u6027\u5bf9\u7167\u8868"},{"location":"%E6%8A%80%E6%9C%AF/%E4%BD%BF%E7%94%A8%20Kind%20%E5%9C%A8%205%20%E5%88%86%E9%92%9F%E5%86%85%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%20Kubernetes%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/","text":"\u4ec0\u4e48\u662f Kind Kind \uff08Kubernetes in Docker\uff09 \u662f\u4e00\u4e2a Kubernetes \u5b75\u5316\u9879\u76ee\uff0c Kind \u662f\u4e00\u5957\u5f00\u7bb1\u5373\u7528\u7684 Kubernetes \u73af\u5883\u642d\u5efa\u65b9\u6848\u3002\u987e\u540d\u601d\u4e49\uff0c\u5c31\u662f\u5c06 Kubernetes \u6240\u9700\u8981\u7684\u6240\u6709\u7ec4\u4ef6\uff0c\u5168\u90e8\u90e8\u7f72\u5728\u4e00\u4e2a Docker \u5bb9\u5668 \u4e2d\uff0c\u53ef\u4ee5\u5f88\u65b9\u4fbf\u7684\u642d\u5efa Kubernetes \u96c6\u7fa4\u3002 Kind \u5df2\u7ecf\u5e7f\u6cdb\u7684\u5e94\u7528\u4e8e Kubernetes \u4e0a\u6e38\u53ca\u76f8\u5173\u9879\u76ee\u7684 CI \u73af\u5883\u4e2d\uff0c\u5b98\u65b9\u6587\u6863\u4e2d\u4e5f\u628a Kind \u4f5c\u4e3a\u4e00\u79cd\u672c\u5730\u96c6\u7fa4\u642d\u5efa\u7684\u5de5\u5177\u63a8\u8350\u7ed9\u5927\u5bb6\u3002 \u9879\u76ee\u5730\u5740\uff1ahttps://github.com/kubernetes-sigs/kind Kind \u53ef\u4ee5\u505a\u4ec0\u4e48\uff1f \u5feb\u901f\u521b\u5efa\u4e00\u4e2a\u6216\u591a\u4e2a Kubernetes \u96c6\u7fa4 \u652f\u6301\u90e8\u7f72\u9ad8\u53ef\u7528\u7684 Kubernetes \u96c6\u7fa4 \u652f\u6301\u4ece\u6e90\u7801\u6784\u5efa\u5e76\u90e8\u7f72\u4e00\u4e2a Kubernetes \u96c6\u7fa4 \u53ef\u4ee5\u5feb\u901f\u4f4e\u6210\u672c\u4f53\u9a8c\u4e00\u4e2a\u6700\u65b0\u7684 Kubernetes \u96c6\u7fa4\uff0c\u5e76\u652f\u6301 Kubernetes \u7684\u7edd\u5927\u90e8\u5206\u529f\u80fd \u652f\u6301\u672c\u5730\u79bb\u7ebf\u8fd0\u884c\u4e00\u4e2a\u591a\u8282\u70b9\u96c6\u7fa4 Kind \u6709\u54ea\u4e9b\u4f18\u52bf\uff1f \u6700\u5c0f\u7684\u5b89\u88c5\u4f9d\u8d56\uff0c\u4ec5\u9700\u8981\u5b89\u88c5 Docker \u5373\u53ef \u4f7f\u7528\u65b9\u6cd5\u7b80\u5355\uff0c\u53ea\u9700 Kind Cli \u5de5\u5177\u5373\u53ef\u5feb\u901f\u521b\u5efa\u96c6\u7fa4 \u4f7f\u7528\u5bb9\u5668\u6765\u6a21\u4f3c Kubernetes \u8282\u70b9 \u5185\u90e8\u4f7f\u7528 Kubeadm \u7684\u5b98\u65b9\u4e3b\u6d41\u90e8\u7f72\u5de5\u5177 \u901a\u8fc7\u4e86 CNCF \u5b98\u65b9\u7684 K8S Conformance \u6d4b\u8bd5 Kind \u662f\u5982\u4f55\u5de5\u4f5c\u7684\uff1f Kind \u4f7f\u7528\u5bb9\u5668\u6765\u6a21\u62df\u6bcf\u4e00\u4e2a Kubernetes \u8282\u70b9\uff0c\u5e76\u5728\u5bb9\u5668\u91cc\u9762\u8fd0\u884c Systemd \u3002 \u5bb9\u5668\u91cc\u7684 Systemd \u6258\u7ba1\u4e86 Kubelet \u548c Containerd \uff0c\u7136\u540e\u5bb9\u5668\u5185\u90e8\u7684 Kubelet \u628a\u5176\u5b83 Kubernetes \u7ec4\u4ef6\uff1a Kube-Apiserver \u3001 Etcd \u3001 CNI \u7b49\u7b49\u7ec4\u4ef6\u8fd0\u884c\u8d77\u6765\u3002 Kind \u5185\u90e8\u4f7f\u7528\u4e86 Kubeadm \u8fd9\u4e2a\u5de5\u5177\u6765\u505a\u96c6\u7fa4\u7684\u90e8\u7f72\uff0c\u5305\u62ec\u9ad8\u53ef\u7528\u96c6\u7fa4\u4e5f\u662f\u501f\u52a9 Kubeadm \u63d0\u4f9b\u7684\u7279\u6027\u6765\u5b8c\u6210\u7684\u3002\u5728\u9ad8\u7528\u96c6\u7fa4\u4e0b\u8fd8\u4f1a\u989d\u5916\u90e8\u7f72\u4e86\u4e00\u4e2a Nginx \u6765\u63d0\u4f9b \u8d1f\u8f7d\u5747\u8861 VIP \u3002 \u5b89\u88c5 Kind \u5b89\u88c5 Kind \u4f9d\u8d56\u7ec4\u4ef6 Kind \u628a\u90e8\u7f72 Kubernetes \u73af\u5883\u7684\u4f9d\u8d56\u964d\u4f4e\u5230\u4e86\u6700\u5c0f\uff0c\u4ec5\u9700\u8981\u673a\u5668\u5b89\u88c5 Docker \u5373\u53ef\u3002 \u5b89\u88c5 Docker \u8fd9\u91cc\u4ee5 Linux \u7cfb\u7edf\u4e3a\u4f8b\uff1a $ curl -sSL https://get.daocloud.io/docker | sh \u66f4\u591a\u5e73\u53f0\u7684\u5b89\u88c5\u65b9\u6cd5\u53ef\u53c2\u8003\u5b98\u65b9\u6587\u6863\uff1ahttps://docs.docker.com/install/ \u5b89\u88c5 Kubectl \u5982\u679c\u4f60\u9700\u8981\u901a\u8fc7\u547d\u4ee4\u884c\u7ba1\u7406\u96c6\u7fa4\uff0c\u5219\u9700\u8981\u5b89\u88c5 Kubectl \u3002 \u8fd9\u91cc\u4ee5 Linux \u7cfb\u7edf\u4e3a\u4f8b\uff1a $ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.15.0/bin/linux/amd64/kubectl $ chmod +x ./kubectl $ sudo mv ./kubectl /usr/local/bin/kubectl \u66f4\u591a\u5e73\u53f0\u7684\u5b89\u88c5\u65b9\u6cd5\u53ef\u53c2\u8003\u5b98\u65b9\u6587\u6863\uff1ahttps://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl \u5b89\u88c5 Kind Kind \u4f7f\u7528 Golang \u8fdb\u884c\u5f00\u53d1\uff0c\u539f\u751f\u652f\u6301\u826f\u597d\u7684\u8de8\u5e73\u53f0\u7279\u6027\uff0c\u901a\u5e38\u53ea\u9700\u8981\u76f4\u63a5\u4e0b\u8f7d\u6784\u5efa\u597d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u5c31\u53ef\u4f7f\u7528\u3002 \u901a\u8fc7\u4e8c\u8fdb\u5236\u5b89\u88c5 Linux $ curl -Lo ./kind https://github.com/kubernetes-sigs/kind/releases/download/v0.5.1/kind-linux-amd64 $ chmod +x ./kind $ mv ./kind /usr/local/bin/kind Windows $ curl.exe -Lo kind-windows-amd64.exe https://github.com/kubernetes-sigs/kind/releases/download/v0.5.1/kind-windows-amd64 $ mv .\\kind-windows-amd64.exe c:\\kind.exe \u66f4\u591a\u5e73\u53f0\u7684\u5b89\u88c5\u65b9\u6cd5\u53ef\u53c2\u8003\u5b98\u65b9\u6587\u6863\uff1ahttps://kind.sigs.k8s.io/docs/user/quick-start/ \u901a\u8fc7\u6e90\u7801\u5b89\u88c5 \u5982\u679c\u672c\u5730\u73af\u5883\u5df2\u7ecf\u914d\u7f6e\u597d Golang (1.11+) \u7684\u5f00\u53d1\u73af\u5883\uff0c\u4f60\u4e5f\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7\u6e90\u7801\u8fdb\u884c\u5b89\u88c5\u3002 $ go get sigs.k8s.io/kind@v0.5.1 \u8fd0\u884c\u5b8c\u4e0a\u8ff0\u547d\u4ee4\u540e\uff0c\u4f1a\u5c06 Kind \u7684\u53ef\u6267\u884c\u6587\u4ef6\u653e\u5230 $GOPATH/bin \u76ee\u5f55\u5185\u3002\u4e3a\u4e86\u65b9\u4fbf\u4f7f\u7528\uff0c\u4f60\u9700\u8981\u5c06\u6b64\u76ee\u5f55\u52a0\u5165\u5230 $PATH \u4e2d\u3002 \u4f7f\u7528 Kind \u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u6765\u770b\u770b Kind \u652f\u6301\u54ea\u4e9b\u547d\u4ee4\u884c\u64cd\u4f5c\u3002 $ kind kind creates and manages local Kubernetes clusters using Docker container 'nodes' Usage: kind [command] Available Commands: build Build one of [base-image, node-image] create Creates one of [cluster] delete Deletes one of [cluster] export exports one of [logs] get Gets one of [clusters, nodes, kubeconfig-path] help Help about any command load Loads images into nodes version prints the kind CLI version Flags: -h, --help help for kind --loglevel string logrus log level [panic, fatal, error, warning, info, debug] (default \"warning\") --version version for kind Use \"kind [command] --help\" for more information about a command. \u7b80\u5355\u8bf4\u4e0b\u51e0\u4e2a\u6bd4\u8f83\u5e38\u7528\u9009\u9879\u7684\u542b\u4e49\uff1a build \uff1a\u7528\u6765\u4ece Kubernetes \u6e90\u4ee3\u7801\u6784\u5efa\u4e00\u4e2a\u65b0\u7684\u955c\u50cf\u3002 create \uff1a\u521b\u5efa\u4e00\u4e2a Kubernetes \u96c6\u7fa4\u3002 delete \uff1a\u5220\u9664\u4e00\u4e2a Kubernetes \u96c6\u7fa4\u3002 get \uff1a\u53ef\u7528\u6765\u67e5\u770b\u5f53\u524d\u96c6\u7fa4\u3001\u8282\u70b9\u4fe1\u606f\u4ee5\u53ca Kubectl \u914d\u7f6e\u6587\u4ef6\u7684\u5730\u5740\u3002 load \uff1a\u4ece \u5bbf\u4e3b\u673a \u5411 Kubernetes \u8282\u70b9\u5185\u5bfc\u5165\u955c\u50cf\u3002 \u4f7f\u7528 Kind \u521b\u5efa Kubernetes \u96c6\u7fa4 \u642d\u5efa\u4e00\u4e2a\u5355\u8282\u70b9\u96c6\u7fa4 \u642d\u5efa\u5355\u8282\u70b9\u96c6\u7fa4\u662f Kind \u6700\u57fa\u7840\u7684\u529f\u80fd\uff0c\u5f53\u7136\u4f7f\u7528\u8d77\u6765\u4e5f\u5f88\u7b80\u5355\uff0c\u4ec5\u9700\u4e00\u6761\u6307\u4ee4\u5373\u53ef\u5b8c\u6210\u3002 $ kind create cluster --name my-cluster Creating cluster \"my-cluster\" ... \u2713 Ensuring node image (kindest/node:v1.15.3) ? \u2713 Preparing nodes ? \u2713 Creating kubeadm config ? \u2713 Starting control-plane ?\ufe0f Cluster creation complete. You can now use the cluster with: export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"my-cluster\")\" kubectl cluster-info \u4ee5\u4e0a\u547d\u4ee4\u4e2d --name \u662f\u53ef\u9009\u53c2\u6570\u3002\u5982\u679c\u4e0d\u6307\u5b9a\uff0c\u9ed8\u8ba4\u521b\u5efa\u51fa\u6765\u7684\u96c6\u7fa4\u540d\u5b57\u4e3a kind \u3002 \u4f7f\u7528\u9ed8\u8ba4\u5b89\u88c5\u7684\u65b9\u5f0f\u65f6\uff0c\u6211\u4eec\u6ca1\u6709\u6307\u5b9a\u4efb\u4f55\u914d\u7f6e\u6587\u4ef6\u3002\u4ece\u5b89\u88c5\u8fc7\u7a0b\u7684\u8f93\u51fa\u6765\u770b\uff0c\u4e00\u5171\u5206\u4e3a 4 \u6b65\uff1a \u68c0\u67e5\u672c\u5730\u73af\u5883\u662f\u5426\u5b58\u5728\u4e00\u4e2a\u57fa\u7840\u7684\u5b89\u88c5\u955c\u50cf\uff0c\u9ed8\u8ba4\u662f kindest/node:v1.15.3 \uff0c\u8be5\u955c\u50cf\u91cc\u9762\u5305\u542b\u4e86\u6240\u6709\u9700\u8981\u5b89\u88c5\u7684\u4e1c\u897f\uff0c\u5305\u62ec\uff1a kubectl \u3001 kubeadm \u3001 kubelet \u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u4ee5\u53ca\u5b89\u88c5\u5bf9\u5e94\u7248\u672c Kubernetes \u6240\u9700\u8981\u7684\u955c\u50cf\u3002 \u51c6\u5907 Kubernetes \u8282\u70b9\uff0c\u4e3b\u8981\u5c31\u662f\u542f\u52a8\u5bb9\u5668\u3001\u89e3\u538b\u955c\u50cf\u8fd9\u7c7b\u7684\u64cd\u4f5c\u3002 \u5efa\u7acb\u5bf9\u5e94\u7684 kubeadm \u7684\u914d\u7f6e\uff0c\u5b8c\u6210\u4e4b\u540e\u5c31\u901a\u8fc7 kubeadm \u8fdb\u884c\u5b89\u88c5\u3002\u5b89\u88c5\u5b8c\u6210\u540e\u8fd8\u4f1a\u505a\u4e00\u4e9b\u6e05\u7406\u64cd\u4f5c\uff0c\u6bd4\u5982\uff1a\u5220\u6389\u4e3b\u8282\u70b9\u4e0a\u7684\u6c61\u70b9\uff0c\u5426\u5219\u5bf9\u4e8e\u6ca1\u6709\u5bb9\u5fcd\u7684 Pod \u65e0\u6cd5\u5b8c\u6210\u90e8\u7f72\u3002 \u4e0a\u9762\u6240\u6709\u64cd\u4f5c\u90fd\u5b8c\u6210\u540e\uff0c\u5c31\u6210\u529f\u542f\u52a8\u4e86\u4e00\u4e2a Kubernetes \u96c6\u7fa4\u5e76\u8f93\u51fa\u4e00\u4e9b\u64cd\u4f5c\u96c6\u7fa4\u7684\u63d0\u793a\u4fe1\u606f\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c Kind \u4f1a\u5148\u4e0b\u8f7d kindest/node:v1.15.3 \u955c\u50cf\u3002\u5982\u679c\u4f60\u60f3\u6307\u5b9a\u4e0d\u540c\u7248\u672c\uff0c\u53ef\u4ee5\u4f7f\u7528 --image \u53c2\u6570\uff0c\u7c7b\u4f3c\u8fd9\u6837\uff1a kind create cluster --image kindest/node:v1.15.3 kindest/node \u8fd9\u4e2a\u955c\u50cf\u76ee\u524d\u6258\u7ba1\u4e8e Docker Hub \u4e0a\uff0c\u4e0b\u8f7d\u65f6\u53ef\u80fd\u4f1a\u8f83\u6162\u3002\u540c\u6837\u7684\u95ee\u9898 Kind \u8fdb\u884c\u96c6\u7fa4\u7684\u521b\u5efa\u4e5f\u662f\u5b58\u5728\u7684\uff0c Kind \u5b9e\u9645\u4f7f\u7528 Kubeadm \u8fdb\u884c\u96c6\u7fa4\u7684\u521b\u5efa\u3002\u5bf9 Kubeadm \u6709\u6240\u4e86\u89e3\u7684\u540c\u5b66\u90fd\u77e5\u9053\u5b83\u9ed8\u8ba4\u4f7f\u7528\u7684\u955c\u50cf\u5728\u56fd\u5185\u662f\u4e0d\u80fd\u8bbf\u95ee\u7684\uff0c\u6240\u4ee5\u4e00\u6837\u9700\u8981\u81ea\u884c\u89e3\u51b3\u7f51\u7edc\u95ee\u9898\u3002 \u5982\u679c\u4f60\u5b58\u5728\u4e0a\u9762\u8bf4\u7684\u7f51\u7edc\u95ee\u9898\uff0c\u6700\u597d\u914d\u7f6e\u4e00\u4e2a\u56fd\u5185\u7684\u52a0\u901f\u5668\u6216\u8005\u955c\u50cf\u6e90\u3002\u5982\u679c\u4f60\u8fd8\u4e0d\u77e5\u9053\u5982\u4f55\u914d\u7f6e\u52a0\u901f\u5668\u548c\u955c\u50cf\u6e90\u53ef\u4ee5\u53c2\u8003\uff1a\u300c Docker / Kubernetes \u955c\u50cf\u6e90\u4e0d\u53ef\u7528\uff0c\u6559\u4f60\u51e0\u62db\u641e\u5b9a\u5b83\uff01 \u300d\u548c \u300c Docker \u4e0b\u4f7f\u7528 DaoCloud / \u963f\u91cc\u4e91\u955c\u50cf\u52a0\u901f \u300d\u4e24\u7bc7\u6587\u7ae0\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u6839\u636e\u4e0a\u9762\u547d\u4ee4\u6267\u884c\u5b8c\u540e\uff0c\u8f93\u51fa\u7684\u63d0\u793a\u4fe1\u606f\u8fdb\u884c\u64cd\u4f5c\u6765\u9a8c\u8bc1\u4e00\u4e0b\u96c6\u7fa4\u662f\u5426\u90e8\u7f72\u6210\u529f\u3002 # \u83b7\u53d6\u6307\u5b9a\u96c6\u7fa4\u7684\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u7684\u8def\u5f84 $ export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"my-cluster\")\" $ kubectl cluster-info Kubernetes master is running at https://localhost:34458 KubeDNS is running at https://localhost:34458/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. $ kubectl get nodes NAME STATUS ROLES AGE VERSION my-cluster-control-plane Ready master 2m v1.15.3 $ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-6g66f 1/1 Running 0 21m coredns-86c58d9df4-pqcc4 1/1 Running 0 21m etcd-my-cluster-control-plane 1/1 Running 0 20m kube-apiserver-my-cluster-control-plane 1/1 Running 0 20m kube-controller-manager-my-cluster-control-plane 1/1 Running 0 20m kube-proxy-cjgnt 1/1 Running 0 21m kube-scheduler-my-cluster-control-plane 1/1 Running 0 21m weave-net-ls2v8 2/2 Running 1 21m \u4ece\u4e0a\u9762\u7684\u8f93\u51fa\u7ed3\u679c\uff0c\u53ef\u4ee5\u770b\u5230\u5355\u8282\u70b9\u7684 Kubernetes \u5df2\u7ecf\u642d\u5efa\u6210\u529f\u3002\u5355\u8282\u70b9\u96c6\u7fa4\u9ed8\u8ba4\u65b9\u5f0f\u542f\u52a8\u7684\u8282\u70b9\u7c7b\u578b\u662f control-plane \uff0c\u8be5\u8282\u70b9\u5305\u542b\u4e86\u6240\u6709\u7684\u7ec4\u4ef6\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u5206\u522b\u662f\uff1a 2*Coredns \u3001 Etcd \u3001 Api-Server \u3001 Controller-Manager \u3001 Kube-Proxy \u3001 Sheduler \u548c\u7f51\u7edc\u63d2\u4ef6 Weave \uff0c\u76ee\u524d\u9ed8\u8ba4\u4f7f\u7528\u7684\u7f51\u7edc\u63d2\u4ef6\u4e5f\u662f Weave \u3002 \u521b\u5efa\u591a\u8282\u70b9\u7684\u96c6\u7fa4 \u9ed8\u8ba4\u5b89\u88c5\u7684\u96c6\u7fa4\u53ea\u90e8\u7f72\u4e86\u4e00\u4e2a\u63a7\u5236\u8282\u70b9\uff0c\u5982\u679c\u9700\u8981\u90e8\u7f72\u591a\u8282\u70b9\u96c6\u7fa4\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u7684\u65b9\u5f0f\u6765\u521b\u5efa\u591a\u4e2a\u5bb9\u5668\u3002\u8fd9\u6837\u5c31\u53ef\u4ee5\u8fbe\u5230\u6a21\u62df\u591a\u4e2a\u8282\u70b9\u76ee\u7684\uff0c\u5e76\u4ee5\u8fd9\u4e9b\u8282\u70b9\u6765\u6784\u5efa\u4e00\u4e2a\u591a\u8282\u70b9\u7684 Kubernetes \u96c6\u7fa4\u3002 \u521b\u5efa\u591a\u8282\u70b9 Kubernetes \u96c6\u7fa4\u914d\u7f6e\u6587\u4ef6 Kind \u5728\u521b\u5efa\u96c6\u7fa4\u7684\u65f6\u5019\uff0c\u652f\u6301\u901a\u8fc7 --config \u53c2\u6570\u4f20\u9012\u914d\u7f6e\u6587\u4ef6\u7ed9 Kind \uff0c\u914d\u7f6e\u6587\u4ef6\u53ef\u4fee\u6539\u7684\u5185\u5bb9\u4e3b\u8981\u6709 role \u548c \u8282\u70b9\u4f7f\u7528\u7684\u955c\u50cf\u3002 $ vim my-cluster-multi-node.yaml # \u4e00\u5171\u4e24\u4e2a\u8282\u70b9\uff0c\u4e00\u4e2a\u4e3b\u8282\u70b9\uff0c\u4e00\u4e2a\u4ece\u8282\u70b9\u3002 kind: Cluster apiVersion: kind.sigs.k8s.io/v1alpha3 nodes: - role: control-plane - role: worker \u521b\u5efa\u591a\u8282\u70b9 Kubernetes \u96c6\u7fa4 \u914d\u7f6e\u6587\u4ef6\u521b\u5efa\u5b8c\u6210\u540e\uff0c\u5c31\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u6765\u5b8c\u6210\u591a\u8282\u70b9 Kubernetes \u96c6\u7fa4\u642d\u5efa\u3002 $ kind create cluster --config my-cluster-multi-node.yaml --name my-cluster-multi-node Creating cluster \"my-cluster-multi-node\" ... \u2713 Ensuring node image (kindest/node:v1.15.3) ? \u2713 Preparing nodes ?? \u2713 Creating kubeadm config ? \u2713 Starting control-plane ?\ufe0f \u2713 Joining worker nodes ? Cluster creation complete. You can now use the cluster with: export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"my-cluster-multi-node\")\" kubectl cluster-info \u548c\u4e0a\u9762\u521b\u5efa\u7684\u5355\u8282\u70b9\u96c6\u7fa4\u4e00\u6837\uff0c\u6211\u4eec\u540c\u6837\u6839\u636e\u4e0a\u9762\u547d\u4ee4\u6267\u884c\u5b8c\u540e\uff0c\u8f93\u51fa\u7684\u63d0\u793a\u4fe1\u606f\u8fdb\u884c\u64cd\u4f5c\u6765\u9a8c\u8bc1\u4e00\u4e0b\u96c6\u7fa4\u662f\u5426\u90e8\u7f72\u6210\u529f\u3002 $ kubectl get nodes NAME STATUS ROLES AGE VERSION my-cluster-multi-node-control-plane Ready master 3m20s v1.15.3 my-cluster-multi-node-worker Ready <none> 3m8s v1.15.3 $ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-cnqhc 1/1 Running 0 5m29s coredns-86c58d9df4-hn9mv 1/1 Running 0 5m29s etcd-my-cluster-multi-node-control-plane 1/1 Running 0 4m24s kube-apiserver-my-cluster-multi-node-control-plane 1/1 Running 0 4m17s kube-controller-manager-my-cluster-multi-node-control-plane 1/1 Running 0 4m21s kube-proxy-8t4xt 1/1 Running 0 5m27s kube-proxy-skd5v 1/1 Running 0 5m29s kube-scheduler-my-cluster-multi-node-control-plane 1/1 Running 0 4m18s weave-net-nmfq2 2/2 Running 1 5m27s weave-net-srdfw 2/2 Running 0 5m29s \u521b\u5efa\u9ad8\u53ef\u7528 Kubernetes \u96c6\u7fa4 Kind \u4e5f\u652f\u6301\u642d\u5efa\u9ad8\u53ef\u7528\u7684 Kubernetes \u96c6\u7fa4\uff0c\u521b\u5efa\u65b9\u5f0f\u548c\u591a\u8282\u70b9\u96c6\u7fa4\u7c7b\u4f3c\uff0c\u4e5f\u662f\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u6765\u5b9e\u73b0\u3002 \u521b\u5efa\u9ad8\u53ef\u7528 Kubernetes \u96c6\u7fa4\u914d\u7f6e\u6587\u4ef6 $ vim my-cluster-ha.yaml # \u4e00\u5171\u516d\u4e2a\u8282\u70b9\uff0c\u4e09\u4e2a control-plane \u8282\u70b9\uff0c\u4e09\u4e2a workers \u8282\u70b9 kind: Cluster apiVersion: kind.sigs.k8s.io/v1alpha3 kubeadmConfigPatches: - | apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration metadata: name: config networking: serviceSubnet: 10.0.0.0/16 imageRepository: registry.aliyuncs.com/google_containers nodeRegistration: kubeletExtraArgs: pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1 - | apiVersion: kubeadm.k8s.io/v1beta2 kind: InitConfiguration metadata: name: config networking: serviceSubnet: 10.0.0.0/16 imageRepository: registry.aliyuncs.com/google_containers nodes: - role: control-plane - role: control-plane - role: control-plane - role: worker - role: worker - role: worker \u8fd9\u91cc\uff0c\u6211\u4eec\u901a\u8fc7\u76f4\u63a5\u5728\u914d\u7f6e\u6587\u4ef6\u91cc\u4f7f\u7528\u56fd\u5185\u5bb9\u5668\u955c\u50cf\u6e90\u7684\u65b9\u5f0f\u89e3\u51b3\u4e86\u5b98\u65b9\u5bb9\u5668\u955c\u50cf\u6e90\u4e0d\u53ef\u7528\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4e5f\u8fbe\u5230\u4e86\u52a0\u901f\u96c6\u7fa4\u521b\u5efa\u7684\u76ee\u7684\u3002 \u521b\u5efa\u9ad8\u53ef\u7528 Kubernetes \u96c6\u7fa4 \u914d\u7f6e\u6587\u4ef6\u521b\u5efa\u5b8c\u6210\u540e\uff0c\u5c31\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u6765\u5b8c\u6210\u9ad8\u53ef\u7528 Kubernetes \u96c6\u7fa4\u642d\u5efa\u3002 $ kind create cluster --name my-cluster-ha --config my-cluster-ha.yaml Creating cluster \"my-cluster-ha\" ... \u2713 Ensuring node image (kindest/node:v1.15.3) ? \u2713 Preparing nodes ??????? \u2713 Starting the external load balancer \u2696\ufe0f \u2713 Creating kubeadm config ? \u2713 Starting control-plane ?\ufe0f \u2713 Joining more control-plane nodes ? \u2713 Joining worker nodes ? Cluster creation complete. You can now use the cluster with: export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"my-cluster-ha\")\" kubectl cluster-info master $ export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"my-cluster-ha\")\" master $ kubectl cluster-info Kubernetes master is running at https://localhost:44019 KubeDNS is running at https://localhost:44019/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. \u540c\u6837\uff0c\u6211\u4eec\u6839\u636e\u4e0a\u9762\u547d\u4ee4\u6267\u884c\u5b8c\u540e\uff0c\u8f93\u51fa\u7684\u63d0\u793a\u4fe1\u606f\u8fdb\u884c\u64cd\u4f5c\u6765\u9a8c\u8bc1\u4e00\u4e0b\u96c6\u7fa4\u662f\u5426\u90e8\u7f72\u6210\u529f\u3002 $ kubectl get nodes NAME STATUS ROLES AGE VERSION my-cluster-ha-control-plane Ready master 3m42s v1.15.3 my-cluster-ha-control-plane2 Ready master 3m24s v1.15.3 my-cluster-ha-control-plane3 Ready master 2m13s v1.15.3 my-cluster-ha-worker Ready <none> 96s v1.15.3 my-cluster-ha-worker2 Ready <none> 98s v1.15.3 my-cluster-ha-worker3 Ready <none> 95s v1.15.3 \u4ece\u4e0a\u9762\u7684\u8f93\u51fa\u7ed3\u679c\uff0c\u53ef\u4ee5\u770b\u5230\u5305\u542b\u4e86\u591a\u4e2a master \u8282\u70b9\uff0c\u8bf4\u660e\u9ad8\u53ef\u7528\u7684 Kubernetes \u96c6\u7fa4\u5df2\u7ecf\u642d\u5efa\u6210\u529f\u3002 \u5176\u5b83\u76f8\u5173\u77e5\u8bc6 Kind \u7684\u955c\u50cf\u91cc\u7684\u79d8\u5bc6 Kind \u955c\u50cf\u4e00\u5171\u5206\u4e3a\u4e24\u7c7b\uff0c\u4e00\u7c7b\u662f Base \u955c\u50cf\uff0c\u53e6\u4e00\u7c7b\u662f Node \u955c\u50cf\u3002 Base \u955c\u50cf Base \u955c\u50cf\u76ee\u524d\u4f7f\u7528\u4e86 ubuntu:19.04 \u4f5c\u4e3a\u57fa\u7840\u955c\u50cf\uff0c\u5e76\u505a\u4e86\u4e0b\u9762\u7684\u8c03\u6574\uff1a \u5b89\u88c5 Systemd \u76f8\u5173\u7684\u5305\uff0c\u5e76\u8c03\u6574\u4e00\u4e9b\u914d\u7f6e\u4ee5\u9002\u5e94\u5728\u5bb9\u5668\u5185\u8fd0\u884c\u3002 \u5b89\u88c5 Kubernetes \u8fd0\u884c\u65f6\u7684\u4f9d\u8d56\u5305\uff0c\u6bd4\u5982: Conntrack \u3001 Socat \u3001 CNI \u7b49\u3002 \u5b89\u88c5\u5bb9\u5668\u8fd0\u884c\u73af\u5883\uff0c\u6bd4\u5982: Containerd \u3001 Crictl \u7b49\u3002 \u914d\u7f6e\u81ea\u5df1\u7684 ENTRYPOINT \u811a\u672c\uff0c\u4ee5\u9002\u5e94\u548c\u8c03\u6574\u5bb9\u5668\u5185\u8fd0\u884c\u7684\u95ee\u9898\u3002 \u66f4\u591a\u5177\u4f53\u7684\u6784\u5efa\u903b\u8f91\u53ef\u4ee5\u53c2\u8003\uff1ahttps://github.com/kubernetes-sigs/kind/blob/master/images/base/Dockerfile Node \u955c\u50cf Node \u955c\u50cf\u7684\u6784\u5efa\u6bd4\u8f83\u590d\u6742\uff0c\u76ee\u524d\u662f\u901a\u8fc7\u8fd0\u884c Base \u955c\u50cf\u5e76\u5728 Base \u955c\u50cf\u5185\u6267\u884c\u64cd\u4f5c\uff0c\u518d\u4fdd\u5b58\u6b64\u5bb9\u5668\u5185\u5bb9\u4e3a\u955c\u50cf\u7684\u65b9\u5f0f\u6765\u6784\u5efa\u7684\uff0c\u5305\u542b\u7684\u64cd\u4f5c\u6709\uff1a \u6784\u5efa Kubernetes \u76f8\u5173\u8d44\u6e90\uff0c\u6bd4\u5982\uff1a\u4e8c\u8fdb\u5236\u6587\u4ef6\u548c\u955c\u50cf\u3002 \u8fd0\u884c\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u7684\u5bb9\u5668 \u628a\u6784\u5efa\u7684 Kubernetes \u76f8\u5173\u8d44\u6e90\u590d\u5236\u5230\u5bb9\u5668\u91cc \u8c03\u6574\u90e8\u5206\u7ec4\u4ef6\u914d\u7f6e\u53c2\u6570\uff0c\u4ee5\u652f\u6301\u5728\u5bb9\u5668\u5185\u8fd0\u884c \u9884\u5148\u62c9\u53bb\u8fd0\u884c\u73af\u5883\u9700\u8981\u7684\u955c\u50cf \u901a\u8fc7 docker commit \u65b9\u5f0f\u4fdd\u5b58\u5f53\u524d\u7684\u6784\u5efa\u5bb9\u5668\u4e3a Node \u955c\u50cf \u5982\u4f55\u5feb\u901f\u5220\u9664\u4e00\u4e2a\u96c6\u7fa4 \u5982\u679c\u4f60\u4e0d\u9700\u8981\u672c\u5730\u7684\u96c6\u7fa4\u73af\u5883\uff0c\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u5220\u9664\uff1a $ kind delete cluster --name my-cluster Deleting cluster \"my-cluster\" ... $KUBECONFIG is still set to use /root/.kube/kind-config-my-cluster even though that file has been deleted, remember to unset it \u81f3\u6b64\uff0c\u6211\u4eec\u5c31\u6f14\u793a\u5b8c\u4e86\u5982\u4f55\u4f7f\u7528 Kind \u5feb\u901f\u642d\u5efa\u4e00\u4e2a Kubernetes \u96c6\u7fa4\u3002\u4e0d\u8fc7\u6709\u4e00\u4e2a\u4f60\u9700\u8981\u6ce8\u610f\u7684\u5730\u65b9\uff0c Kind \u642d\u5efa\u7684\u96c6\u7fa4\u4e0d\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883\u4e2d\u4f7f\u7528\u3002\u4f46\u662f\u5982\u679c\u4f60\u60f3\u5728\u672c\u5730\u5feb\u901f\u6784\u5efa\u4e00\u4e2a Kubernetes \u96c6\u7fa4\u73af\u5883\uff0c\u5e76\u4e14\u4e0d\u60f3\u5360\u7528\u592a\u591a\u7684\u786c\u4ef6\u8d44\u6e90\uff0c\u90a3\u4e48 Kind \u4f1a\u662f\u4f60\u4e0d\u9519\u7684\u9009\u62e9\u3002 \u53c2\u8003\u6587\u6863 https://www.google.com http://t.cn/AiRVBwDS https://zhuanlan.zhihu.com/p/60464867 https://yeya24.github.io/post/kind/ http://dockerone.com/article/8974 https://kind.sigs.k8s.io/docs/user/quick-start/","title":"\u4f7f\u7528 Kind \u5728 5 \u5206\u949f\u5185\u5feb\u901f\u90e8\u7f72\u4e00\u4e2a Kubernetes \u9ad8\u53ef\u7528\u96c6\u7fa4"},{"location":"%E6%8A%80%E6%9C%AF/%E4%BD%BF%E7%94%A8%20Kind%20%E5%9C%A8%205%20%E5%88%86%E9%92%9F%E5%86%85%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%20Kubernetes%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/#kind","text":"","title":"\u5b89\u88c5 Kind"},{"location":"%E6%8A%80%E6%9C%AF/%E4%BD%BF%E7%94%A8%20Kind%20%E5%9C%A8%205%20%E5%88%86%E9%92%9F%E5%86%85%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%20Kubernetes%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/#kind_1","text":"Kind \u628a\u90e8\u7f72 Kubernetes \u73af\u5883\u7684\u4f9d\u8d56\u964d\u4f4e\u5230\u4e86\u6700\u5c0f\uff0c\u4ec5\u9700\u8981\u673a\u5668\u5b89\u88c5 Docker \u5373\u53ef\u3002 \u5b89\u88c5 Docker \u8fd9\u91cc\u4ee5 Linux \u7cfb\u7edf\u4e3a\u4f8b\uff1a $ curl -sSL https://get.daocloud.io/docker | sh \u66f4\u591a\u5e73\u53f0\u7684\u5b89\u88c5\u65b9\u6cd5\u53ef\u53c2\u8003\u5b98\u65b9\u6587\u6863\uff1ahttps://docs.docker.com/install/ \u5b89\u88c5 Kubectl \u5982\u679c\u4f60\u9700\u8981\u901a\u8fc7\u547d\u4ee4\u884c\u7ba1\u7406\u96c6\u7fa4\uff0c\u5219\u9700\u8981\u5b89\u88c5 Kubectl \u3002 \u8fd9\u91cc\u4ee5 Linux \u7cfb\u7edf\u4e3a\u4f8b\uff1a $ curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.15.0/bin/linux/amd64/kubectl $ chmod +x ./kubectl $ sudo mv ./kubectl /usr/local/bin/kubectl \u66f4\u591a\u5e73\u53f0\u7684\u5b89\u88c5\u65b9\u6cd5\u53ef\u53c2\u8003\u5b98\u65b9\u6587\u6863\uff1ahttps://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl","title":"\u5b89\u88c5 Kind \u4f9d\u8d56\u7ec4\u4ef6"},{"location":"%E6%8A%80%E6%9C%AF/%E4%BD%BF%E7%94%A8%20Kind%20%E5%9C%A8%205%20%E5%88%86%E9%92%9F%E5%86%85%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%20Kubernetes%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/#kind_2","text":"Kind \u4f7f\u7528 Golang \u8fdb\u884c\u5f00\u53d1\uff0c\u539f\u751f\u652f\u6301\u826f\u597d\u7684\u8de8\u5e73\u53f0\u7279\u6027\uff0c\u901a\u5e38\u53ea\u9700\u8981\u76f4\u63a5\u4e0b\u8f7d\u6784\u5efa\u597d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u5c31\u53ef\u4f7f\u7528\u3002 \u901a\u8fc7\u4e8c\u8fdb\u5236\u5b89\u88c5 Linux $ curl -Lo ./kind https://github.com/kubernetes-sigs/kind/releases/download/v0.5.1/kind-linux-amd64 $ chmod +x ./kind $ mv ./kind /usr/local/bin/kind Windows $ curl.exe -Lo kind-windows-amd64.exe https://github.com/kubernetes-sigs/kind/releases/download/v0.5.1/kind-windows-amd64 $ mv .\\kind-windows-amd64.exe c:\\kind.exe \u66f4\u591a\u5e73\u53f0\u7684\u5b89\u88c5\u65b9\u6cd5\u53ef\u53c2\u8003\u5b98\u65b9\u6587\u6863\uff1ahttps://kind.sigs.k8s.io/docs/user/quick-start/ \u901a\u8fc7\u6e90\u7801\u5b89\u88c5 \u5982\u679c\u672c\u5730\u73af\u5883\u5df2\u7ecf\u914d\u7f6e\u597d Golang (1.11+) \u7684\u5f00\u53d1\u73af\u5883\uff0c\u4f60\u4e5f\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7\u6e90\u7801\u8fdb\u884c\u5b89\u88c5\u3002 $ go get sigs.k8s.io/kind@v0.5.1 \u8fd0\u884c\u5b8c\u4e0a\u8ff0\u547d\u4ee4\u540e\uff0c\u4f1a\u5c06 Kind \u7684\u53ef\u6267\u884c\u6587\u4ef6\u653e\u5230 $GOPATH/bin \u76ee\u5f55\u5185\u3002\u4e3a\u4e86\u65b9\u4fbf\u4f7f\u7528\uff0c\u4f60\u9700\u8981\u5c06\u6b64\u76ee\u5f55\u52a0\u5165\u5230 $PATH \u4e2d\u3002","title":"\u5b89\u88c5 Kind"},{"location":"%E6%8A%80%E6%9C%AF/%E4%BD%BF%E7%94%A8%20Kind%20%E5%9C%A8%205%20%E5%88%86%E9%92%9F%E5%86%85%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%20Kubernetes%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/#kind_3","text":"\u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u6765\u770b\u770b Kind \u652f\u6301\u54ea\u4e9b\u547d\u4ee4\u884c\u64cd\u4f5c\u3002 $ kind kind creates and manages local Kubernetes clusters using Docker container 'nodes' Usage: kind [command] Available Commands: build Build one of [base-image, node-image] create Creates one of [cluster] delete Deletes one of [cluster] export exports one of [logs] get Gets one of [clusters, nodes, kubeconfig-path] help Help about any command load Loads images into nodes version prints the kind CLI version Flags: -h, --help help for kind --loglevel string logrus log level [panic, fatal, error, warning, info, debug] (default \"warning\") --version version for kind Use \"kind [command] --help\" for more information about a command. \u7b80\u5355\u8bf4\u4e0b\u51e0\u4e2a\u6bd4\u8f83\u5e38\u7528\u9009\u9879\u7684\u542b\u4e49\uff1a build \uff1a\u7528\u6765\u4ece Kubernetes \u6e90\u4ee3\u7801\u6784\u5efa\u4e00\u4e2a\u65b0\u7684\u955c\u50cf\u3002 create \uff1a\u521b\u5efa\u4e00\u4e2a Kubernetes \u96c6\u7fa4\u3002 delete \uff1a\u5220\u9664\u4e00\u4e2a Kubernetes \u96c6\u7fa4\u3002 get \uff1a\u53ef\u7528\u6765\u67e5\u770b\u5f53\u524d\u96c6\u7fa4\u3001\u8282\u70b9\u4fe1\u606f\u4ee5\u53ca Kubectl \u914d\u7f6e\u6587\u4ef6\u7684\u5730\u5740\u3002 load \uff1a\u4ece \u5bbf\u4e3b\u673a \u5411 Kubernetes \u8282\u70b9\u5185\u5bfc\u5165\u955c\u50cf\u3002","title":"\u4f7f\u7528 Kind"},{"location":"%E6%8A%80%E6%9C%AF/%E4%BD%BF%E7%94%A8%20Kind%20%E5%9C%A8%205%20%E5%88%86%E9%92%9F%E5%86%85%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%20Kubernetes%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/#kind-kubernetes","text":"","title":"\u4f7f\u7528 Kind \u521b\u5efa Kubernetes \u96c6\u7fa4"},{"location":"%E6%8A%80%E6%9C%AF/%E4%BD%BF%E7%94%A8%20Kind%20%E5%9C%A8%205%20%E5%88%86%E9%92%9F%E5%86%85%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%20Kubernetes%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/#_1","text":"\u642d\u5efa\u5355\u8282\u70b9\u96c6\u7fa4\u662f Kind \u6700\u57fa\u7840\u7684\u529f\u80fd\uff0c\u5f53\u7136\u4f7f\u7528\u8d77\u6765\u4e5f\u5f88\u7b80\u5355\uff0c\u4ec5\u9700\u4e00\u6761\u6307\u4ee4\u5373\u53ef\u5b8c\u6210\u3002 $ kind create cluster --name my-cluster Creating cluster \"my-cluster\" ... \u2713 Ensuring node image (kindest/node:v1.15.3) ? \u2713 Preparing nodes ? \u2713 Creating kubeadm config ? \u2713 Starting control-plane ?\ufe0f Cluster creation complete. You can now use the cluster with: export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"my-cluster\")\" kubectl cluster-info \u4ee5\u4e0a\u547d\u4ee4\u4e2d --name \u662f\u53ef\u9009\u53c2\u6570\u3002\u5982\u679c\u4e0d\u6307\u5b9a\uff0c\u9ed8\u8ba4\u521b\u5efa\u51fa\u6765\u7684\u96c6\u7fa4\u540d\u5b57\u4e3a kind \u3002 \u4f7f\u7528\u9ed8\u8ba4\u5b89\u88c5\u7684\u65b9\u5f0f\u65f6\uff0c\u6211\u4eec\u6ca1\u6709\u6307\u5b9a\u4efb\u4f55\u914d\u7f6e\u6587\u4ef6\u3002\u4ece\u5b89\u88c5\u8fc7\u7a0b\u7684\u8f93\u51fa\u6765\u770b\uff0c\u4e00\u5171\u5206\u4e3a 4 \u6b65\uff1a \u68c0\u67e5\u672c\u5730\u73af\u5883\u662f\u5426\u5b58\u5728\u4e00\u4e2a\u57fa\u7840\u7684\u5b89\u88c5\u955c\u50cf\uff0c\u9ed8\u8ba4\u662f kindest/node:v1.15.3 \uff0c\u8be5\u955c\u50cf\u91cc\u9762\u5305\u542b\u4e86\u6240\u6709\u9700\u8981\u5b89\u88c5\u7684\u4e1c\u897f\uff0c\u5305\u62ec\uff1a kubectl \u3001 kubeadm \u3001 kubelet \u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\uff0c\u4ee5\u53ca\u5b89\u88c5\u5bf9\u5e94\u7248\u672c Kubernetes \u6240\u9700\u8981\u7684\u955c\u50cf\u3002 \u51c6\u5907 Kubernetes \u8282\u70b9\uff0c\u4e3b\u8981\u5c31\u662f\u542f\u52a8\u5bb9\u5668\u3001\u89e3\u538b\u955c\u50cf\u8fd9\u7c7b\u7684\u64cd\u4f5c\u3002 \u5efa\u7acb\u5bf9\u5e94\u7684 kubeadm \u7684\u914d\u7f6e\uff0c\u5b8c\u6210\u4e4b\u540e\u5c31\u901a\u8fc7 kubeadm \u8fdb\u884c\u5b89\u88c5\u3002\u5b89\u88c5\u5b8c\u6210\u540e\u8fd8\u4f1a\u505a\u4e00\u4e9b\u6e05\u7406\u64cd\u4f5c\uff0c\u6bd4\u5982\uff1a\u5220\u6389\u4e3b\u8282\u70b9\u4e0a\u7684\u6c61\u70b9\uff0c\u5426\u5219\u5bf9\u4e8e\u6ca1\u6709\u5bb9\u5fcd\u7684 Pod \u65e0\u6cd5\u5b8c\u6210\u90e8\u7f72\u3002 \u4e0a\u9762\u6240\u6709\u64cd\u4f5c\u90fd\u5b8c\u6210\u540e\uff0c\u5c31\u6210\u529f\u542f\u52a8\u4e86\u4e00\u4e2a Kubernetes \u96c6\u7fa4\u5e76\u8f93\u51fa\u4e00\u4e9b\u64cd\u4f5c\u96c6\u7fa4\u7684\u63d0\u793a\u4fe1\u606f\u3002 \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c Kind \u4f1a\u5148\u4e0b\u8f7d kindest/node:v1.15.3 \u955c\u50cf\u3002\u5982\u679c\u4f60\u60f3\u6307\u5b9a\u4e0d\u540c\u7248\u672c\uff0c\u53ef\u4ee5\u4f7f\u7528 --image \u53c2\u6570\uff0c\u7c7b\u4f3c\u8fd9\u6837\uff1a kind create cluster --image kindest/node:v1.15.3 kindest/node \u8fd9\u4e2a\u955c\u50cf\u76ee\u524d\u6258\u7ba1\u4e8e Docker Hub \u4e0a\uff0c\u4e0b\u8f7d\u65f6\u53ef\u80fd\u4f1a\u8f83\u6162\u3002\u540c\u6837\u7684\u95ee\u9898 Kind \u8fdb\u884c\u96c6\u7fa4\u7684\u521b\u5efa\u4e5f\u662f\u5b58\u5728\u7684\uff0c Kind \u5b9e\u9645\u4f7f\u7528 Kubeadm \u8fdb\u884c\u96c6\u7fa4\u7684\u521b\u5efa\u3002\u5bf9 Kubeadm \u6709\u6240\u4e86\u89e3\u7684\u540c\u5b66\u90fd\u77e5\u9053\u5b83\u9ed8\u8ba4\u4f7f\u7528\u7684\u955c\u50cf\u5728\u56fd\u5185\u662f\u4e0d\u80fd\u8bbf\u95ee\u7684\uff0c\u6240\u4ee5\u4e00\u6837\u9700\u8981\u81ea\u884c\u89e3\u51b3\u7f51\u7edc\u95ee\u9898\u3002 \u5982\u679c\u4f60\u5b58\u5728\u4e0a\u9762\u8bf4\u7684\u7f51\u7edc\u95ee\u9898\uff0c\u6700\u597d\u914d\u7f6e\u4e00\u4e2a\u56fd\u5185\u7684\u52a0\u901f\u5668\u6216\u8005\u955c\u50cf\u6e90\u3002\u5982\u679c\u4f60\u8fd8\u4e0d\u77e5\u9053\u5982\u4f55\u914d\u7f6e\u52a0\u901f\u5668\u548c\u955c\u50cf\u6e90\u53ef\u4ee5\u53c2\u8003\uff1a\u300c Docker / Kubernetes \u955c\u50cf\u6e90\u4e0d\u53ef\u7528\uff0c\u6559\u4f60\u51e0\u62db\u641e\u5b9a\u5b83\uff01 \u300d\u548c \u300c Docker \u4e0b\u4f7f\u7528 DaoCloud / \u963f\u91cc\u4e91\u955c\u50cf\u52a0\u901f \u300d\u4e24\u7bc7\u6587\u7ae0\u3002 \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u6839\u636e\u4e0a\u9762\u547d\u4ee4\u6267\u884c\u5b8c\u540e\uff0c\u8f93\u51fa\u7684\u63d0\u793a\u4fe1\u606f\u8fdb\u884c\u64cd\u4f5c\u6765\u9a8c\u8bc1\u4e00\u4e0b\u96c6\u7fa4\u662f\u5426\u90e8\u7f72\u6210\u529f\u3002 # \u83b7\u53d6\u6307\u5b9a\u96c6\u7fa4\u7684\u914d\u7f6e\u6587\u4ef6\u6240\u5728\u7684\u8def\u5f84 $ export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"my-cluster\")\" $ kubectl cluster-info Kubernetes master is running at https://localhost:34458 KubeDNS is running at https://localhost:34458/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. $ kubectl get nodes NAME STATUS ROLES AGE VERSION my-cluster-control-plane Ready master 2m v1.15.3 $ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-6g66f 1/1 Running 0 21m coredns-86c58d9df4-pqcc4 1/1 Running 0 21m etcd-my-cluster-control-plane 1/1 Running 0 20m kube-apiserver-my-cluster-control-plane 1/1 Running 0 20m kube-controller-manager-my-cluster-control-plane 1/1 Running 0 20m kube-proxy-cjgnt 1/1 Running 0 21m kube-scheduler-my-cluster-control-plane 1/1 Running 0 21m weave-net-ls2v8 2/2 Running 1 21m \u4ece\u4e0a\u9762\u7684\u8f93\u51fa\u7ed3\u679c\uff0c\u53ef\u4ee5\u770b\u5230\u5355\u8282\u70b9\u7684 Kubernetes \u5df2\u7ecf\u642d\u5efa\u6210\u529f\u3002\u5355\u8282\u70b9\u96c6\u7fa4\u9ed8\u8ba4\u65b9\u5f0f\u542f\u52a8\u7684\u8282\u70b9\u7c7b\u578b\u662f control-plane \uff0c\u8be5\u8282\u70b9\u5305\u542b\u4e86\u6240\u6709\u7684\u7ec4\u4ef6\u3002\u8fd9\u4e9b\u7ec4\u4ef6\u5206\u522b\u662f\uff1a 2*Coredns \u3001 Etcd \u3001 Api-Server \u3001 Controller-Manager \u3001 Kube-Proxy \u3001 Sheduler \u548c\u7f51\u7edc\u63d2\u4ef6 Weave \uff0c\u76ee\u524d\u9ed8\u8ba4\u4f7f\u7528\u7684\u7f51\u7edc\u63d2\u4ef6\u4e5f\u662f Weave \u3002","title":"\u642d\u5efa\u4e00\u4e2a\u5355\u8282\u70b9\u96c6\u7fa4"},{"location":"%E6%8A%80%E6%9C%AF/%E4%BD%BF%E7%94%A8%20Kind%20%E5%9C%A8%205%20%E5%88%86%E9%92%9F%E5%86%85%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%20Kubernetes%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/#_2","text":"\u9ed8\u8ba4\u5b89\u88c5\u7684\u96c6\u7fa4\u53ea\u90e8\u7f72\u4e86\u4e00\u4e2a\u63a7\u5236\u8282\u70b9\uff0c\u5982\u679c\u9700\u8981\u90e8\u7f72\u591a\u8282\u70b9\u96c6\u7fa4\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u7684\u65b9\u5f0f\u6765\u521b\u5efa\u591a\u4e2a\u5bb9\u5668\u3002\u8fd9\u6837\u5c31\u53ef\u4ee5\u8fbe\u5230\u6a21\u62df\u591a\u4e2a\u8282\u70b9\u76ee\u7684\uff0c\u5e76\u4ee5\u8fd9\u4e9b\u8282\u70b9\u6765\u6784\u5efa\u4e00\u4e2a\u591a\u8282\u70b9\u7684 Kubernetes \u96c6\u7fa4\u3002 \u521b\u5efa\u591a\u8282\u70b9 Kubernetes \u96c6\u7fa4\u914d\u7f6e\u6587\u4ef6 Kind \u5728\u521b\u5efa\u96c6\u7fa4\u7684\u65f6\u5019\uff0c\u652f\u6301\u901a\u8fc7 --config \u53c2\u6570\u4f20\u9012\u914d\u7f6e\u6587\u4ef6\u7ed9 Kind \uff0c\u914d\u7f6e\u6587\u4ef6\u53ef\u4fee\u6539\u7684\u5185\u5bb9\u4e3b\u8981\u6709 role \u548c \u8282\u70b9\u4f7f\u7528\u7684\u955c\u50cf\u3002 $ vim my-cluster-multi-node.yaml # \u4e00\u5171\u4e24\u4e2a\u8282\u70b9\uff0c\u4e00\u4e2a\u4e3b\u8282\u70b9\uff0c\u4e00\u4e2a\u4ece\u8282\u70b9\u3002 kind: Cluster apiVersion: kind.sigs.k8s.io/v1alpha3 nodes: - role: control-plane - role: worker \u521b\u5efa\u591a\u8282\u70b9 Kubernetes \u96c6\u7fa4 \u914d\u7f6e\u6587\u4ef6\u521b\u5efa\u5b8c\u6210\u540e\uff0c\u5c31\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u6765\u5b8c\u6210\u591a\u8282\u70b9 Kubernetes \u96c6\u7fa4\u642d\u5efa\u3002 $ kind create cluster --config my-cluster-multi-node.yaml --name my-cluster-multi-node Creating cluster \"my-cluster-multi-node\" ... \u2713 Ensuring node image (kindest/node:v1.15.3) ? \u2713 Preparing nodes ?? \u2713 Creating kubeadm config ? \u2713 Starting control-plane ?\ufe0f \u2713 Joining worker nodes ? Cluster creation complete. You can now use the cluster with: export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"my-cluster-multi-node\")\" kubectl cluster-info \u548c\u4e0a\u9762\u521b\u5efa\u7684\u5355\u8282\u70b9\u96c6\u7fa4\u4e00\u6837\uff0c\u6211\u4eec\u540c\u6837\u6839\u636e\u4e0a\u9762\u547d\u4ee4\u6267\u884c\u5b8c\u540e\uff0c\u8f93\u51fa\u7684\u63d0\u793a\u4fe1\u606f\u8fdb\u884c\u64cd\u4f5c\u6765\u9a8c\u8bc1\u4e00\u4e0b\u96c6\u7fa4\u662f\u5426\u90e8\u7f72\u6210\u529f\u3002 $ kubectl get nodes NAME STATUS ROLES AGE VERSION my-cluster-multi-node-control-plane Ready master 3m20s v1.15.3 my-cluster-multi-node-worker Ready <none> 3m8s v1.15.3 $ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE coredns-86c58d9df4-cnqhc 1/1 Running 0 5m29s coredns-86c58d9df4-hn9mv 1/1 Running 0 5m29s etcd-my-cluster-multi-node-control-plane 1/1 Running 0 4m24s kube-apiserver-my-cluster-multi-node-control-plane 1/1 Running 0 4m17s kube-controller-manager-my-cluster-multi-node-control-plane 1/1 Running 0 4m21s kube-proxy-8t4xt 1/1 Running 0 5m27s kube-proxy-skd5v 1/1 Running 0 5m29s kube-scheduler-my-cluster-multi-node-control-plane 1/1 Running 0 4m18s weave-net-nmfq2 2/2 Running 1 5m27s weave-net-srdfw 2/2 Running 0 5m29s","title":"\u521b\u5efa\u591a\u8282\u70b9\u7684\u96c6\u7fa4"},{"location":"%E6%8A%80%E6%9C%AF/%E4%BD%BF%E7%94%A8%20Kind%20%E5%9C%A8%205%20%E5%88%86%E9%92%9F%E5%86%85%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%20Kubernetes%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/#kubernetes","text":"Kind \u4e5f\u652f\u6301\u642d\u5efa\u9ad8\u53ef\u7528\u7684 Kubernetes \u96c6\u7fa4\uff0c\u521b\u5efa\u65b9\u5f0f\u548c\u591a\u8282\u70b9\u96c6\u7fa4\u7c7b\u4f3c\uff0c\u4e5f\u662f\u901a\u8fc7\u914d\u7f6e\u6587\u4ef6\u6765\u5b9e\u73b0\u3002 \u521b\u5efa\u9ad8\u53ef\u7528 Kubernetes \u96c6\u7fa4\u914d\u7f6e\u6587\u4ef6 $ vim my-cluster-ha.yaml # \u4e00\u5171\u516d\u4e2a\u8282\u70b9\uff0c\u4e09\u4e2a control-plane \u8282\u70b9\uff0c\u4e09\u4e2a workers \u8282\u70b9 kind: Cluster apiVersion: kind.sigs.k8s.io/v1alpha3 kubeadmConfigPatches: - | apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration metadata: name: config networking: serviceSubnet: 10.0.0.0/16 imageRepository: registry.aliyuncs.com/google_containers nodeRegistration: kubeletExtraArgs: pod-infra-container-image: registry.aliyuncs.com/google_containers/pause:3.1 - | apiVersion: kubeadm.k8s.io/v1beta2 kind: InitConfiguration metadata: name: config networking: serviceSubnet: 10.0.0.0/16 imageRepository: registry.aliyuncs.com/google_containers nodes: - role: control-plane - role: control-plane - role: control-plane - role: worker - role: worker - role: worker \u8fd9\u91cc\uff0c\u6211\u4eec\u901a\u8fc7\u76f4\u63a5\u5728\u914d\u7f6e\u6587\u4ef6\u91cc\u4f7f\u7528\u56fd\u5185\u5bb9\u5668\u955c\u50cf\u6e90\u7684\u65b9\u5f0f\u89e3\u51b3\u4e86\u5b98\u65b9\u5bb9\u5668\u955c\u50cf\u6e90\u4e0d\u53ef\u7528\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4e5f\u8fbe\u5230\u4e86\u52a0\u901f\u96c6\u7fa4\u521b\u5efa\u7684\u76ee\u7684\u3002 \u521b\u5efa\u9ad8\u53ef\u7528 Kubernetes \u96c6\u7fa4 \u914d\u7f6e\u6587\u4ef6\u521b\u5efa\u5b8c\u6210\u540e\uff0c\u5c31\u53ef\u4ee5\u4f7f\u7528\u4e0b\u9762\u7684\u547d\u4ee4\u6765\u5b8c\u6210\u9ad8\u53ef\u7528 Kubernetes \u96c6\u7fa4\u642d\u5efa\u3002 $ kind create cluster --name my-cluster-ha --config my-cluster-ha.yaml Creating cluster \"my-cluster-ha\" ... \u2713 Ensuring node image (kindest/node:v1.15.3) ? \u2713 Preparing nodes ??????? \u2713 Starting the external load balancer \u2696\ufe0f \u2713 Creating kubeadm config ? \u2713 Starting control-plane ?\ufe0f \u2713 Joining more control-plane nodes ? \u2713 Joining worker nodes ? Cluster creation complete. You can now use the cluster with: export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"my-cluster-ha\")\" kubectl cluster-info master $ export KUBECONFIG=\"$(kind get kubeconfig-path --name=\"my-cluster-ha\")\" master $ kubectl cluster-info Kubernetes master is running at https://localhost:44019 KubeDNS is running at https://localhost:44019/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'. \u540c\u6837\uff0c\u6211\u4eec\u6839\u636e\u4e0a\u9762\u547d\u4ee4\u6267\u884c\u5b8c\u540e\uff0c\u8f93\u51fa\u7684\u63d0\u793a\u4fe1\u606f\u8fdb\u884c\u64cd\u4f5c\u6765\u9a8c\u8bc1\u4e00\u4e0b\u96c6\u7fa4\u662f\u5426\u90e8\u7f72\u6210\u529f\u3002 $ kubectl get nodes NAME STATUS ROLES AGE VERSION my-cluster-ha-control-plane Ready master 3m42s v1.15.3 my-cluster-ha-control-plane2 Ready master 3m24s v1.15.3 my-cluster-ha-control-plane3 Ready master 2m13s v1.15.3 my-cluster-ha-worker Ready <none> 96s v1.15.3 my-cluster-ha-worker2 Ready <none> 98s v1.15.3 my-cluster-ha-worker3 Ready <none> 95s v1.15.3 \u4ece\u4e0a\u9762\u7684\u8f93\u51fa\u7ed3\u679c\uff0c\u53ef\u4ee5\u770b\u5230\u5305\u542b\u4e86\u591a\u4e2a master \u8282\u70b9\uff0c\u8bf4\u660e\u9ad8\u53ef\u7528\u7684 Kubernetes \u96c6\u7fa4\u5df2\u7ecf\u642d\u5efa\u6210\u529f\u3002","title":"\u521b\u5efa\u9ad8\u53ef\u7528 Kubernetes \u96c6\u7fa4"},{"location":"%E6%8A%80%E6%9C%AF/%E4%BD%BF%E7%94%A8%20Kind%20%E5%9C%A8%205%20%E5%88%86%E9%92%9F%E5%86%85%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%20Kubernetes%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/#_3","text":"","title":"\u5176\u5b83\u76f8\u5173\u77e5\u8bc6"},{"location":"%E6%8A%80%E6%9C%AF/%E4%BD%BF%E7%94%A8%20Kind%20%E5%9C%A8%205%20%E5%88%86%E9%92%9F%E5%86%85%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%20Kubernetes%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/#kind_4","text":"Kind \u955c\u50cf\u4e00\u5171\u5206\u4e3a\u4e24\u7c7b\uff0c\u4e00\u7c7b\u662f Base \u955c\u50cf\uff0c\u53e6\u4e00\u7c7b\u662f Node \u955c\u50cf\u3002 Base \u955c\u50cf Base \u955c\u50cf\u76ee\u524d\u4f7f\u7528\u4e86 ubuntu:19.04 \u4f5c\u4e3a\u57fa\u7840\u955c\u50cf\uff0c\u5e76\u505a\u4e86\u4e0b\u9762\u7684\u8c03\u6574\uff1a \u5b89\u88c5 Systemd \u76f8\u5173\u7684\u5305\uff0c\u5e76\u8c03\u6574\u4e00\u4e9b\u914d\u7f6e\u4ee5\u9002\u5e94\u5728\u5bb9\u5668\u5185\u8fd0\u884c\u3002 \u5b89\u88c5 Kubernetes \u8fd0\u884c\u65f6\u7684\u4f9d\u8d56\u5305\uff0c\u6bd4\u5982: Conntrack \u3001 Socat \u3001 CNI \u7b49\u3002 \u5b89\u88c5\u5bb9\u5668\u8fd0\u884c\u73af\u5883\uff0c\u6bd4\u5982: Containerd \u3001 Crictl \u7b49\u3002 \u914d\u7f6e\u81ea\u5df1\u7684 ENTRYPOINT \u811a\u672c\uff0c\u4ee5\u9002\u5e94\u548c\u8c03\u6574\u5bb9\u5668\u5185\u8fd0\u884c\u7684\u95ee\u9898\u3002 \u66f4\u591a\u5177\u4f53\u7684\u6784\u5efa\u903b\u8f91\u53ef\u4ee5\u53c2\u8003\uff1ahttps://github.com/kubernetes-sigs/kind/blob/master/images/base/Dockerfile Node \u955c\u50cf Node \u955c\u50cf\u7684\u6784\u5efa\u6bd4\u8f83\u590d\u6742\uff0c\u76ee\u524d\u662f\u901a\u8fc7\u8fd0\u884c Base \u955c\u50cf\u5e76\u5728 Base \u955c\u50cf\u5185\u6267\u884c\u64cd\u4f5c\uff0c\u518d\u4fdd\u5b58\u6b64\u5bb9\u5668\u5185\u5bb9\u4e3a\u955c\u50cf\u7684\u65b9\u5f0f\u6765\u6784\u5efa\u7684\uff0c\u5305\u542b\u7684\u64cd\u4f5c\u6709\uff1a \u6784\u5efa Kubernetes \u76f8\u5173\u8d44\u6e90\uff0c\u6bd4\u5982\uff1a\u4e8c\u8fdb\u5236\u6587\u4ef6\u548c\u955c\u50cf\u3002 \u8fd0\u884c\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u7684\u5bb9\u5668 \u628a\u6784\u5efa\u7684 Kubernetes \u76f8\u5173\u8d44\u6e90\u590d\u5236\u5230\u5bb9\u5668\u91cc \u8c03\u6574\u90e8\u5206\u7ec4\u4ef6\u914d\u7f6e\u53c2\u6570\uff0c\u4ee5\u652f\u6301\u5728\u5bb9\u5668\u5185\u8fd0\u884c \u9884\u5148\u62c9\u53bb\u8fd0\u884c\u73af\u5883\u9700\u8981\u7684\u955c\u50cf \u901a\u8fc7 docker commit \u65b9\u5f0f\u4fdd\u5b58\u5f53\u524d\u7684\u6784\u5efa\u5bb9\u5668\u4e3a Node \u955c\u50cf","title":"Kind \u7684\u955c\u50cf\u91cc\u7684\u79d8\u5bc6"},{"location":"%E6%8A%80%E6%9C%AF/%E4%BD%BF%E7%94%A8%20Kind%20%E5%9C%A8%205%20%E5%88%86%E9%92%9F%E5%86%85%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%20Kubernetes%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/#_4","text":"\u5982\u679c\u4f60\u4e0d\u9700\u8981\u672c\u5730\u7684\u96c6\u7fa4\u73af\u5883\uff0c\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u8fdb\u884c\u5220\u9664\uff1a $ kind delete cluster --name my-cluster Deleting cluster \"my-cluster\" ... $KUBECONFIG is still set to use /root/.kube/kind-config-my-cluster even though that file has been deleted, remember to unset it \u81f3\u6b64\uff0c\u6211\u4eec\u5c31\u6f14\u793a\u5b8c\u4e86\u5982\u4f55\u4f7f\u7528 Kind \u5feb\u901f\u642d\u5efa\u4e00\u4e2a Kubernetes \u96c6\u7fa4\u3002\u4e0d\u8fc7\u6709\u4e00\u4e2a\u4f60\u9700\u8981\u6ce8\u610f\u7684\u5730\u65b9\uff0c Kind \u642d\u5efa\u7684\u96c6\u7fa4\u4e0d\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883\u4e2d\u4f7f\u7528\u3002\u4f46\u662f\u5982\u679c\u4f60\u60f3\u5728\u672c\u5730\u5feb\u901f\u6784\u5efa\u4e00\u4e2a Kubernetes \u96c6\u7fa4\u73af\u5883\uff0c\u5e76\u4e14\u4e0d\u60f3\u5360\u7528\u592a\u591a\u7684\u786c\u4ef6\u8d44\u6e90\uff0c\u90a3\u4e48 Kind \u4f1a\u662f\u4f60\u4e0d\u9519\u7684\u9009\u62e9\u3002","title":"\u5982\u4f55\u5feb\u901f\u5220\u9664\u4e00\u4e2a\u96c6\u7fa4"},{"location":"%E6%8A%80%E6%9C%AF/%E4%BD%BF%E7%94%A8%20Kind%20%E5%9C%A8%205%20%E5%88%86%E9%92%9F%E5%86%85%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%20Kubernetes%20%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4/#_5","text":"https://www.google.com http://t.cn/AiRVBwDS https://zhuanlan.zhihu.com/p/60464867 https://yeya24.github.io/post/kind/ http://dockerone.com/article/8974 https://kind.sigs.k8s.io/docs/user/quick-start/","title":"\u53c2\u8003\u6587\u6863"},{"location":"%E6%95%85%E9%9A%9C/docker%E6%95%85%E9%9A%9C/","text":"docker\u6545\u969c","title":"docker\u6545\u969c"},{"location":"%E6%95%85%E9%9A%9C/docker%E6%95%85%E9%9A%9C/#docker","text":"","title":"docker\u6545\u969c"},{"location":"%E6%95%85%E9%9A%9C/etcd%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/","text":"etcd\u914d\u7f6e\u6587\u4ef6 [root@host-134-84-77-200 ~]# cat /etc/etcd/etcd.conf ETCD_NAME=\"etcd_77_200\" ETCD_DATA_DIR=\"/apps/etcd_data/etcd\" ETCD_CIPHER_SUITES=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 ETCD_CERT_FILE=/etc/kubernetes/ssl/etcd_server.crt ETCD_KEY_FILE=/etc/kubernetes/ssl/etcd_server.key ETCD_TRUSTED_CA_FILE=/etc/kubernetes/ssl/ca.crt ETCD_CLIENT_CERT_AUTH=true ETCD_LISTEN_CLIENT_URLS=\"https://134.84.77.200:1159,https://127.0.0.1:1159\" ETCD_ADVERTISE_CLIENT_URLS=\"https://134.84.77.200:1159,https://127.0.0.1:1159\" ETCD_PEER_CERT_FILE=/etc/kubernetes/ssl/etcd_server.crt ETCD_PEER_KEY_FILE=/etc/kubernetes/ssl/etcd_server.key ETCD_PEER_TRUSTED_CA_FILE=/etc/kubernetes/ssl/ca.crt ETCD_INITIAL_CLUSTER=\"etcd_77_200=https://134.84.77.200:2380,etcd_76_180=https://134.84.76.180:2380,etcd_78_84=https://134.84.78.84:2380\" ETCD_INITIAL_CLUSTER_STATE=\"new\" ETCD_INITIAL_CLUSTER_TOKEN=\"cc34c326-4694-48c6-afdf-c317f40c1847\" ETCD_LISTEN_PEER_URLS=\"https://134.84.77.200:2380\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://134.84.77.200:2380\" [root@host-134-84-77-200 ~]# etcd\u670d\u52a1\u5355\u5143\u6587\u4ef6 [root@host-134-84-77-200 ~]# cat /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target [Service] Type=notify WorkingDirectory=/apps/etcd_data/etcd EnvironmentFile=-/etc/etcd/etcd.conf ExecStart=/usr/bin/etcd RestartSec=3 Restart=on-failure [Install] WantedBy=multi-user.target [root@host-134-84-77-200 ~]# etcd \u670d\u52a1\u72b6\u6001 [root@host-134-84-77-200 ~]# systemctl status etcd \u25cf etcd.service - Etcd Server Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: inactive (dead) (Result: exit-code) since Thu 2025-01-23 22:49:49 CST; 8 months 1 days ago Main PID: 34358 (code=exited, status=1/FAILURE) Sep 25 16:38:05 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:38:05 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:39:59 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:39:59 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:48:40 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:48:40 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:54:39 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:54:39 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 17:00:49 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 17:00:49 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. etcd\u65e5\u5fd7 [root@host-134-84-77-200 ~]# journalctl -xe -u etcd -- -- The job identifier is 51123310 and the job result is dependency. Sep 25 16:16:23 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:22:35 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. -- Subject: A start job for unit etcd.service has failed -- Defined-By: systemd -- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel -- -- A start job for unit etcd.service has finished with a failure. -- -- The job identifier is 51123502 and the job result is dependency. Sep 25 16:22:35 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:38:05 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. -- Subject: A start job for unit etcd.service has failed -- Defined-By: systemd -- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel -- -- A start job for unit etcd.service has finished with a failure. -- etcd\u6392\u67e5\u811a\u672c #!/bin/bash # check_etcd_node.sh # \u7528\u4e8e\u6392\u67e5 etcd \u8282\u70b9\u542f\u52a8\u95ee\u9898 ETCD_CONF=\"/etc/etcd/etcd.conf\" ETCD_BIN=\"/usr/bin/etcd\" WORKDIR=$(grep \"^ETCD_DATA_DIR\" $ETCD_CONF | cut -d= -f2 | tr -d '\"') CERT=$(grep \"^ETCD_CERT_FILE\" $ETCD_CONF | cut -d= -f2 | tr -d '\"') KEY=$(grep \"^ETCD_KEY_FILE\" $ETCD_CONF | cut -d= -f2 | tr -d '\"') CA=$(grep \"^ETCD_TRUSTED_CA_FILE\" $ETCD_CONF | cut -d= -f2 | tr -d '\"') echo \"===== 1. \u68c0\u67e5\u7f51\u7edc\u670d\u52a1 =====\" systemctl is-active network &>/dev/null && echo \"network \u670d\u52a1\u6b63\u5e38\" || echo \"network \u670d\u52a1\u672a\u542f\u52a8\" systemctl is-active NetworkManager &>/dev/null && echo \"NetworkManager \u670d\u52a1\u6b63\u5e38\" || echo \"NetworkManager \u670d\u52a1\u672a\u542f\u52a8\" echo \"===== 2. \u68c0\u67e5 etcd \u670d\u52a1\u72b6\u6001 =====\" systemctl status etcd -l --no-pager echo \"===== 3. \u68c0\u67e5 etcd \u8fdb\u7a0b =====\" ps -ef | grep [e]tcd echo \"===== 4. \u68c0\u67e5\u6570\u636e\u76ee\u5f55 =====\" if [ -d \"$WORKDIR\" ]; then echo \"\u6570\u636e\u76ee\u5f55\u5b58\u5728: $WORKDIR\" ls -ld \"$WORKDIR\" else echo \"\u6570\u636e\u76ee\u5f55\u4e0d\u5b58\u5728: $WORKDIR\" fi echo \"===== 5. \u68c0\u67e5\u8bc1\u4e66\u6587\u4ef6 =====\" for f in \"$CERT\" \"$KEY\" \"$CA\"; do if [ -f \"$f\" ]; then echo \"\u8bc1\u4e66\u6587\u4ef6\u5b58\u5728: $f\" ls -l \"$f\" else echo \"\u8bc1\u4e66\u6587\u4ef6\u7f3a\u5931: $f\" fi done echo \"===== 6. \u68c0\u67e5\u7aef\u53e3\u76d1\u542c =====\" ss -lntp | grep etcd || echo \"etcd \u7aef\u53e3\u672a\u76d1\u542c\" echo \"===== 7. \u5c1d\u8bd5\u624b\u52a8\u542f\u52a8 etcd \u68c0\u67e5\u9519\u8bef =====\" echo \"==== \u6ce8\u610f\uff1a\u8fd9\u662f\u6d4b\u8bd5\u542f\u52a8\uff0c\u4e0d\u4f1a\u540e\u53f0\u8fd0\u884c ====\" $ETCD_BIN --config-file $ETCD_CONF etcd \u811a\u672c\u6267\u884c\u7ed3\u679c [root@host-134-84-77-200 ~]# sh check_etcd_node.sh ===== 1. \u68c0\u67e5\u7f51\u7edc\u670d\u52a1 ===== network \u670d\u52a1\u672a\u542f\u52a8 NetworkManager \u670d\u52a1\u6b63\u5e38 ===== 2. \u68c0\u67e5 etcd \u670d\u52a1\u72b6\u6001 ===== \u25cf etcd.service - Etcd Server Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: inactive (dead) (Result: exit-code) since Thu 2025-01-23 22:49:49 CST; 8 months 1 days ago Main PID: 34358 (code=exited, status=1/FAILURE) Sep 25 16:38:05 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:38:05 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:39:59 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:39:59 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:48:40 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:48:40 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:54:39 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:54:39 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 17:00:49 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 17:00:49 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. ===== 3. \u68c0\u67e5 etcd \u8fdb\u7a0b ===== root 8468 1 4 2024 ? 16-05:08:06 /usr/bin/kube-apiserver --bind-address=0.0.0.0 --secure-port=6443 --authorization-mode=RBAC --anonymous-auth=false --etcd-servers=https://134.84.77.200:1159,https://134.84.76.180:1159,https://134.84.78.84:1159 --service-cluster-ip-range=169.169.0.0/16 --etcd-certfile=/etc/kubernetes/ssl/etcd_client.crt --etcd-keyfile=/etc/kubernetes/ssl/etcd_client.key --etcd-cafile=/etc/kubernetes/ssl/ca.crt --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/ssl/sa.pub --service-account-signing-key-file=/etc/kubernetes/ssl/sa.key --kubelet-client-certificate=/etc/kubernetes/ssl/client.crt --kubelet-client-key=/etc/kubernetes/ssl/client.key --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook --service-node-port-range=20000-32767 --client-ca-file=/etc/kubernetes/ssl/ca.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 --tls-private-key-file=/etc/kubernetes/ssl/apiserver.key --tls-cert-file=/etc/kubernetes/ssl/apiserver.crt --v=0 --allow-privileged=true --event-ttl=48h0m0s --max-mutating-requests-inflight=500 --max-requests-inflight=1500 --default-watch-cache-size=10000 --apiserver-count=3 --endpoint-reconciler-type=lease --requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.crt --requestheader-allowed-names= --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client.key --enable-aggregator-routing=true --oidc-issuer-url=https://keycloak.sdpod-77-200-20030.4a.cmit.cloud:20129/auth/realms/kem --oidc-client-id=kubernetes --oidc-username-claim=preferred_username --oidc-username-prefix=- --oidc-groups-claim=groups --oidc-ca-file=/etc/kubernetes/pki/ca_ssl/tls.crt --audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/apps/monitor/oblogs/kube-apiserver/audit.log --feature-gates= aiox 12572 12538 0 May08 ? 09:59:59 /usr/local/bin/etcd 5000 33511 33473 0 17:11 ? 00:00:00 /usr/local/openresty//luajit/bin/luajit ./apisix/cli/apisix.lua init_etcd root 33758 18036 0 17:11 pts/1 00:00:00 sh check_etcd_node.sh ===== 4. \u68c0\u67e5\u6570\u636e\u76ee\u5f55 ===== \u6570\u636e\u76ee\u5f55\u5b58\u5728: /apps/etcd_data/etcd drwxr-xr-x 3 root root 20 Aug 27 18:20 /apps/etcd_data/etcd ===== 5. \u68c0\u67e5\u8bc1\u4e66\u6587\u4ef6 ===== \u8bc1\u4e66\u6587\u4ef6\u5b58\u5728: /etc/kubernetes/ssl/etcd_server.crt -rw-r--r-- 1 root root 1367 Sep 19 2024 /etc/kubernetes/ssl/etcd_server.crt \u8bc1\u4e66\u6587\u4ef6\u5b58\u5728: /etc/kubernetes/ssl/etcd_server.key -rw-r--r-- 1 root root 1679 Sep 19 2024 /etc/kubernetes/ssl/etcd_server.key \u8bc1\u4e66\u6587\u4ef6\u5b58\u5728: /etc/kubernetes/ssl/ca.crt -rw-r--r-- 1 root root 1346 Sep 19 2024 /etc/kubernetes/ssl/ca.crt ===== 6. \u68c0\u67e5\u7aef\u53e3\u76d1\u542c ===== etcd \u7aef\u53e3\u672a\u76d1\u542c ===== 7. \u5c1d\u8bd5\u624b\u52a8\u542f\u52a8 etcd \u68c0\u67e5\u9519\u8bef ===== ==== \u6ce8\u610f\uff1a\u8fd9\u662f\u6d4b\u8bd5\u542f\u52a8\uff0c\u4e0d\u4f1a\u540e\u53f0\u8fd0\u884c ==== {\"level\":\"info\",\"ts\":\"2025-09-25T17:11:33.367+0800\",\"caller\":\"etcdmain/etcd.go:73\",\"msg\":\"Running: \",\"args\":[\"/usr/bin/etcd\",\"--config-file\",\"/etc/etcd/etcd.conf\"]} {\"level\":\"warn\",\"ts\":\"2025-09-25T17:11:33.367+0800\",\"caller\":\"etcdmain/etcd.go:75\",\"msg\":\"failed to verify flags\",\"error\":\"error unmarshaling JSON: while decoding JSON: json: cannot unmarshal string into Go value of type embed.configYAML\"} [root@host-134-84-77-200 ~]# \u68c0\u67e5etcd\u96c6\u7fa4\u72b6\u6001 [root@host-134-84-77-200 ~]# cat check_etcd.sh ETCDCTL_API=3 etcdctl --endpoints=https://134.84.77.200:1159,https://134.84.76.180:1159,https://134.84.78.84:1159 --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key --cacert=/etc/kubernetes/ssl/ca.crt member list -w table ETCDCTL_API=3 etcdctl --endpoints=https://134.84.77.200:1159,https://134.84.76.180:1159,https://134.84.78.84:1159 --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key --cacert=/etc/kubernetes/ssl/ca.crt endpoint health -w table ETCDCTL_API=3 etcdctl --endpoints=https://134.84.77.200:1159,https://134.84.76.180:1159,https://134.84.78.84:1159 --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key --cacert=/etc/kubernetes/ssl/ca.crt endpoint status -w table [root@host-134-84-77-200 ~]# [root@host-134-84-77-200 ~]# sh check_etcd.sh +------------------+---------+-------------+----------------------------+---------------------------------------------------+------------+ | ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS | IS LEARNER | +------------------+---------+-------------+----------------------------+---------------------------------------------------+------------+ | 21bc8eac53683297 | started | etcd_77_200 | https://134.84.77.200:2380 | https://127.0.0.1:1159,https://134.84.77.200:1159 | false | | 5fe1cd5d0739c590 | started | etcd_76_180 | https://134.84.76.180:2380 | https://127.0.0.1:1159,https://134.84.76.180:1159 | false | | c36ef91fe0394790 | started | etcd_78_84 | https://134.84.78.84:2380 | https://127.0.0.1:1159,https://134.84.78.84:1159 | false | +------------------+---------+-------------+----------------------------+---------------------------------------------------+------------+ {\"level\":\"warn\",\"ts\":\"2025-09-25T17:13:47.180+0800\",\"logger\":\"client\",\"caller\":\"v3/retry_interceptor.go:62\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc0005901c0/134.84.77.200:1159\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing dial tcp 134.84.77.200:1159: connect: connection refused\\\"\"} +----------------------------+--------+--------------+---------------------------+ | ENDPOINT | HEALTH | TOOK | ERROR | +----------------------------+--------+--------------+---------------------------+ | https://134.84.76.180:1159 | true | 11.228944ms | | | https://134.84.78.84:1159 | true | 11.46168ms | | | https://134.84.77.200:1159 | false | 5.002421269s | context deadline exceeded | +----------------------------+--------+--------------+---------------------------+ Error: unhealthy cluster {\"level\":\"warn\",\"ts\":\"2025-09-25T17:13:52.205+0800\",\"logger\":\"etcd-client\",\"caller\":\"v3/retry_interceptor.go:62\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc00037ea80/134.84.77.200:1159\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing dial tcp 134.84.77.200:1159: connect: connection refused\\\"\"} Failed to get the status of endpoint https://134.84.77.200:1159 (context deadline exceeded) +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | https://134.84.76.180:1159 | 5fe1cd5d0739c590 | 3.5.4 | 76 MB | true | false | 108 | 122835465 | 122835465 | | | https://134.84.78.84:1159 | c36ef91fe0394790 | 3.5.4 | 76 MB | false | false | 108 | 122835465 | 122835465 | | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ [root@host-134-84-77-200 ~]# NetworkManager\u72b6\u6001 [root@host-134-84-77-200 ~]# systemctl status NetworkManager \u25cf NetworkManager.service - Network Manager Loaded: loaded (/usr/lib/systemd/system/NetworkManager.service; enabled; vendor preset: enabled) Drop-In: /usr/lib/systemd/system/NetworkManager.service.d \u2514\u2500NetworkManager-ovs.conf Active: active (running) since Thu 2025-09-25 16:38:22 CST; 39min ago Docs: man:NetworkManager(8) Main PID: 37777 (NetworkManager) Tasks: 4 Memory: 4.9M CGroup: /system.slice/NetworkManager.service \u251c\u250037777 /usr/sbin/NetworkManager --no-daemon \u2514\u250037797 /sbin/dhclient -d -q -sf /usr/libexec/nm-dhcp-helper -pf /var/run/NetworkManager/dhclient-enp4s3.pid -lf /var/lib/NetworkManager/dhclient-533eea9f-b33d-480b-828f-1ef3d511> Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3411] manager: NetworkManager state is now CONNECTED_LOCAL Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3420] policy: set 'enp4s3' (enp4s3) as default for IPv4 routing and DNS Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3421] policy: set 'enp4s3' (enp4s3) as default for IPv6 routing and DNS Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3473] device (docker0): Activation: successful, device activated. Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3483] device (enp4s3): state change: ip-check -> secondaries (reason 'none', sys-iface-state: 'assume') Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3485] device (enp4s3): state change: secondaries -> activated (reason 'none', sys-iface-state: 'assume') Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3489] manager: NetworkManager state is now CONNECTED_SITE Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3496] device (enp4s3): Activation: successful, device activated. Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3502] manager: NetworkManager state is now CONNECTED_GLOBAL Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3512] manager: startup complete [root@host-134-84-77-200 ~]# \u6587\u4ef6\u76ee\u5f55\u6743\u9650 [root@host-134-84-77-200 ~]# ls -ltr /apps/etcd_data/etcd/ total 0 drwx------ 4 root root 29 Aug 27 18:20 member [root@host-134-84-77-200 ~]# ls -ltr /apps/etcd_data/ total 0 drwxr-xr-x 3 root root 20 Aug 27 18:20 etcd [root@host-134-84-77-200 ~]# ls -ltr /etc/etcd/etcd.conf -rw-r--r-- 1 root root 1225 Sep 25 16:55 /etc/etcd/etcd.conf [root@host-134-84-77-200 ~]# \u4f7f\u7528 strace systemctl start etcd \u547d\u4ee4 \u67e5\u770b\u53d1\u73b0\u8d44\u6e90\u4e34\u65f6\u4e0d\u53ef\u7528 ppoll([{fd=3, events=POLLIN}], 1, NULL, NULL, 8) = 1 ([{fd=3, revents=POLLIN}]) recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"l\\4\\1\\1\\220\\0\\0\\0\\310\\0\\0\\0\\276\\0\\0\\0\\1\\1o\\0002\\0\\0\\0\", iov_len=24}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 24 recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"/org/freedesktop/systemd1/unit/d\"..., iov_len=328}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 328 recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"l\\4\\1\\1d\\3\\0\\0\\311\\0\\0\\0\\276\\0\\0\\0\\1\\1o\\0002\\0\\0\\0\", iov_len=24}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 24 recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"/org/freedesktop/systemd1/unit/d\"..., iov_len=1052}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 1052 recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"l\\4\\1\\1d\\0\\0\\0\\312\\0\\0\\0\\36\\1\\0\\0\\1\\1o\\0\\220\\0\\0\\0\", iov_len=24}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 24 recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"/org/freedesktop/systemd1/unit/d\"..., iov_len=380}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 380 recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"l\\4\\1\\1d\\3\\0\\0\\313\\0\\0\\0\\36\\1\\0\\0\\1\\1o\\0\\220\\0\\0\\0\", iov_len=24}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 24 recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"/org/freedesktop/systemd1/unit/d\"..., iov_len=1148}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 1148 recvmsg(3, {msg_namelen=0}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = -1 EAGAIN (Resource temporarily unavailable) \u89e3\u51b3\u65b9\u6848 \u91cd\u542f\u5bbf\u4e3b\u673a\u53d1\u73b0etcd\u670d\u52a1\u542f\u52a8\u6b63\u5e38\uff01\uff01\uff01","title":"Etcd\u8282\u70b9\u542f\u52a8\u5931\u8d25"},{"location":"%E6%95%85%E9%9A%9C/etcd%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/#etcd","text":"[root@host-134-84-77-200 ~]# cat /etc/etcd/etcd.conf ETCD_NAME=\"etcd_77_200\" ETCD_DATA_DIR=\"/apps/etcd_data/etcd\" ETCD_CIPHER_SUITES=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 ETCD_CERT_FILE=/etc/kubernetes/ssl/etcd_server.crt ETCD_KEY_FILE=/etc/kubernetes/ssl/etcd_server.key ETCD_TRUSTED_CA_FILE=/etc/kubernetes/ssl/ca.crt ETCD_CLIENT_CERT_AUTH=true ETCD_LISTEN_CLIENT_URLS=\"https://134.84.77.200:1159,https://127.0.0.1:1159\" ETCD_ADVERTISE_CLIENT_URLS=\"https://134.84.77.200:1159,https://127.0.0.1:1159\" ETCD_PEER_CERT_FILE=/etc/kubernetes/ssl/etcd_server.crt ETCD_PEER_KEY_FILE=/etc/kubernetes/ssl/etcd_server.key ETCD_PEER_TRUSTED_CA_FILE=/etc/kubernetes/ssl/ca.crt ETCD_INITIAL_CLUSTER=\"etcd_77_200=https://134.84.77.200:2380,etcd_76_180=https://134.84.76.180:2380,etcd_78_84=https://134.84.78.84:2380\" ETCD_INITIAL_CLUSTER_STATE=\"new\" ETCD_INITIAL_CLUSTER_TOKEN=\"cc34c326-4694-48c6-afdf-c317f40c1847\" ETCD_LISTEN_PEER_URLS=\"https://134.84.77.200:2380\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://134.84.77.200:2380\" [root@host-134-84-77-200 ~]#","title":"etcd\u914d\u7f6e\u6587\u4ef6"},{"location":"%E6%95%85%E9%9A%9C/etcd%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/#etcd_1","text":"[root@host-134-84-77-200 ~]# cat /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target [Service] Type=notify WorkingDirectory=/apps/etcd_data/etcd EnvironmentFile=-/etc/etcd/etcd.conf ExecStart=/usr/bin/etcd RestartSec=3 Restart=on-failure [Install] WantedBy=multi-user.target [root@host-134-84-77-200 ~]#","title":"etcd\u670d\u52a1\u5355\u5143\u6587\u4ef6"},{"location":"%E6%95%85%E9%9A%9C/etcd%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/#etcd_2","text":"[root@host-134-84-77-200 ~]# systemctl status etcd \u25cf etcd.service - Etcd Server Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: inactive (dead) (Result: exit-code) since Thu 2025-01-23 22:49:49 CST; 8 months 1 days ago Main PID: 34358 (code=exited, status=1/FAILURE) Sep 25 16:38:05 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:38:05 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:39:59 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:39:59 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:48:40 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:48:40 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:54:39 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:54:39 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 17:00:49 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 17:00:49 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'.","title":"etcd \u670d\u52a1\u72b6\u6001"},{"location":"%E6%95%85%E9%9A%9C/etcd%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/#etcd_3","text":"[root@host-134-84-77-200 ~]# journalctl -xe -u etcd -- -- The job identifier is 51123310 and the job result is dependency. Sep 25 16:16:23 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:22:35 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. -- Subject: A start job for unit etcd.service has failed -- Defined-By: systemd -- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel -- -- A start job for unit etcd.service has finished with a failure. -- -- The job identifier is 51123502 and the job result is dependency. Sep 25 16:22:35 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:38:05 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. -- Subject: A start job for unit etcd.service has failed -- Defined-By: systemd -- Support: https://lists.freedesktop.org/mailman/listinfo/systemd-devel -- -- A start job for unit etcd.service has finished with a failure. --","title":"etcd\u65e5\u5fd7"},{"location":"%E6%95%85%E9%9A%9C/etcd%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/#etcd_4","text":"#!/bin/bash # check_etcd_node.sh # \u7528\u4e8e\u6392\u67e5 etcd \u8282\u70b9\u542f\u52a8\u95ee\u9898 ETCD_CONF=\"/etc/etcd/etcd.conf\" ETCD_BIN=\"/usr/bin/etcd\" WORKDIR=$(grep \"^ETCD_DATA_DIR\" $ETCD_CONF | cut -d= -f2 | tr -d '\"') CERT=$(grep \"^ETCD_CERT_FILE\" $ETCD_CONF | cut -d= -f2 | tr -d '\"') KEY=$(grep \"^ETCD_KEY_FILE\" $ETCD_CONF | cut -d= -f2 | tr -d '\"') CA=$(grep \"^ETCD_TRUSTED_CA_FILE\" $ETCD_CONF | cut -d= -f2 | tr -d '\"') echo \"===== 1. \u68c0\u67e5\u7f51\u7edc\u670d\u52a1 =====\" systemctl is-active network &>/dev/null && echo \"network \u670d\u52a1\u6b63\u5e38\" || echo \"network \u670d\u52a1\u672a\u542f\u52a8\" systemctl is-active NetworkManager &>/dev/null && echo \"NetworkManager \u670d\u52a1\u6b63\u5e38\" || echo \"NetworkManager \u670d\u52a1\u672a\u542f\u52a8\" echo \"===== 2. \u68c0\u67e5 etcd \u670d\u52a1\u72b6\u6001 =====\" systemctl status etcd -l --no-pager echo \"===== 3. \u68c0\u67e5 etcd \u8fdb\u7a0b =====\" ps -ef | grep [e]tcd echo \"===== 4. \u68c0\u67e5\u6570\u636e\u76ee\u5f55 =====\" if [ -d \"$WORKDIR\" ]; then echo \"\u6570\u636e\u76ee\u5f55\u5b58\u5728: $WORKDIR\" ls -ld \"$WORKDIR\" else echo \"\u6570\u636e\u76ee\u5f55\u4e0d\u5b58\u5728: $WORKDIR\" fi echo \"===== 5. \u68c0\u67e5\u8bc1\u4e66\u6587\u4ef6 =====\" for f in \"$CERT\" \"$KEY\" \"$CA\"; do if [ -f \"$f\" ]; then echo \"\u8bc1\u4e66\u6587\u4ef6\u5b58\u5728: $f\" ls -l \"$f\" else echo \"\u8bc1\u4e66\u6587\u4ef6\u7f3a\u5931: $f\" fi done echo \"===== 6. \u68c0\u67e5\u7aef\u53e3\u76d1\u542c =====\" ss -lntp | grep etcd || echo \"etcd \u7aef\u53e3\u672a\u76d1\u542c\" echo \"===== 7. \u5c1d\u8bd5\u624b\u52a8\u542f\u52a8 etcd \u68c0\u67e5\u9519\u8bef =====\" echo \"==== \u6ce8\u610f\uff1a\u8fd9\u662f\u6d4b\u8bd5\u542f\u52a8\uff0c\u4e0d\u4f1a\u540e\u53f0\u8fd0\u884c ====\" $ETCD_BIN --config-file $ETCD_CONF","title":"etcd\u6392\u67e5\u811a\u672c"},{"location":"%E6%95%85%E9%9A%9C/etcd%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/#etcd_5","text":"[root@host-134-84-77-200 ~]# sh check_etcd_node.sh ===== 1. \u68c0\u67e5\u7f51\u7edc\u670d\u52a1 ===== network \u670d\u52a1\u672a\u542f\u52a8 NetworkManager \u670d\u52a1\u6b63\u5e38 ===== 2. \u68c0\u67e5 etcd \u670d\u52a1\u72b6\u6001 ===== \u25cf etcd.service - Etcd Server Loaded: loaded (/usr/lib/systemd/system/etcd.service; enabled; vendor preset: disabled) Active: inactive (dead) (Result: exit-code) since Thu 2025-01-23 22:49:49 CST; 8 months 1 days ago Main PID: 34358 (code=exited, status=1/FAILURE) Sep 25 16:38:05 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:38:05 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:39:59 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:39:59 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:48:40 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:48:40 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 16:54:39 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 16:54:39 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. Sep 25 17:00:49 host-134-84-77-200 systemd[1]: Dependency failed for Etcd Server. Sep 25 17:00:49 host-134-84-77-200 systemd[1]: etcd.service: Job etcd.service/start failed with result 'dependency'. ===== 3. \u68c0\u67e5 etcd \u8fdb\u7a0b ===== root 8468 1 4 2024 ? 16-05:08:06 /usr/bin/kube-apiserver --bind-address=0.0.0.0 --secure-port=6443 --authorization-mode=RBAC --anonymous-auth=false --etcd-servers=https://134.84.77.200:1159,https://134.84.76.180:1159,https://134.84.78.84:1159 --service-cluster-ip-range=169.169.0.0/16 --etcd-certfile=/etc/kubernetes/ssl/etcd_client.crt --etcd-keyfile=/etc/kubernetes/ssl/etcd_client.key --etcd-cafile=/etc/kubernetes/ssl/ca.crt --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/ssl/sa.pub --service-account-signing-key-file=/etc/kubernetes/ssl/sa.key --kubelet-client-certificate=/etc/kubernetes/ssl/client.crt --kubelet-client-key=/etc/kubernetes/ssl/client.key --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook --service-node-port-range=20000-32767 --client-ca-file=/etc/kubernetes/ssl/ca.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256 --tls-private-key-file=/etc/kubernetes/ssl/apiserver.key --tls-cert-file=/etc/kubernetes/ssl/apiserver.crt --v=0 --allow-privileged=true --event-ttl=48h0m0s --max-mutating-requests-inflight=500 --max-requests-inflight=1500 --default-watch-cache-size=10000 --apiserver-count=3 --endpoint-reconciler-type=lease --requestheader-client-ca-file=/etc/kubernetes/ssl/front-proxy-ca.crt --requestheader-allowed-names= --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --proxy-client-cert-file=/etc/kubernetes/ssl/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/ssl/front-proxy-client.key --enable-aggregator-routing=true --oidc-issuer-url=https://keycloak.sdpod-77-200-20030.4a.cmit.cloud:20129/auth/realms/kem --oidc-client-id=kubernetes --oidc-username-claim=preferred_username --oidc-username-prefix=- --oidc-groups-claim=groups --oidc-ca-file=/etc/kubernetes/pki/ca_ssl/tls.crt --audit-policy-file=/etc/kubernetes/audit-policy.yaml --audit-log-path=/apps/monitor/oblogs/kube-apiserver/audit.log --feature-gates= aiox 12572 12538 0 May08 ? 09:59:59 /usr/local/bin/etcd 5000 33511 33473 0 17:11 ? 00:00:00 /usr/local/openresty//luajit/bin/luajit ./apisix/cli/apisix.lua init_etcd root 33758 18036 0 17:11 pts/1 00:00:00 sh check_etcd_node.sh ===== 4. \u68c0\u67e5\u6570\u636e\u76ee\u5f55 ===== \u6570\u636e\u76ee\u5f55\u5b58\u5728: /apps/etcd_data/etcd drwxr-xr-x 3 root root 20 Aug 27 18:20 /apps/etcd_data/etcd ===== 5. \u68c0\u67e5\u8bc1\u4e66\u6587\u4ef6 ===== \u8bc1\u4e66\u6587\u4ef6\u5b58\u5728: /etc/kubernetes/ssl/etcd_server.crt -rw-r--r-- 1 root root 1367 Sep 19 2024 /etc/kubernetes/ssl/etcd_server.crt \u8bc1\u4e66\u6587\u4ef6\u5b58\u5728: /etc/kubernetes/ssl/etcd_server.key -rw-r--r-- 1 root root 1679 Sep 19 2024 /etc/kubernetes/ssl/etcd_server.key \u8bc1\u4e66\u6587\u4ef6\u5b58\u5728: /etc/kubernetes/ssl/ca.crt -rw-r--r-- 1 root root 1346 Sep 19 2024 /etc/kubernetes/ssl/ca.crt ===== 6. \u68c0\u67e5\u7aef\u53e3\u76d1\u542c ===== etcd \u7aef\u53e3\u672a\u76d1\u542c ===== 7. \u5c1d\u8bd5\u624b\u52a8\u542f\u52a8 etcd \u68c0\u67e5\u9519\u8bef ===== ==== \u6ce8\u610f\uff1a\u8fd9\u662f\u6d4b\u8bd5\u542f\u52a8\uff0c\u4e0d\u4f1a\u540e\u53f0\u8fd0\u884c ==== {\"level\":\"info\",\"ts\":\"2025-09-25T17:11:33.367+0800\",\"caller\":\"etcdmain/etcd.go:73\",\"msg\":\"Running: \",\"args\":[\"/usr/bin/etcd\",\"--config-file\",\"/etc/etcd/etcd.conf\"]} {\"level\":\"warn\",\"ts\":\"2025-09-25T17:11:33.367+0800\",\"caller\":\"etcdmain/etcd.go:75\",\"msg\":\"failed to verify flags\",\"error\":\"error unmarshaling JSON: while decoding JSON: json: cannot unmarshal string into Go value of type embed.configYAML\"} [root@host-134-84-77-200 ~]#","title":"etcd \u811a\u672c\u6267\u884c\u7ed3\u679c"},{"location":"%E6%95%85%E9%9A%9C/etcd%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/#etcd_6","text":"[root@host-134-84-77-200 ~]# cat check_etcd.sh ETCDCTL_API=3 etcdctl --endpoints=https://134.84.77.200:1159,https://134.84.76.180:1159,https://134.84.78.84:1159 --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key --cacert=/etc/kubernetes/ssl/ca.crt member list -w table ETCDCTL_API=3 etcdctl --endpoints=https://134.84.77.200:1159,https://134.84.76.180:1159,https://134.84.78.84:1159 --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key --cacert=/etc/kubernetes/ssl/ca.crt endpoint health -w table ETCDCTL_API=3 etcdctl --endpoints=https://134.84.77.200:1159,https://134.84.76.180:1159,https://134.84.78.84:1159 --cert=/etc/kubernetes/ssl/etcd_server.crt --key=/etc/kubernetes/ssl/etcd_server.key --cacert=/etc/kubernetes/ssl/ca.crt endpoint status -w table [root@host-134-84-77-200 ~]# [root@host-134-84-77-200 ~]# sh check_etcd.sh +------------------+---------+-------------+----------------------------+---------------------------------------------------+------------+ | ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS | IS LEARNER | +------------------+---------+-------------+----------------------------+---------------------------------------------------+------------+ | 21bc8eac53683297 | started | etcd_77_200 | https://134.84.77.200:2380 | https://127.0.0.1:1159,https://134.84.77.200:1159 | false | | 5fe1cd5d0739c590 | started | etcd_76_180 | https://134.84.76.180:2380 | https://127.0.0.1:1159,https://134.84.76.180:1159 | false | | c36ef91fe0394790 | started | etcd_78_84 | https://134.84.78.84:2380 | https://127.0.0.1:1159,https://134.84.78.84:1159 | false | +------------------+---------+-------------+----------------------------+---------------------------------------------------+------------+ {\"level\":\"warn\",\"ts\":\"2025-09-25T17:13:47.180+0800\",\"logger\":\"client\",\"caller\":\"v3/retry_interceptor.go:62\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc0005901c0/134.84.77.200:1159\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing dial tcp 134.84.77.200:1159: connect: connection refused\\\"\"} +----------------------------+--------+--------------+---------------------------+ | ENDPOINT | HEALTH | TOOK | ERROR | +----------------------------+--------+--------------+---------------------------+ | https://134.84.76.180:1159 | true | 11.228944ms | | | https://134.84.78.84:1159 | true | 11.46168ms | | | https://134.84.77.200:1159 | false | 5.002421269s | context deadline exceeded | +----------------------------+--------+--------------+---------------------------+ Error: unhealthy cluster {\"level\":\"warn\",\"ts\":\"2025-09-25T17:13:52.205+0800\",\"logger\":\"etcd-client\",\"caller\":\"v3/retry_interceptor.go:62\",\"msg\":\"retrying of unary invoker failed\",\"target\":\"etcd-endpoints://0xc00037ea80/134.84.77.200:1159\",\"attempt\":0,\"error\":\"rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = \\\"transport: Error while dialing dial tcp 134.84.77.200:1159: connect: connection refused\\\"\"} Failed to get the status of endpoint https://134.84.77.200:1159 (context deadline exceeded) +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | https://134.84.76.180:1159 | 5fe1cd5d0739c590 | 3.5.4 | 76 MB | true | false | 108 | 122835465 | 122835465 | | | https://134.84.78.84:1159 | c36ef91fe0394790 | 3.5.4 | 76 MB | false | false | 108 | 122835465 | 122835465 | | +----------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ [root@host-134-84-77-200 ~]#","title":"\u68c0\u67e5etcd\u96c6\u7fa4\u72b6\u6001"},{"location":"%E6%95%85%E9%9A%9C/etcd%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/#networkmanager","text":"[root@host-134-84-77-200 ~]# systemctl status NetworkManager \u25cf NetworkManager.service - Network Manager Loaded: loaded (/usr/lib/systemd/system/NetworkManager.service; enabled; vendor preset: enabled) Drop-In: /usr/lib/systemd/system/NetworkManager.service.d \u2514\u2500NetworkManager-ovs.conf Active: active (running) since Thu 2025-09-25 16:38:22 CST; 39min ago Docs: man:NetworkManager(8) Main PID: 37777 (NetworkManager) Tasks: 4 Memory: 4.9M CGroup: /system.slice/NetworkManager.service \u251c\u250037777 /usr/sbin/NetworkManager --no-daemon \u2514\u250037797 /sbin/dhclient -d -q -sf /usr/libexec/nm-dhcp-helper -pf /var/run/NetworkManager/dhclient-enp4s3.pid -lf /var/lib/NetworkManager/dhclient-533eea9f-b33d-480b-828f-1ef3d511> Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3411] manager: NetworkManager state is now CONNECTED_LOCAL Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3420] policy: set 'enp4s3' (enp4s3) as default for IPv4 routing and DNS Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3421] policy: set 'enp4s3' (enp4s3) as default for IPv6 routing and DNS Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3473] device (docker0): Activation: successful, device activated. Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3483] device (enp4s3): state change: ip-check -> secondaries (reason 'none', sys-iface-state: 'assume') Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3485] device (enp4s3): state change: secondaries -> activated (reason 'none', sys-iface-state: 'assume') Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3489] manager: NetworkManager state is now CONNECTED_SITE Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3496] device (enp4s3): Activation: successful, device activated. Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3502] manager: NetworkManager state is now CONNECTED_GLOBAL Sep 25 16:38:22 host-134-84-77-200 NetworkManager[37777]: <info> [1758789502.3512] manager: startup complete [root@host-134-84-77-200 ~]#","title":"NetworkManager\u72b6\u6001"},{"location":"%E6%95%85%E9%9A%9C/etcd%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/#_1","text":"[root@host-134-84-77-200 ~]# ls -ltr /apps/etcd_data/etcd/ total 0 drwx------ 4 root root 29 Aug 27 18:20 member [root@host-134-84-77-200 ~]# ls -ltr /apps/etcd_data/ total 0 drwxr-xr-x 3 root root 20 Aug 27 18:20 etcd [root@host-134-84-77-200 ~]# ls -ltr /etc/etcd/etcd.conf -rw-r--r-- 1 root root 1225 Sep 25 16:55 /etc/etcd/etcd.conf [root@host-134-84-77-200 ~]#","title":"\u6587\u4ef6\u76ee\u5f55\u6743\u9650"},{"location":"%E6%95%85%E9%9A%9C/etcd%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/#strace-systemctl-start-etcd","text":"\u67e5\u770b\u53d1\u73b0\u8d44\u6e90\u4e34\u65f6\u4e0d\u53ef\u7528 ppoll([{fd=3, events=POLLIN}], 1, NULL, NULL, 8) = 1 ([{fd=3, revents=POLLIN}]) recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"l\\4\\1\\1\\220\\0\\0\\0\\310\\0\\0\\0\\276\\0\\0\\0\\1\\1o\\0002\\0\\0\\0\", iov_len=24}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 24 recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"/org/freedesktop/systemd1/unit/d\"..., iov_len=328}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 328 recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"l\\4\\1\\1d\\3\\0\\0\\311\\0\\0\\0\\276\\0\\0\\0\\1\\1o\\0002\\0\\0\\0\", iov_len=24}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 24 recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"/org/freedesktop/systemd1/unit/d\"..., iov_len=1052}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 1052 recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"l\\4\\1\\1d\\0\\0\\0\\312\\0\\0\\0\\36\\1\\0\\0\\1\\1o\\0\\220\\0\\0\\0\", iov_len=24}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 24 recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"/org/freedesktop/systemd1/unit/d\"..., iov_len=380}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 380 recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"l\\4\\1\\1d\\3\\0\\0\\313\\0\\0\\0\\36\\1\\0\\0\\1\\1o\\0\\220\\0\\0\\0\", iov_len=24}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 24 recvmsg(3, {msg_name=NULL, msg_namelen=0, msg_iov=[{iov_base=\"/org/freedesktop/systemd1/unit/d\"..., iov_len=1148}], msg_iovlen=1, msg_controllen=0, msg_flags=MSG_CMSG_CLOEXEC}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = 1148 recvmsg(3, {msg_namelen=0}, MSG_DONTWAIT|MSG_CMSG_CLOEXEC) = -1 EAGAIN (Resource temporarily unavailable)","title":"\u4f7f\u7528strace systemctl start etcd\u547d\u4ee4"},{"location":"%E6%95%85%E9%9A%9C/etcd%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/#_2","text":"","title":"\u89e3\u51b3\u65b9\u6848"},{"location":"%E6%95%85%E9%9A%9C/etcd%E8%8A%82%E7%82%B9%E5%90%AF%E5%8A%A8%E5%A4%B1%E8%B4%A5/#etcd_7","text":"","title":"\u91cd\u542f\u5bbf\u4e3b\u673a\u53d1\u73b0etcd\u670d\u52a1\u542f\u52a8\u6b63\u5e38\uff01\uff01\uff01"},{"location":"%E6%95%85%E9%9A%9C/kubernetes%E6%95%85%E9%9A%9C/","text":"kubernetes\u6545\u969c","title":"kubernetes\u6545\u969c"},{"location":"%E6%95%85%E9%9A%9C/kubernetes%E6%95%85%E9%9A%9C/#kubernetes","text":"","title":"kubernetes\u6545\u969c"},{"location":"%E7%9B%91%E6%8E%A7/%E5%9C%A8kubeadm%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%90%AD%E5%BB%BA%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9FEFK/","text":"\u5728kubeadm\u96c6\u7fa4\u4e2d\u642d\u5efa\u4f01\u4e1a\u7ea7\u65e5\u5fd7\u7cfb\u7edfEFK \u786e\u8ba4\u96c6\u7fa4\u6b63\u5e38\uff1a kubectl cluster-info kubectl get nodes -o wide \u7531\u4e8e\u4f60\u662f \u5355\u8282\u70b9\u96c6\u7fa4 \uff0c\u9ed8\u8ba4 control-plane \u8282\u70b9\u6709\u6c61\u70b9\uff0cPod \u4e0d\u4f1a\u8c03\u5ea6\u5230\u5b83\u3002\u53ef\u4ee5\u9009\u62e9\uff1a \u65b9\u6cd5A\uff08\u63a8\u8350\uff09\uff1a \u5728\u540e\u9762 YAML \u91cc\u90fd\u52a0\u4e0a tolerations\u3002 \u65b9\u6cd5B\uff1a \u76f4\u63a5\u79fb\u9664\u6c61\u70b9\uff1a kubectl taint nodes --all node-role.kubernetes.io/control-plane- || true \u68c0\u67e5\u6709\u6ca1\u6709\u9ed8\u8ba4\u7684 StorageClass \uff08\u540e\u9762 Elasticsearch \u8981\u7528PVC\u5b58\u50a8\u6570\u636e\uff09\uff1a kubectl get sc \u5982\u679c\u6ca1\u6709\uff0c\u53ef\u4ee5\u5feb\u901f\u88c5\u4e00\u4e2a\u8f7b\u91cf\u7684\u672c\u5730\u5b58\u50a8\uff1a kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml kubectl annotate sc local-path storageclass.kubernetes.io/is-default-class=\"true\" \u8981\u4e0d\u8981\u6211\u5e2e\u4f60\u5148\u5199\u4e00\u4e2a \u521b\u5efa\u547d\u540d\u7a7a\u95f4 efk \u7684 YAML \u548c\u547d\u4ee4\uff1f\u8fd9\u6837\u6211\u4eec\u4ece\u8fd9\u91cc\u5f00\u59cb\uff0c\u4e00\u6b65\u6b65\u5f80\u4e0b\u642d\u5efa\u3002 1\ufe0f\u20e3 \u521b\u5efa\u547d\u540d\u7a7a\u95f4 efk \u4f60\u53ef\u4ee5\u76f4\u63a5\u6267\u884c\u547d\u4ee4\uff1a kubectl create namespace efk 1\ufe0f\u20e3 \u521b\u5efa\u547d\u540d\u7a7a\u95f4 efk \u4f60\u53ef\u4ee5\u76f4\u63a5\u6267\u884c\u547d\u4ee4\uff1a kubectl create namespace efk \u6216\u8005\u5199\u6210\u4e00\u4e2a YAML \u6e05\u5355\uff08\u5efa\u8bae\u7528\u8fd9\u79cd\u65b9\u5f0f\uff0c\u65b9\u4fbf\u7248\u672c\u7ba1\u7406\uff09\uff1a # efk-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: efk \u6267\u884c\uff1a kubectl apply -f efk-namespace.yaml \u7136\u540e\u9a8c\u8bc1\uff1a kubectl get ns \ud83d\udc49 \u4e0b\u4e00\u6b65\u6211\u4eec\u8981\u5904\u7406 Elasticsearch \u7684\u524d\u7f6e\u5185\u6838\u53c2\u6570 vm.max_map_count \uff08\u5426\u5219\u542f\u52a8\u4f1a\u62a5\u9519\uff09\u3002 \u8981\u4e0d\u8981\u6211\u5148\u5e2e\u4f60\u5199\u4e00\u4e2a DaemonSet \u6765\u8bbe\u7f6e vm.max_map_count \uff1f 2\ufe0f\u20e3 \u521b\u5efa DaemonSet \u8bbe\u7f6e\u5185\u6838\u53c2\u6570 \u65b0\u5efa\u6587\u4ef6 efk-sysctl.yaml \uff1a apiVersion: apps/v1 kind: DaemonSet metadata: name: efk-sysctl namespace: efk spec: selector: matchLabels: app: efk-sysctl template: metadata: labels: app: efk-sysctl spec: hostPID: true tolerations: - key: \"node-role.kubernetes.io/control-plane\" operator: \"Exists\" effect: \"NoSchedule\" containers: - name: sysctl image: busybox:1.36 securityContext: privileged: true command: [\"sh\", \"-c\"] args: - | sysctl -w vm.max_map_count=262144 sysctl -w fs.file-max=1048576 # \u5bb9\u5668\u4fdd\u6301\u5b58\u6d3b sleep 3600000 \u5e94\u7528\uff1a kubectl apply -f efk-sysctl.yaml \u68c0\u67e5\u662f\u5426\u8fd0\u884c\u6210\u529f\uff1a kubectl -n efk get pods -o wide \u767b\u5f55\u8282\u70b9\u786e\u8ba4\u53c2\u6570\u5df2\u751f\u6548\uff1a sysctl -n vm.max_map_count \u6b63\u5e38\u7684\u8bdd\u5e94\u8be5\u8f93\u51fa\uff1a 262144 3\ufe0f\u20e3 \u90e8\u7f72 Elasticsearch\uff08\u5355\u8282\u70b9\uff09 \u65b0\u5efa\u6587\u4ef6 elasticsearch-single.yaml \uff1a apiVersion: v1 kind: Service metadata: name: elasticsearch namespace: efk labels: app: elasticsearch spec: ports: - name: http port: 9200 targetPort: 9200 selector: app: elasticsearch --- apiVersion: apps/v1 kind: StatefulSet metadata: name: elasticsearch namespace: efk spec: serviceName: elasticsearch replicas: 1 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch spec: tolerations: - key: \"node-role.kubernetes.io/control-plane\" operator: \"Exists\" effect: \"NoSchedule\" containers: - name: es image: docker.io/elasticsearch:8.14.1 ports: - containerPort: 9200 name: http - containerPort: 9300 name: transport env: - name: discovery.type value: single-node - name: ES_JAVA_OPTS value: \"-Xms512m -Xmx512m\" - name: xpack.security.enabled value: \"false\" # \u5355\u673a\u6d4b\u8bd5\u5173\u95ed\u5b89\u5168\u8ba4\u8bc1 volumeMounts: - name: es-data mountPath: /usr/share/elasticsearch/data resources: requests: cpu: \"250m\" memory: \"1Gi\" limits: cpu: \"1\" memory: \"2Gi\" volumeClaimTemplates: - metadata: name: es-data spec: accessModes: [\"ReadWriteOnce\"] storageClassName: \"local-path\" # \u5982\u679c\u4f60\u7528\u4e86 local-path-provisioner resources: requests: storage: 20Gi \u5e94\u7528\uff1a kubectl apply -f elasticsearch-single.yaml \u7b49\u5f85\u542f\u52a8\u5b8c\u6210\uff1a kubectl -n efk rollout status sts/elasticsearch \u68c0\u67e5 Pod\uff1a kubectl -n efk get pods -o wide \u6d4b\u8bd5\u8bbf\u95ee\uff1a kubectl -n efk port-forward svc/elasticsearch 9200:9200 & curl http://127.0.0.1:9200 \u6b63\u5e38\u7684\u8bdd\u4f1a\u8fd4\u56de Elasticsearch \u7684 JSON \u4fe1\u606f\u3002 ChatGPT \u8bf4\uff1a \u597d\u7684 \ud83d\udc4d \u6211\u4eec\u7ee7\u7eed\u6765\u90e8\u7f72 Kibana \uff0c\u5b83\u662f Elasticsearch \u7684\u53ef\u89c6\u5316\u754c\u9762\u3002 4\ufe0f\u20e3 \u90e8\u7f72 Kibana \u65b0\u5efa\u6587\u4ef6 kibana.yaml \uff1a apiVersion: v1 kind: Service metadata: name: kibana namespace: efk labels: app: kibana spec: type: NodePort # \u5355\u8282\u70b9\u96c6\u7fa4\uff0c\u65b9\u4fbf\u5916\u90e8\u8bbf\u95ee ports: - name: http port: 5601 targetPort: 5601 nodePort: 30601 # \u8bbf\u95ee\u7aef\u53e3\uff0c\u53ef\u81ea\u884c\u4fee\u6539 selector: app: kibana --- apiVersion: apps/v1 kind: Deployment metadata: name: kibana namespace: efk spec: replicas: 1 selector: matchLabels: app: kibana template: metadata: labels: app: kibana spec: tolerations: - key: \"node-role.kubernetes.io/control-plane\" operator: \"Exists\" effect: \"NoSchedule\" containers: - name: kibana image: docker.io/kibana:8.14.1 ports: - containerPort: 5601 env: - name: ELASTICSEARCH_HOSTS value: \"[\\\"http://elasticsearch.efk.svc:9200\\\"]\" - name: NODE_OPTIONS value: \"--max-old-space-size=2048\" resources: requests: cpu: \"100m\" memory: \"256Mi\" limits: cpu: \"500m\" memory: \"512Mi\" \u5e94\u7528\uff1a kubectl apply -f kibana.yaml \u7b49\u5f85\u542f\u52a8\uff1a kubectl -n efk rollout status deploy/kibana \u68c0\u67e5 Pod\uff1a kubectl -n efk get pods -o wide \u7136\u540e\u8bbf\u95ee\uff1a http://<\u4f60\u7684\u8282\u70b9IP>:30601 \u5c31\u80fd\u770b\u5230 Kibana \u754c\u9762\u5566 \ud83c\udf89 \u2705 Kibana \u5df2\u7ecf\u642d\u5efa\u597d\u4e86\u3002 5\ufe0f\u20e3 \u90e8\u7f72 Fluent Bit 5.1 \u521b\u5efa RBAC \u65b0\u5efa\u6587\u4ef6 fluent-bit-rbac.yaml \uff1a apiVersion: v1 kind: ServiceAccount metadata: name: fluent-bit namespace: efk --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: fluent-bit-read rules: - apiGroups: [\"\"] resources: [\"pods\", \"namespaces\"] verbs: [\"get\", \"list\", \"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: fluent-bit-read roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: fluent-bit-read subjects: - kind: ServiceAccount name: fluent-bit namespace: efk \u5e94\u7528\uff1a kubectl apply -f fluent-bit-rbac.yaml 5.2 \u521b\u5efa ConfigMap \u65b0\u5efa\u6587\u4ef6 fluent-bit-config.yaml \uff1a apiVersion: v1 kind: ConfigMap metadata: name: fluent-bit-config namespace: efk data: fluent-bit.conf: | [SERVICE] Flush 5 Daemon Off Log_Level debug Parsers_File parsers.conf [INPUT] Name tail Path /var/log/containers/*.log Exclude_Path /var/log/containers/*_fluent-bit-* Tag kube.* Mem_Buf_Limit 50MB Skip_Long_Lines On Refresh_Interval 5 Parser docker [FILTER] Name kubernetes Match kube.* Kube_URL https://kubernetes.default.svc:443 Kube_Tag_Prefix kube.var.log.containers. Merge_Log On Keep_Log Off K8S-Logging.Parser On K8S-Logging.Exclude Off [OUTPUT] Name es Match kube.* Host elasticsearch.efk.svc Port 9200 Logstash_Format On Logstash_Prefix kubernetes Replace_Dots On Retry_Limit False parsers.conf: | [PARSER] Name docker Format json Time_Key time Time_Format %Y-%m-%dT%H:%M:%S.%L%z \u5e94\u7528\uff1a kubectl apply -f fluent-bit-config.yaml \u2705 \u5230\u8fd9\u91cc\uff0cRBAC \u548c\u914d\u7f6e\u6587\u4ef6\u5c31\u51c6\u5907\u597d\u4e86\u3002 \u4e0b\u4e00\u6b65\u5c31\u662f\u90e8\u7f72 Fluent Bit DaemonSet \uff0c\u5b83\u4f1a\u5728\u6bcf\u4e2a\u8282\u70b9\u8fd0\u884c\u5e76\u91c7\u96c6\u65e5\u5fd7\u3002 \u597d\uff0c\u6211\u4eec\u6700\u540e\u4e00\u6b65\u6765\u5199 Fluent Bit DaemonSet \uff0c\u8fd9\u6837\u5c31\u80fd\u5728\u8282\u70b9\u4e0a\u91c7\u96c6\u5bb9\u5668\u65e5\u5fd7\u5e76\u53d1\u9001\u5230 Elasticsearch\u3002 5.3 \u521b\u5efa Fluent Bit DaemonSet \u65b0\u5efa\u6587\u4ef6 fluent-bit-daemonset.yaml \uff1a apiVersion: apps/v1 kind: DaemonSet metadata: name: fluent-bit namespace: efk labels: app.kubernetes.io/name: fluent-bit spec: selector: matchLabels: app.kubernetes.io/name: fluent-bit template: metadata: labels: app.kubernetes.io/name: fluent-bit spec: serviceAccountName: fluent-bit securityContext: runAsUser: 0 runAsGroup: 0 fsGroup: 0 tolerations: - key: \"node-role.kubernetes.io/control-plane\" operator: \"Exists\" effect: \"NoSchedule\" containers: - name: fluent-bit image: fluent/fluent-bit:2.9.9 args: [\"-c\", \"/fluent-bit/etc/fluent-bit.conf\"] resources: requests: cpu: 100m memory: 128Mi limits: cpu: 500m memory: 256Mi volumeMounts: - name: varlog mountPath: /var/log - name: config mountPath: /fluent-bit/etc/ terminationGracePeriodSeconds: 10 volumes: - name: varlog hostPath: path: /var/log type: Directory - name: config configMap: name: fluent-bit-config defaultMode: 420 \u5e94\u7528\uff1a kubectl apply -f fluent-bit-daemonset.yaml \ud83d\udd0d \u9a8c\u8bc1\u65e5\u5fd7\u91c7\u96c6 \u67e5\u770b Pod \u662f\u5426\u6b63\u5e38\u8fd0\u884c\uff1a kubectl -n efk get pods -o wide \u67e5\u770b Fluent Bit \u65e5\u5fd7\u786e\u8ba4\u8f93\u51fa\u5230 ES\uff1a kubectl -n efk logs -l app.kubernetes.io/name=fluent-bit \u68c0\u67e5 Elasticsearch \u662f\u5426\u6709\u65b0\u7d22\u5f15\uff1a kubectl -n efk port-forward svc/elasticsearch 9200:9200 & curl 'http://127.0.0.1:9200/_cat/indices?v' \u5e94\u8be5\u80fd\u770b\u5230 kubernetes-* \u5f00\u5934\u7684\u7d22\u5f15\u3002 \u6253\u5f00 Kibana\uff08http://<\u4f60\u7684\u8282\u70b9IP>:30601\uff09\uff0c\u5728 Stack Management \u2192 Index Patterns \u91cc\u6dfb\u52a0 kubernetes-* \uff0c\u9009\u62e9 @timestamp \u5b57\u6bb5\uff0c\u5c31\u80fd\u5f00\u59cb\u5728 Discover \u91cc\u67e5\u770b\u65e5\u5fd7\u4e86 \ud83c\udf89 \u2705 \u5230\u8fd9\u91cc\uff0c\u4e00\u6574\u5957 EFK \u65e5\u5fd7\u7cfb\u7edf \u5c31\u90e8\u7f72\u5b8c\u6210\u4e86\u3002 \u6267\u884c\u62a5 kubectl -n efk port-forward svc/elasticsearch 9200:9200 & \u62a5\u9519 [root@k8s-master efk]# kubectl -n efk port-forward svc/elasticsearch 9200:9200 & [1] 67634 [root@k8s-master efk]# Forwarding from 127.0.0.1:9200 -> 9200 Handling connection for 9200 E0825 16:44:40.755824 67634 portforward.go:424] \"Unhandled Error\" err=\"an error occurred forwarding 9200 -> 9200: error forwarding port 9200 to pod 95b8653b6a8b5d4382d72af972b42cdc3a967c0cb50429cbd00ca703b424a510, uid : unable to do port forwarding: socat not found\" error: lost connection to pod \u89e3\u51b3\u65b9\u6848\uff1a # \u8bf4\u660e Kubernetes \u672c\u5730\u7684 port-forward \u529f\u80fd\u9700\u8981 socat \u6765\u8f6c\u53d1\u7aef\u53e3\uff0c\u4f46\u4f60\u7684\u7cfb\u7edf\u91cc\u6ca1\u6709\u5b89\u88c5\u5b83\u3002 yum install -y socat","title":"\u5728kubeadm\u96c6\u7fa4\u4e2d\u642d\u5efa\u4f01\u4e1a\u7ea7\u65e5\u5fd7\u7cfb\u7edfEFK"},{"location":"%E7%9B%91%E6%8E%A7/%E5%9C%A8kubeadm%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%90%AD%E5%BB%BA%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9FEFK/#kubeadmefk","text":"\u786e\u8ba4\u96c6\u7fa4\u6b63\u5e38\uff1a kubectl cluster-info kubectl get nodes -o wide \u7531\u4e8e\u4f60\u662f \u5355\u8282\u70b9\u96c6\u7fa4 \uff0c\u9ed8\u8ba4 control-plane \u8282\u70b9\u6709\u6c61\u70b9\uff0cPod \u4e0d\u4f1a\u8c03\u5ea6\u5230\u5b83\u3002\u53ef\u4ee5\u9009\u62e9\uff1a \u65b9\u6cd5A\uff08\u63a8\u8350\uff09\uff1a \u5728\u540e\u9762 YAML \u91cc\u90fd\u52a0\u4e0a tolerations\u3002 \u65b9\u6cd5B\uff1a \u76f4\u63a5\u79fb\u9664\u6c61\u70b9\uff1a kubectl taint nodes --all node-role.kubernetes.io/control-plane- || true \u68c0\u67e5\u6709\u6ca1\u6709\u9ed8\u8ba4\u7684 StorageClass \uff08\u540e\u9762 Elasticsearch \u8981\u7528PVC\u5b58\u50a8\u6570\u636e\uff09\uff1a kubectl get sc \u5982\u679c\u6ca1\u6709\uff0c\u53ef\u4ee5\u5feb\u901f\u88c5\u4e00\u4e2a\u8f7b\u91cf\u7684\u672c\u5730\u5b58\u50a8\uff1a kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml kubectl annotate sc local-path storageclass.kubernetes.io/is-default-class=\"true\" \u8981\u4e0d\u8981\u6211\u5e2e\u4f60\u5148\u5199\u4e00\u4e2a \u521b\u5efa\u547d\u540d\u7a7a\u95f4 efk \u7684 YAML \u548c\u547d\u4ee4\uff1f\u8fd9\u6837\u6211\u4eec\u4ece\u8fd9\u91cc\u5f00\u59cb\uff0c\u4e00\u6b65\u6b65\u5f80\u4e0b\u642d\u5efa\u3002","title":"\u5728kubeadm\u96c6\u7fa4\u4e2d\u642d\u5efa\u4f01\u4e1a\u7ea7\u65e5\u5fd7\u7cfb\u7edfEFK"},{"location":"%E7%9B%91%E6%8E%A7/%E5%9C%A8kubeadm%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%90%AD%E5%BB%BA%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9FEFK/#1-efk","text":"\u4f60\u53ef\u4ee5\u76f4\u63a5\u6267\u884c\u547d\u4ee4\uff1a kubectl create namespace efk","title":"1\ufe0f\u20e3 \u521b\u5efa\u547d\u540d\u7a7a\u95f4 efk"},{"location":"%E7%9B%91%E6%8E%A7/%E5%9C%A8kubeadm%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%90%AD%E5%BB%BA%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9FEFK/#1-efk_1","text":"\u4f60\u53ef\u4ee5\u76f4\u63a5\u6267\u884c\u547d\u4ee4\uff1a kubectl create namespace efk \u6216\u8005\u5199\u6210\u4e00\u4e2a YAML \u6e05\u5355\uff08\u5efa\u8bae\u7528\u8fd9\u79cd\u65b9\u5f0f\uff0c\u65b9\u4fbf\u7248\u672c\u7ba1\u7406\uff09\uff1a # efk-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: efk \u6267\u884c\uff1a kubectl apply -f efk-namespace.yaml \u7136\u540e\u9a8c\u8bc1\uff1a kubectl get ns \ud83d\udc49 \u4e0b\u4e00\u6b65\u6211\u4eec\u8981\u5904\u7406 Elasticsearch \u7684\u524d\u7f6e\u5185\u6838\u53c2\u6570 vm.max_map_count \uff08\u5426\u5219\u542f\u52a8\u4f1a\u62a5\u9519\uff09\u3002 \u8981\u4e0d\u8981\u6211\u5148\u5e2e\u4f60\u5199\u4e00\u4e2a DaemonSet \u6765\u8bbe\u7f6e vm.max_map_count \uff1f","title":"1\ufe0f\u20e3 \u521b\u5efa\u547d\u540d\u7a7a\u95f4 efk"},{"location":"%E7%9B%91%E6%8E%A7/%E5%9C%A8kubeadm%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%90%AD%E5%BB%BA%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9FEFK/#2-daemonset","text":"\u65b0\u5efa\u6587\u4ef6 efk-sysctl.yaml \uff1a apiVersion: apps/v1 kind: DaemonSet metadata: name: efk-sysctl namespace: efk spec: selector: matchLabels: app: efk-sysctl template: metadata: labels: app: efk-sysctl spec: hostPID: true tolerations: - key: \"node-role.kubernetes.io/control-plane\" operator: \"Exists\" effect: \"NoSchedule\" containers: - name: sysctl image: busybox:1.36 securityContext: privileged: true command: [\"sh\", \"-c\"] args: - | sysctl -w vm.max_map_count=262144 sysctl -w fs.file-max=1048576 # \u5bb9\u5668\u4fdd\u6301\u5b58\u6d3b sleep 3600000 \u5e94\u7528\uff1a kubectl apply -f efk-sysctl.yaml \u68c0\u67e5\u662f\u5426\u8fd0\u884c\u6210\u529f\uff1a kubectl -n efk get pods -o wide \u767b\u5f55\u8282\u70b9\u786e\u8ba4\u53c2\u6570\u5df2\u751f\u6548\uff1a sysctl -n vm.max_map_count \u6b63\u5e38\u7684\u8bdd\u5e94\u8be5\u8f93\u51fa\uff1a 262144","title":"2\ufe0f\u20e3 \u521b\u5efa DaemonSet \u8bbe\u7f6e\u5185\u6838\u53c2\u6570"},{"location":"%E7%9B%91%E6%8E%A7/%E5%9C%A8kubeadm%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%90%AD%E5%BB%BA%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9FEFK/#3-elasticsearch","text":"\u65b0\u5efa\u6587\u4ef6 elasticsearch-single.yaml \uff1a apiVersion: v1 kind: Service metadata: name: elasticsearch namespace: efk labels: app: elasticsearch spec: ports: - name: http port: 9200 targetPort: 9200 selector: app: elasticsearch --- apiVersion: apps/v1 kind: StatefulSet metadata: name: elasticsearch namespace: efk spec: serviceName: elasticsearch replicas: 1 selector: matchLabels: app: elasticsearch template: metadata: labels: app: elasticsearch spec: tolerations: - key: \"node-role.kubernetes.io/control-plane\" operator: \"Exists\" effect: \"NoSchedule\" containers: - name: es image: docker.io/elasticsearch:8.14.1 ports: - containerPort: 9200 name: http - containerPort: 9300 name: transport env: - name: discovery.type value: single-node - name: ES_JAVA_OPTS value: \"-Xms512m -Xmx512m\" - name: xpack.security.enabled value: \"false\" # \u5355\u673a\u6d4b\u8bd5\u5173\u95ed\u5b89\u5168\u8ba4\u8bc1 volumeMounts: - name: es-data mountPath: /usr/share/elasticsearch/data resources: requests: cpu: \"250m\" memory: \"1Gi\" limits: cpu: \"1\" memory: \"2Gi\" volumeClaimTemplates: - metadata: name: es-data spec: accessModes: [\"ReadWriteOnce\"] storageClassName: \"local-path\" # \u5982\u679c\u4f60\u7528\u4e86 local-path-provisioner resources: requests: storage: 20Gi \u5e94\u7528\uff1a kubectl apply -f elasticsearch-single.yaml \u7b49\u5f85\u542f\u52a8\u5b8c\u6210\uff1a kubectl -n efk rollout status sts/elasticsearch \u68c0\u67e5 Pod\uff1a kubectl -n efk get pods -o wide \u6d4b\u8bd5\u8bbf\u95ee\uff1a kubectl -n efk port-forward svc/elasticsearch 9200:9200 & curl http://127.0.0.1:9200 \u6b63\u5e38\u7684\u8bdd\u4f1a\u8fd4\u56de Elasticsearch \u7684 JSON \u4fe1\u606f\u3002","title":"3\ufe0f\u20e3 \u90e8\u7f72 Elasticsearch\uff08\u5355\u8282\u70b9\uff09"},{"location":"%E7%9B%91%E6%8E%A7/%E5%9C%A8kubeadm%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%90%AD%E5%BB%BA%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9FEFK/#chatgpt","text":"\u597d\u7684 \ud83d\udc4d \u6211\u4eec\u7ee7\u7eed\u6765\u90e8\u7f72 Kibana \uff0c\u5b83\u662f Elasticsearch \u7684\u53ef\u89c6\u5316\u754c\u9762\u3002","title":"ChatGPT \u8bf4\uff1a"},{"location":"%E7%9B%91%E6%8E%A7/%E5%9C%A8kubeadm%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%90%AD%E5%BB%BA%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9FEFK/#4-kibana","text":"\u65b0\u5efa\u6587\u4ef6 kibana.yaml \uff1a apiVersion: v1 kind: Service metadata: name: kibana namespace: efk labels: app: kibana spec: type: NodePort # \u5355\u8282\u70b9\u96c6\u7fa4\uff0c\u65b9\u4fbf\u5916\u90e8\u8bbf\u95ee ports: - name: http port: 5601 targetPort: 5601 nodePort: 30601 # \u8bbf\u95ee\u7aef\u53e3\uff0c\u53ef\u81ea\u884c\u4fee\u6539 selector: app: kibana --- apiVersion: apps/v1 kind: Deployment metadata: name: kibana namespace: efk spec: replicas: 1 selector: matchLabels: app: kibana template: metadata: labels: app: kibana spec: tolerations: - key: \"node-role.kubernetes.io/control-plane\" operator: \"Exists\" effect: \"NoSchedule\" containers: - name: kibana image: docker.io/kibana:8.14.1 ports: - containerPort: 5601 env: - name: ELASTICSEARCH_HOSTS value: \"[\\\"http://elasticsearch.efk.svc:9200\\\"]\" - name: NODE_OPTIONS value: \"--max-old-space-size=2048\" resources: requests: cpu: \"100m\" memory: \"256Mi\" limits: cpu: \"500m\" memory: \"512Mi\" \u5e94\u7528\uff1a kubectl apply -f kibana.yaml \u7b49\u5f85\u542f\u52a8\uff1a kubectl -n efk rollout status deploy/kibana \u68c0\u67e5 Pod\uff1a kubectl -n efk get pods -o wide \u7136\u540e\u8bbf\u95ee\uff1a http://<\u4f60\u7684\u8282\u70b9IP>:30601 \u5c31\u80fd\u770b\u5230 Kibana \u754c\u9762\u5566 \ud83c\udf89 \u2705 Kibana \u5df2\u7ecf\u642d\u5efa\u597d\u4e86\u3002","title":"4\ufe0f\u20e3 \u90e8\u7f72 Kibana"},{"location":"%E7%9B%91%E6%8E%A7/%E5%9C%A8kubeadm%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%90%AD%E5%BB%BA%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9FEFK/#5-fluent-bit","text":"","title":"5\ufe0f\u20e3 \u90e8\u7f72 Fluent Bit"},{"location":"%E7%9B%91%E6%8E%A7/%E5%9C%A8kubeadm%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%90%AD%E5%BB%BA%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9FEFK/#51-rbac","text":"\u65b0\u5efa\u6587\u4ef6 fluent-bit-rbac.yaml \uff1a apiVersion: v1 kind: ServiceAccount metadata: name: fluent-bit namespace: efk --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: fluent-bit-read rules: - apiGroups: [\"\"] resources: [\"pods\", \"namespaces\"] verbs: [\"get\", \"list\", \"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: fluent-bit-read roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: fluent-bit-read subjects: - kind: ServiceAccount name: fluent-bit namespace: efk \u5e94\u7528\uff1a kubectl apply -f fluent-bit-rbac.yaml","title":"5.1 \u521b\u5efa RBAC"},{"location":"%E7%9B%91%E6%8E%A7/%E5%9C%A8kubeadm%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%90%AD%E5%BB%BA%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9FEFK/#52-configmap","text":"\u65b0\u5efa\u6587\u4ef6 fluent-bit-config.yaml \uff1a apiVersion: v1 kind: ConfigMap metadata: name: fluent-bit-config namespace: efk data: fluent-bit.conf: | [SERVICE] Flush 5 Daemon Off Log_Level debug Parsers_File parsers.conf [INPUT] Name tail Path /var/log/containers/*.log Exclude_Path /var/log/containers/*_fluent-bit-* Tag kube.* Mem_Buf_Limit 50MB Skip_Long_Lines On Refresh_Interval 5 Parser docker [FILTER] Name kubernetes Match kube.* Kube_URL https://kubernetes.default.svc:443 Kube_Tag_Prefix kube.var.log.containers. Merge_Log On Keep_Log Off K8S-Logging.Parser On K8S-Logging.Exclude Off [OUTPUT] Name es Match kube.* Host elasticsearch.efk.svc Port 9200 Logstash_Format On Logstash_Prefix kubernetes Replace_Dots On Retry_Limit False parsers.conf: | [PARSER] Name docker Format json Time_Key time Time_Format %Y-%m-%dT%H:%M:%S.%L%z \u5e94\u7528\uff1a kubectl apply -f fluent-bit-config.yaml \u2705 \u5230\u8fd9\u91cc\uff0cRBAC \u548c\u914d\u7f6e\u6587\u4ef6\u5c31\u51c6\u5907\u597d\u4e86\u3002 \u4e0b\u4e00\u6b65\u5c31\u662f\u90e8\u7f72 Fluent Bit DaemonSet \uff0c\u5b83\u4f1a\u5728\u6bcf\u4e2a\u8282\u70b9\u8fd0\u884c\u5e76\u91c7\u96c6\u65e5\u5fd7\u3002 \u597d\uff0c\u6211\u4eec\u6700\u540e\u4e00\u6b65\u6765\u5199 Fluent Bit DaemonSet \uff0c\u8fd9\u6837\u5c31\u80fd\u5728\u8282\u70b9\u4e0a\u91c7\u96c6\u5bb9\u5668\u65e5\u5fd7\u5e76\u53d1\u9001\u5230 Elasticsearch\u3002","title":"5.2 \u521b\u5efa ConfigMap"},{"location":"%E7%9B%91%E6%8E%A7/%E5%9C%A8kubeadm%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%90%AD%E5%BB%BA%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9FEFK/#53-fluent-bit-daemonset","text":"\u65b0\u5efa\u6587\u4ef6 fluent-bit-daemonset.yaml \uff1a apiVersion: apps/v1 kind: DaemonSet metadata: name: fluent-bit namespace: efk labels: app.kubernetes.io/name: fluent-bit spec: selector: matchLabels: app.kubernetes.io/name: fluent-bit template: metadata: labels: app.kubernetes.io/name: fluent-bit spec: serviceAccountName: fluent-bit securityContext: runAsUser: 0 runAsGroup: 0 fsGroup: 0 tolerations: - key: \"node-role.kubernetes.io/control-plane\" operator: \"Exists\" effect: \"NoSchedule\" containers: - name: fluent-bit image: fluent/fluent-bit:2.9.9 args: [\"-c\", \"/fluent-bit/etc/fluent-bit.conf\"] resources: requests: cpu: 100m memory: 128Mi limits: cpu: 500m memory: 256Mi volumeMounts: - name: varlog mountPath: /var/log - name: config mountPath: /fluent-bit/etc/ terminationGracePeriodSeconds: 10 volumes: - name: varlog hostPath: path: /var/log type: Directory - name: config configMap: name: fluent-bit-config defaultMode: 420 \u5e94\u7528\uff1a kubectl apply -f fluent-bit-daemonset.yaml","title":"5.3 \u521b\u5efa Fluent Bit DaemonSet"},{"location":"%E7%9B%91%E6%8E%A7/%E5%9C%A8kubeadm%E9%9B%86%E7%BE%A4%E4%B8%AD%E6%90%AD%E5%BB%BA%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9FEFK/#_1","text":"\u67e5\u770b Pod \u662f\u5426\u6b63\u5e38\u8fd0\u884c\uff1a kubectl -n efk get pods -o wide \u67e5\u770b Fluent Bit \u65e5\u5fd7\u786e\u8ba4\u8f93\u51fa\u5230 ES\uff1a kubectl -n efk logs -l app.kubernetes.io/name=fluent-bit \u68c0\u67e5 Elasticsearch \u662f\u5426\u6709\u65b0\u7d22\u5f15\uff1a kubectl -n efk port-forward svc/elasticsearch 9200:9200 & curl 'http://127.0.0.1:9200/_cat/indices?v' \u5e94\u8be5\u80fd\u770b\u5230 kubernetes-* \u5f00\u5934\u7684\u7d22\u5f15\u3002 \u6253\u5f00 Kibana\uff08http://<\u4f60\u7684\u8282\u70b9IP>:30601\uff09\uff0c\u5728 Stack Management \u2192 Index Patterns \u91cc\u6dfb\u52a0 kubernetes-* \uff0c\u9009\u62e9 @timestamp \u5b57\u6bb5\uff0c\u5c31\u80fd\u5f00\u59cb\u5728 Discover \u91cc\u67e5\u770b\u65e5\u5fd7\u4e86 \ud83c\udf89 \u2705 \u5230\u8fd9\u91cc\uff0c\u4e00\u6574\u5957 EFK \u65e5\u5fd7\u7cfb\u7edf \u5c31\u90e8\u7f72\u5b8c\u6210\u4e86\u3002 \u6267\u884c\u62a5 kubectl -n efk port-forward svc/elasticsearch 9200:9200 & \u62a5\u9519 [root@k8s-master efk]# kubectl -n efk port-forward svc/elasticsearch 9200:9200 & [1] 67634 [root@k8s-master efk]# Forwarding from 127.0.0.1:9200 -> 9200 Handling connection for 9200 E0825 16:44:40.755824 67634 portforward.go:424] \"Unhandled Error\" err=\"an error occurred forwarding 9200 -> 9200: error forwarding port 9200 to pod 95b8653b6a8b5d4382d72af972b42cdc3a967c0cb50429cbd00ca703b424a510, uid : unable to do port forwarding: socat not found\" error: lost connection to pod \u89e3\u51b3\u65b9\u6848\uff1a # \u8bf4\u660e Kubernetes \u672c\u5730\u7684 port-forward \u529f\u80fd\u9700\u8981 socat \u6765\u8f6c\u53d1\u7aef\u53e3\uff0c\u4f46\u4f60\u7684\u7cfb\u7edf\u91cc\u6ca1\u6709\u5b89\u88c5\u5b83\u3002 yum install -y socat","title":"\ud83d\udd0d \u9a8c\u8bc1\u65e5\u5fd7\u91c7\u96c6"},{"location":"%E9%98%85%E8%AF%BB/1/","text":"1","title":"1"},{"location":"%E9%98%85%E8%AF%BB/%E6%96%87%E5%AD%A6/","text":"","title":"Index"},{"location":"%E9%98%85%E8%AF%BB/%E7%BB%8F%E6%B5%8E/","text":"","title":"Index"},{"location":"%E9%9D%A2%E8%AF%95/","text":"index.md","title":"Index"}]}